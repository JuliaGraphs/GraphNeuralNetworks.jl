[{"id":3,"pagetitle":"Home","title":"GraphNeuralNetworks.jl","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/#GraphNeuralNetworks.jl","content":" GraphNeuralNetworks.jl GraphNeuralNetworks.jl is a graph neural network package based on the deep learning framework  Flux.jl . It provides a set of  graph convolutional layers and utilities to build graph neural networks. Among its features: Implements common graph convolutional layers. Supports computations on batched graphs.  Easy to define custom layers. CUDA and AMDGPU support. Integration with  Graphs.jl . Examples  of node, edge, and graph level machine learning tasks.  Heterogeneous and temporal graphs. The package is part of a larger ecosystem of packages that includes  GNNlib.jl ,  GNNGraphs.jl , and  GNNLux.jl .  GraphNeuralNetworks.jl is the fronted package for Flux.jl users.  Lux.jl  users instead, can rely on  GNNLux.jl"},{"id":4,"pagetitle":"Home","title":"Installation","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/#Installation","content":" Installation GraphNeuralNetworks.jl is a registered Julia package. You can easily install it through the package manager : pkg> add GraphNeuralNetworks"},{"id":5,"pagetitle":"Home","title":"Package overview","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/#Package-overview","content":" Package overview Let's give a brief overview of the package by solving a graph regression problem with synthetic data.  Other usage examples can be found in the  examples  folder, in the  notebooks  folder, and in the  tutorials  section of the documentation."},{"id":6,"pagetitle":"Home","title":"Data preparation","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/#Data-preparation","content":" Data preparation We create a dataset consisting in multiple random graphs and associated data features.  using GraphNeuralNetworks, Flux, CUDA, Statistics, MLUtils\nusing Flux: DataLoader\n\nall_graphs = GNNGraph[]\n\nfor _ in 1:1000\n    g = rand_graph(10, 40,  \n            ndata=(; x = randn(Float32, 16,10)),  # Input node features\n            gdata=(; y = randn(Float32)))         # Regression target   \n    push!(all_graphs, g)\nend"},{"id":7,"pagetitle":"Home","title":"Model building","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/#Model-building","content":" Model building We concisely define our model as a  GNNChain  containing two graph convolutional layers. If CUDA is available, our model will live on the gpu. device = gpu_device()\n\nmodel = GNNChain(GCNConv(16 => 64),\n                BatchNorm(64),     # Apply batch normalization on node features (nodes dimension is batch dimension)\n                x -> relu.(x),     \n                GCNConv(64 => 64, relu),\n                GlobalPool(mean),  # Aggregate node-wise features into graph-wise features\n                Dense(64, 1)) |> device\n\nopt = Flux.setup(Adam(1f-4), model)"},{"id":8,"pagetitle":"Home","title":"Training","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/#Training","content":" Training Finally, we use a standard Flux training pipeline to fit our dataset. We use Flux's  DataLoader  to iterate over mini-batches of graphs  that are glued together into a single  GNNGraph  using the  MLUtils.batch  method. This is what happens under the hood when creating a  DataLoader  with the  collate=true  option.  train_graphs, test_graphs = MLUtils.splitobs(all_graphs, at=0.8)\n\ntrain_loader = DataLoader(train_graphs, \n                batchsize=32, shuffle=true, collate=true)\ntest_loader = DataLoader(test_graphs, \n                batchsize=32, shuffle=false, collate=true)\n\nloss(model, g::GNNGraph) = mean((vec(model(g, g.x)) - g.y).^2)\n\nloss(model, loader) = mean(loss(model, g |> device) for g in loader)\n\nfor epoch in 1:100\n    for g in train_loader\n        g = g |> device\n        grad = gradient(model -> loss(model, g), model)\n        Flux.update!(opt, model, grad[1])\n    end\n\n    @info (; epoch, train_loss=loss(model, train_loader), test_loss=loss(model, test_loader))\nend"},{"id":9,"pagetitle":"Home","title":"Google Summer of Code","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/#Google-Summer-of-Code","content":" Google Summer of Code Potential candidates to Google Summer of Code's scholarships can find out about the available projects involving GraphNeuralNetworks.jl on the  dedicated page  in the Julia Language website."},{"id":10,"pagetitle":"Home","title":"Citing","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/#Citing","content":" Citing If you use GraphNeuralNetworks.jl in a scientific publication, we would appreciate the following reference: @misc{Lucibello2021GNN,\n  author       = {Carlo Lucibello and other contributors},\n  title        = {GraphNeuralNetworks.jl: a geometric deep learning library for the Julia programming language},\n  year         = 2021,\n  url          = {https://github.com/JuliaGraphs/GraphNeuralNetworks.jl}\n}"},{"id":11,"pagetitle":"Home","title":"Acknowledgments","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/#Acknowledgments","content":" Acknowledgments GraphNeuralNetworks.jl is largely inspired by  PyTorch Geometric ,  Deep Graph Library , and  GeometricFlux.jl ."},{"id":14,"pagetitle":"GNNGraphs.jl","title":"GNNGraphs.jl","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/#GNNGraphs.jl","content":" GNNGraphs.jl GNNGraphs.jl is a package that provides graph data structures and helper functions specifically designed for working with graph neural networks. This package allows to store not only the graph structure, but also features associated with nodes, edges, and the graph itself. It is the core foundation for the GNNlib.jl, GraphNeuralNetworks.jl, and GNNLux.jl packages. It supports three types of graphs:  Static graph  is the basic graph type represented by  GNNGraph , where each node and edge can have associated features. This type of graph is used in typical graph neural network applications, where neural networks operate on both the structure of the graph and the features stored in it. It can be used to represent a graph where the structure does not change over time, but the features of the nodes and edges can change over time. Heterogeneous graph  is a graph that supports multiple types of nodes and edges, and is represented by  GNNHeteroGraph . Each type can have its own properties and relationships. This is useful in scenarios with different entities and interactions, such as in citation graphs or multi-relational data. Temporal graph  is a graph that changes over time, and is represented by  TemporalSnapshotsGNNGraph . Edges and features can change dynamically. This type of graph is useful for applications that involve tracking time-dependent relationships, such as social networks. This package depends on the package  Graphs.jl ."},{"id":15,"pagetitle":"GNNGraphs.jl","title":"Installation","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/#Installation","content":" Installation The package can be installed with the Julia package manager. From the Julia REPL, type  ]  to enter the Pkg REPL mode and run: pkg> add GNNGraphs"},{"id":18,"pagetitle":"Datasets","title":"Datasets","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/datasets/#Datasets","content":" Datasets"},{"id":19,"pagetitle":"Datasets","title":"GNNGraphs.mldataset2gnngraph","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/datasets/#GNNGraphs.mldataset2gnngraph","content":" GNNGraphs.mldataset2gnngraph  —  Function mldataset2gnngraph(dataset) Convert a graph dataset from the package MLDatasets.jl into one or many  GNNGraph s. Examples julia> using MLDatasets, GNNGraphs\n\njulia> mldataset2gnngraph(Cora())\nGNNGraph:\n  num_nodes: 2708\n  num_edges: 10556\n  ndata:\n        val_mask = 2708-element BitVector\n        targets = 2708-element Vector{Int64}\n        test_mask = 2708-element BitVector\n        features = 1433×2708 Matrix{Float32}\n        train_mask = 2708-element BitVector source"},{"id":22,"pagetitle":"GNNGraph","title":"GNNGraph","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/gnngraph/#GNNGraph","content":" GNNGraph Documentation page for the graph type  GNNGraph  provided by GNNGraphs.jl and related methods.  Besides the methods documented here, one can rely on the large set of functionalities given by  Graphs.jl  thanks to the fact that  GNNGraph  inherits from  Graphs.AbstractGraph ."},{"id":23,"pagetitle":"GNNGraph","title":"GNNGraph type","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/gnngraph/#GNNGraph-type","content":" GNNGraph type"},{"id":24,"pagetitle":"GNNGraph","title":"GNNGraphs.GNNGraph","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.GNNGraph","content":" GNNGraphs.GNNGraph  —  Type GNNGraph(data; [graph_type, ndata, edata, gdata, num_nodes, graph_indicator, dir])\nGNNGraph(g::GNNGraph; [ndata, edata, gdata]) A type representing a graph structure that also stores feature arrays associated to nodes, edges, and the graph itself. The feature arrays are stored in the fields  ndata ,  edata , and  gdata  as  DataStore  objects offering a convenient dictionary-like  and namedtuple-like interface. The features can be passed at construction time or added later. A  GNNGraph  can be constructed out of different  data  objects expressing the connections inside the graph. The internal representation type is determined by  graph_type . When constructed from another  GNNGraph , the internal graph representation is preserved and shared. The node/edge/graph features are retained as well, unless explicitely set by the keyword arguments  ndata ,  edata , and  gdata . A  GNNGraph  can also represent multiple graphs batched togheter (see  MLUtils.batch  or  SparseArrays.blockdiag ). The field  g.graph_indicator  contains the graph membership of each node. GNNGraph s are always directed graphs, therefore each edge is defined by a source node and a target node (see  edge_index ). Self loops (edges connecting a node to itself) and multiple edges (more than one edge between the same pair of nodes) are supported. A  GNNGraph  is a Graphs.jl's  AbstractGraph , therefore it supports most functionality from that library. Arguments data : Some data representing the graph topology. Possible type are An adjacency matrix An adjacency list. A tuple containing the source and target vectors (COO representation) A Graphs.jl' graph. graph_type : A keyword argument that specifies               the underlying representation used by the GNNGraph.               Currently supported values are :coo . Graph represented as a tuple  (source, target) , such that the  k -th edge         connects the node  source[k]  to node  target[k] .         Optionally, also edge weights can be given:  (source, target, weights) . :sparse . A sparse adjacency matrix representation. :dense . A dense adjacency matrix representation. Defaults to  :coo , currently the most supported type. dir : The assumed edge direction when given adjacency matrix or adjacency list input data  g .       Possible values are  :out  and  :in . Default  :out . num_nodes : The number of nodes. If not specified, inferred from  g . Default  nothing . graph_indicator : For batched graphs, a vector containing the graph assignment of each node. Default  nothing . ndata : Node features. An array or named tuple of arrays whose last dimension has size  num_nodes . edata : Edge features. An array or named tuple of arrays whose last dimension has size  num_edges . gdata : Graph features. An array or named tuple of arrays whose last dimension has size  num_graphs . Examples using GNNGraphs\n\n# Construct from adjacency list representation\ndata = [[2,3], [1,4,5], [1], [2,5], [2,4]]\ng = GNNGraph(data)\n\n# Number of nodes, edges, and batched graphs\ng.num_nodes  # 5\ng.num_edges  # 10\ng.num_graphs # 1\n\n# Same graph in COO representation\ns = [1,1,2,2,2,3,4,4,5,5]\nt = [2,3,1,4,5,3,2,5,2,4]\ng = GNNGraph(s, t)\n\n# From a Graphs' graph\ng = GNNGraph(erdos_renyi(100, 20))\n\n# Add 2 node feature arrays at creation time\ng = GNNGraph(g, ndata = (x=rand(100, g.num_nodes), y=rand(g.num_nodes)))\n\n# Add 1 edge feature array, after the graph creation\ng.edata.z = rand(16, g.num_edges)\n\n# Add node features and edge features with default names `x` and `e`\ng = GNNGraph(g, ndata = rand(100, g.num_nodes), edata = rand(16, g.num_edges))\n\ng.ndata.x # or just g.x\ng.edata.e # or just g.e\n\n# Collect edges' source and target nodes.\n# Both source and target are vectors of length num_edges\nsource, target = edge_index(g) A  GNNGraph  can be sent to the GPU, for example by using Flux.jl's  gpu  function or MLDataDevices.jl's utilities.  ``` source"},{"id":25,"pagetitle":"GNNGraph","title":"Base.copy","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/gnngraph/#Base.copy","content":" Base.copy  —  Function copy(g::GNNGraph; deep=false) Create a copy of  g . If  deep  is  true , then copy will be a deep copy (equivalent to  deepcopy(g) ), otherwise it will be a shallow copy with the same underlying graph data. source"},{"id":26,"pagetitle":"GNNGraph","title":"DataStore","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/gnngraph/#DataStore","content":" DataStore"},{"id":27,"pagetitle":"GNNGraph","title":"GNNGraphs.DataStore","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.DataStore","content":" GNNGraphs.DataStore  —  Type DataStore([n, data])\nDataStore([n,] k1 = x1, k2 = x2, ...) A container for feature arrays. The optional argument  n  enforces that  numobs(x) == n  for each array contained in the datastore. At construction time, the  data  can be provided as any iterables of pairs of symbols and arrays or as keyword arguments: julia> ds = DataStore(3, x = rand(Float32, 2, 3), y = rand(Float32, 3))\nDataStore(3) with 2 elements:\n  y = 3-element Vector{Float32}\n  x = 2×3 Matrix{Float32}\n\njulia> ds = DataStore(3, Dict(:x => rand(Float32, 2, 3), :y => rand(Float32, 3))); # equivalent to above The  DataStore  has an interface similar to both dictionaries and named tuples. Arrays can be accessed and added using either the indexing or the property syntax: julia> ds = DataStore(x = ones(Float32, 2, 3), y = zeros(Float32, 3))\nDataStore() with 2 elements:\n  y = 3-element Vector{Float32}\n  x = 2×3 Matrix{Float32}\n\njulia> ds.x   # same as `ds[:x]`\n2×3 Matrix{Float32}:\n 1.0  1.0  1.0\n 1.0  1.0  1.0\n\njulia> ds.z = zeros(Float32, 3)  # Add new feature array `z`. Same as `ds[:z] = rand(Float32, 3)`\n3-element Vector{Float32}:\n 0.0\n 0.0\n 0.0 The  DataStore  can be iterated over, and the keys and values can be accessed using  keys(ds)  and  values(ds) .  map(f, ds)  applies the function  f  to each feature array: julia> ds2 = map(x -> x .+ 1, ds)\nDataStore() with 3 elements:\n  y = 3-element Vector{Float32}\n  z = 3-element Vector{Float32}\n  x = 2×3 Matrix{Float32}\n\njulia> ds2.z\n3-element Vector{Float32}:\n 1.0\n 1.0\n 1.0 source"},{"id":28,"pagetitle":"GNNGraph","title":"Query","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/gnngraph/#Query","content":" Query"},{"id":29,"pagetitle":"GNNGraph","title":"GNNGraphs.adjacency_list","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.adjacency_list-Tuple{GNNGraph, Any}","content":" GNNGraphs.adjacency_list  —  Method adjacency_list(g; dir=:out)\nadjacency_list(g, nodes; dir=:out) Return the adjacency list representation (a vector of vectors) of the graph  g . Calling  a  the adjacency list, if  dir=:out  than  a[i]  will contain the neighbors of node  i  through outgoing edges. If  dir=:in , it will contain neighbors from incoming edges instead. If  nodes  is given, return the neighborhood of the nodes in  nodes  only. source"},{"id":30,"pagetitle":"GNNGraph","title":"GNNGraphs.edge_index","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.edge_index-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}}","content":" GNNGraphs.edge_index  —  Method edge_index(g::GNNGraph) Return a tuple containing two vectors, respectively storing  the source and target nodes for each edges in  g . s, t = edge_index(g) source"},{"id":31,"pagetitle":"GNNGraph","title":"GNNGraphs.get_graph_type","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.get_graph_type-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}}","content":" GNNGraphs.get_graph_type  —  Method get_graph_type(g::GNNGraph) Return the underlying representation for the graph  g  as a symbol. Possible values are: :coo : Coordinate list representation. The graph is stored as a tuple of vectors  (s, t, w) ,         where  s  and  t  are the source and target nodes of the edges, and  w  is the edge weights. :sparse : Sparse matrix representation. The graph is stored as a sparse matrix representing the weighted adjacency matrix. :dense : Dense matrix representation. The graph is stored as a dense matrix representing the weighted adjacency matrix. The default representation for graph constructors GNNGraphs.jl is  :coo . The underlying representation can be accessed through the  g.graph  field. See also  GNNGraph . Examples The default representation for graph constructors GNNGraphs.jl is  :coo . julia> g = rand_graph(5, 10)\nGNNGraph:\n  num_nodes: 5\n  num_edges: 10\n\njulia> get_graph_type(g)\n:coo The  GNNGraph  constructor can also be used to create graphs with different representations. julia> g = GNNGraph([2,3,5], [1,2,4], graph_type=:sparse)\nGNNGraph:\n  num_nodes: 5\n  num_edges: 3\n\njulia> g.graph\n5×5 SparseArrays.SparseMatrixCSC{Int64, Int64} with 3 stored entries:\n ⋅  ⋅  ⋅  ⋅  ⋅\n 1  ⋅  ⋅  ⋅  ⋅\n ⋅  1  ⋅  ⋅  ⋅\n ⋅  ⋅  ⋅  ⋅  ⋅\n ⋅  ⋅  ⋅  1  ⋅\n\njulia> get_graph_type(g)\n:sparse\n\njulia> gcoo = GNNGraph(g, graph_type=:coo);\n\njulia> gcoo.graph\n([2, 3, 5], [1, 2, 4], [1, 1, 1]) source"},{"id":32,"pagetitle":"GNNGraph","title":"GNNGraphs.graph_indicator","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.graph_indicator-Tuple{GNNGraph}","content":" GNNGraphs.graph_indicator  —  Method graph_indicator(g::GNNGraph; edges=false) Return a vector containing the graph membership (an integer from  1  to  g.num_graphs ) of each node in the graph. If  edges=true , return the graph membership of each edge instead. source"},{"id":33,"pagetitle":"GNNGraph","title":"GNNGraphs.has_isolated_nodes","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.has_isolated_nodes-Tuple{GNNGraph}","content":" GNNGraphs.has_isolated_nodes  —  Method has_isolated_nodes(g::GNNGraph; dir=:out) Return true if the graph  g  contains nodes with out-degree (if  dir=:out ) or in-degree (if  dir = :in ) equal to zero. source"},{"id":34,"pagetitle":"GNNGraph","title":"GNNGraphs.has_multi_edges","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.has_multi_edges-Tuple{GNNGraph}","content":" GNNGraphs.has_multi_edges  —  Method has_multi_edges(g::GNNGraph) Return  true  if  g  has any multiple edges. source"},{"id":35,"pagetitle":"GNNGraph","title":"GNNGraphs.is_bidirected","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.is_bidirected-Tuple{GNNGraph}","content":" GNNGraphs.is_bidirected  —  Method is_bidirected(g::GNNGraph) Check if the directed graph  g  essentially corresponds to an undirected graph, i.e. if for each edge it also contains the  reverse edge.  source"},{"id":36,"pagetitle":"GNNGraph","title":"GNNGraphs.khop_adj","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.khop_adj","content":" GNNGraphs.khop_adj  —  Function khop_adj(g::GNNGraph,k::Int,T::DataType=eltype(g); dir=:out, weighted=true) Return  $A^k$  where  $A$  is the adjacency matrix of the graph 'g'. source"},{"id":37,"pagetitle":"GNNGraph","title":"GNNGraphs.laplacian_lambda_max","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.laplacian_lambda_max","content":" GNNGraphs.laplacian_lambda_max  —  Function laplacian_lambda_max(g::GNNGraph, T=Float32; add_self_loops=false, dir=:out) Return the largest eigenvalue of the normalized symmetric Laplacian of the graph  g . If the graph is batched from multiple graphs, return the list of the largest eigenvalue for each graph. source"},{"id":38,"pagetitle":"GNNGraph","title":"GNNGraphs.normalized_laplacian","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.normalized_laplacian","content":" GNNGraphs.normalized_laplacian  —  Function normalized_laplacian(g, T=Float32; add_self_loops=false, dir=:out) Normalized Laplacian matrix of graph  g . Arguments g : A  GNNGraph . T : result element type. add_self_loops : add self-loops while calculating the matrix. dir : the edge directionality considered (:out, :in, :both). source"},{"id":39,"pagetitle":"GNNGraph","title":"GNNGraphs.scaled_laplacian","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.scaled_laplacian","content":" GNNGraphs.scaled_laplacian  —  Function scaled_laplacian(g, T=Float32; dir=:out) Scaled Laplacian matrix of graph  g , defined as  $\\hat{L} = \\frac{2}{\\lambda_{max}} L - I$  where  $L$  is the normalized Laplacian matrix. Arguments g : A  GNNGraph . T : result element type. dir : the edge directionality considered (:out, :in, :both). source"},{"id":40,"pagetitle":"GNNGraph","title":"Graphs.LinAlg.adjacency_matrix","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/gnngraph/#Graphs.LinAlg.adjacency_matrix","content":" Graphs.LinAlg.adjacency_matrix  —  Function adjacency_matrix(g::GNNGraph, T=eltype(g); dir=:out, weighted=true) Return the adjacency matrix  A  for the graph  g .  If  dir=:out ,  A[i,j] > 0  denotes the presence of an edge from node  i  to node  j . If  dir=:in  instead,  A[i,j] > 0  denotes the presence of an edge from node  j  to node  i . User may specify the eltype  T  of the returned matrix.  If  weighted=true , the  A  will contain the edge weights if any, otherwise the elements of  A  will be either 0 or 1. source"},{"id":41,"pagetitle":"GNNGraph","title":"Graphs.degree","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/gnngraph/#Graphs.degree-Union{Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}}, Tuple{TT}, Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}, TT}} where TT<:Union{Nothing, Type{<:Number}}","content":" Graphs.degree  —  Method degree(g::GNNGraph, T=nothing; dir=:out, edge_weight=true) Return a vector containing the degrees of the nodes in  g . The gradient is propagated through this function only if  edge_weight  is  true  or a vector. Arguments g : A graph. T : Element type of the returned vector. If  nothing , is      chosen based on the graph type and will be an integer      if  edge_weight = false . Default  nothing . dir : For  dir = :out  the degree of a node is counted based on the outgoing edges.        For  dir = :in , the ingoing edges are used. If  dir = :both  we have the sum of the two. edge_weight : If  true  and the graph contains weighted edges, the degree will                be weighted. Set to  false  instead to just count the number of               outgoing/ingoing edges.                Finally, you can also pass a vector of weights to be used               instead of the graph's own weights.               Default  true . source"},{"id":42,"pagetitle":"GNNGraph","title":"Graphs.has_self_loops","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/gnngraph/#Graphs.has_self_loops-Tuple{GNNGraph}","content":" Graphs.has_self_loops  —  Method has_self_loops(g::GNNGraph) Return  true  if  g  has any self loops. source"},{"id":43,"pagetitle":"GNNGraph","title":"Graphs.inneighbors","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/gnngraph/#Graphs.inneighbors-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}, Integer}","content":" Graphs.inneighbors  —  Method inneighbors(g::GNNGraph, i::Integer) Return the neighbors of node  i  in the graph  g  through incoming edges. See also  neighbors  and  outneighbors . source"},{"id":44,"pagetitle":"GNNGraph","title":"Graphs.outneighbors","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/gnngraph/#Graphs.outneighbors-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}, Integer}","content":" Graphs.outneighbors  —  Method outneighbors(g::GNNGraph, i::Integer) Return the neighbors of node  i  in the graph  g  through outgoing edges. See also  neighbors  and  inneighbors . source"},{"id":45,"pagetitle":"GNNGraph","title":"Graphs.neighbors","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/gnngraph/#Graphs.neighbors-Tuple{GNNGraph, Integer}","content":" Graphs.neighbors  —  Method neighbors(g::GNNGraph, i::Integer; dir=:out) Return the neighbors of node  i  in the graph  g . If  dir=:out , return the neighbors through outgoing edges. If  dir=:in , return the neighbors through incoming edges. See also  outneighbors ,  inneighbors . source"},{"id":46,"pagetitle":"GNNGraph","title":"Transform","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/gnngraph/#Transform","content":" Transform"},{"id":47,"pagetitle":"GNNGraph","title":"GNNGraphs.add_edges","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.add_edges-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}, AbstractVector, AbstractVector}","content":" GNNGraphs.add_edges  —  Method add_edges(g::GNNGraph, s::AbstractVector, t::AbstractVector; [edata])\nadd_edges(g::GNNGraph, (s, t); [edata])\nadd_edges(g::GNNGraph, (s, t, w); [edata]) Add to graph  g  the edges with source nodes  s  and target nodes  t . Optionally, pass the edge weight  w  and the features   edata  for the new edges. Returns a new graph sharing part of the underlying data with  g . If the  s  or  t  contain nodes that are not already present in the graph, they are added to the graph as well. Examples julia> s, t = [1, 2, 3, 3, 4], [2, 3, 4, 4, 4];\n\njulia> w = Float32[1.0, 2.0, 3.0, 4.0, 5.0];\n\njulia> g = GNNGraph((s, t, w))\nGNNGraph:\n  num_nodes: 4\n  num_edges: 5\n\njulia> add_edges(g, ([2, 3], [4, 1], [10.0, 20.0]))\nGNNGraph:\n  num_nodes: 4\n  num_edges: 7 julia> g = GNNGraph()\nGNNGraph:\n  num_nodes: 0\n  num_edges: 0\n\njulia> add_edges(g, [1,2], [2,3])\nGNNGraph:\n  num_nodes: 3\n  num_edges: 2 source"},{"id":48,"pagetitle":"GNNGraph","title":"GNNGraphs.add_nodes","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.add_nodes-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}, Integer}","content":" GNNGraphs.add_nodes  —  Method add_nodes(g::GNNGraph, n; [ndata]) Add  n  new nodes to graph  g . In the  new graph, these nodes will have indexes from  g.num_nodes + 1  to  g.num_nodes + n . source"},{"id":49,"pagetitle":"GNNGraph","title":"GNNGraphs.add_self_loops","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.add_self_loops-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}}","content":" GNNGraphs.add_self_loops  —  Method add_self_loops(g::GNNGraph) Return a graph with the same features as  g  but also adding edges connecting the nodes to themselves. Nodes with already existing self-loops will obtain a second self-loop. If the graphs has edge weights, the new edges will have weight 1. source"},{"id":50,"pagetitle":"GNNGraph","title":"GNNGraphs.getgraph","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.getgraph-Tuple{GNNGraph, Int64}","content":" GNNGraphs.getgraph  —  Method getgraph(g::GNNGraph, i; nmap=false) Return the subgraph of  g  induced by those nodes  j  for which  g.graph_indicator[j] == i  or, if  i  is a collection,  g.graph_indicator[j] ∈ i .  In other words, it extract the component graphs from a batched graph.  If  nmap=true , return also a vector  v  mapping the new nodes to the old ones.  The node  i  in the subgraph will correspond to the node  v[i]  in  g . source"},{"id":51,"pagetitle":"GNNGraph","title":"GNNGraphs.negative_sample","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.negative_sample-Tuple{GNNGraph}","content":" GNNGraphs.negative_sample  —  Method negative_sample(g::GNNGraph; \n                num_neg_edges = g.num_edges, \n                bidirected = is_bidirected(g)) Return a graph containing random negative edges (i.e. non-edges) from graph  g  as edges. If  bidirected=true , the output graph will be bidirected and there will be no leakage from the origin graph.  See also  is_bidirected . source"},{"id":52,"pagetitle":"GNNGraph","title":"GNNGraphs.perturb_edges","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.perturb_edges-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}, AbstractFloat}","content":" GNNGraphs.perturb_edges  —  Method perturb_edges([rng], g::GNNGraph, perturb_ratio) Return a new graph obtained from  g  by adding random edges, based on a specified  perturb_ratio .  The  perturb_ratio  determines the fraction of new edges to add relative to the current number of edges in the graph.  These new edges are added without creating self-loops.  The function returns a new  GNNGraph  instance that shares some of the underlying data with  g  but includes the additional edges.  The nodes for the new edges are selected randomly, and no edge data ( edata ) or weights ( w ) are assigned to these new edges. Arguments g::GNNGraph : The graph to be perturbed. perturb_ratio : The ratio of the number of new edges to add relative to the current number of edges in the graph. For example, a  perturb_ratio  of 0.1 means that 10% of the current number of edges will be added as new random edges. rng : An optionalrandom number generator to ensure reproducible results. Examples julia> g = GNNGraph((s, t, w))\nGNNGraph:\n  num_nodes: 4\n  num_edges: 5\n\njulia> perturbed_g = perturb_edges(g, 0.2)\nGNNGraph:\n  num_nodes: 4\n  num_edges: 6 source"},{"id":53,"pagetitle":"GNNGraph","title":"GNNGraphs.ppr_diffusion","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.ppr_diffusion-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}}","content":" GNNGraphs.ppr_diffusion  —  Method ppr_diffusion(g::GNNGraph{<:COO_T}, alpha =0.85f0) -> GNNGraph Calculates the Personalized PageRank (PPR) diffusion based on the edge weight matrix of a GNNGraph and updates the graph with new edge weights derived from the PPR matrix. References paper:  The pagerank citation ranking: Bringing order to the web The function performs the following steps: Constructs a modified adjacency matrix  A  using the graph's edge weights, where  A  is adjusted by  (α - 1) * A + I , with  α  being the damping factor ( alpha_f32 ) and  I  the identity matrix. Normalizes  A  to ensure each column sums to 1, representing transition probabilities. Applies the PPR formula  α * (I + (α - 1) * A)^-1  to compute the diffusion matrix. Updates the original edge weights of the graph based on the PPR diffusion matrix, assigning new weights for each edge from the PPR matrix. Arguments g::GNNGraph : The input graph for which PPR diffusion is to be calculated. It should have edge weights available. alpha_f32::Float32 : The damping factor used in PPR calculation, controlling the teleport probability in the random walk. Defaults to  0.85f0 . Returns A new  GNNGraph  instance with the same structure as  g  but with updated edge weights according to the PPR diffusion calculation. source"},{"id":54,"pagetitle":"GNNGraph","title":"GNNGraphs.rand_edge_split","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.rand_edge_split-Tuple{GNNGraph, Any}","content":" GNNGraphs.rand_edge_split  —  Method rand_edge_split(g::GNNGraph, frac; bidirected=is_bidirected(g)) -> g1, g2 Randomly partition the edges in  g  to form two graphs,  g1  and  g2 . Both will have the same number of nodes as  g .  g1  will contain a fraction  frac  of the original edges,  while  g2  wil contain the rest. If  bidirected = true  makes sure that an edge and its reverse go into the same split. This option is supported only for bidirected graphs with no self-loops and multi-edges. rand_edge_split  is tipically used to create train/test splits in link prediction tasks. source"},{"id":55,"pagetitle":"GNNGraph","title":"GNNGraphs.random_walk_pe","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.random_walk_pe-Tuple{GNNGraph, Int64}","content":" GNNGraphs.random_walk_pe  —  Method random_walk_pe(g, walk_length) Return the random walk positional encoding from the paper  Graph Neural Networks with Learnable Structural and Positional Representations  of the given graph  g  and the length of the walk  walk_length  as a matrix of size  (walk_length, g.num_nodes) .  source"},{"id":56,"pagetitle":"GNNGraph","title":"GNNGraphs.remove_edges","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.remove_edges-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}, AbstractVector{<:Integer}}","content":" GNNGraphs.remove_edges  —  Method remove_edges(g::GNNGraph, edges_to_remove::AbstractVector{<:Integer})\nremove_edges(g::GNNGraph, p=0.5) Remove specified edges from a GNNGraph, either by specifying edge indices or by randomly removing edges with a given probability. Arguments g : The input graph from which edges will be removed. edges_to_remove : Vector of edge indices to be removed. This argument is only required for the first method. p : Probability of removing each edge. This argument is only required for the second method and defaults to 0.5. Returns A new GNNGraph with the specified edges removed. Example julia> using GNNGraphs\n\n# Construct a GNNGraph\njulia> g = GNNGraph([1, 1, 2, 2, 3], [2, 3, 1, 3, 1])\nGNNGraph:\n  num_nodes: 3\n  num_edges: 5\n  \n# Remove the second edge\njulia> g_new = remove_edges(g, [2]);\n\njulia> g_new\nGNNGraph:\n  num_nodes: 3\n  num_edges: 4\n\n# Remove edges with a probability of 0.5\njulia> g_new = remove_edges(g, 0.5);\n\njulia> g_new\nGNNGraph:\n  num_nodes: 3\n  num_edges: 2 source"},{"id":57,"pagetitle":"GNNGraph","title":"GNNGraphs.remove_multi_edges","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.remove_multi_edges-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}}","content":" GNNGraphs.remove_multi_edges  —  Method remove_multi_edges(g::GNNGraph; aggr=+) Remove multiple edges (also called parallel edges or repeated edges) from graph  g . Possible edge features are aggregated according to  aggr , that can take value   + , min ,  max  or  mean . See also  remove_self_loops ,  has_multi_edges , and  to_bidirected . source"},{"id":58,"pagetitle":"GNNGraph","title":"GNNGraphs.remove_nodes","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.remove_nodes-Tuple{GNNGraph, AbstractFloat}","content":" GNNGraphs.remove_nodes  —  Method remove_nodes(g::GNNGraph, p) Returns a new graph obtained by dropping nodes from  g  with independent probabilities  p .  Examples julia> g = GNNGraph([1, 1, 2, 2, 3, 4], [1, 2, 3, 1, 3, 1])\nGNNGraph:\n  num_nodes: 4\n  num_edges: 6\n\njulia> g_new = remove_nodes(g, 0.5)\nGNNGraph:\n  num_nodes: 2\n  num_edges: 2 source"},{"id":59,"pagetitle":"GNNGraph","title":"GNNGraphs.remove_nodes","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.remove_nodes-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}, AbstractVector}","content":" GNNGraphs.remove_nodes  —  Method remove_nodes(g::GNNGraph, nodes_to_remove::AbstractVector) Remove specified nodes, and their associated edges, from a GNNGraph. This operation reindexes the remaining nodes to maintain a continuous sequence of node indices, starting from 1. Similarly, edges are reindexed to account for the removal of edges connected to the removed nodes. Arguments g : The input graph from which nodes (and their edges) will be removed. nodes_to_remove : Vector of node indices to be removed. Returns A new GNNGraph with the specified nodes and all edges associated with these nodes removed.  Example using GNNGraphs\n\ng = GNNGraph([1, 1, 2, 2, 3], [2, 3, 1, 3, 1])\n\n# Remove nodes with indices 2 and 3, for example\ng_new = remove_nodes(g, [2, 3])\n\n# g_new now does not contain nodes 2 and 3, and any edges that were connected to these nodes.\nprintln(g_new) source"},{"id":60,"pagetitle":"GNNGraph","title":"GNNGraphs.remove_self_loops","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.remove_self_loops-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}}","content":" GNNGraphs.remove_self_loops  —  Method remove_self_loops(g::GNNGraph) Return a graph constructed from  g  where self-loops (edges from a node to itself) are removed.  See also  add_self_loops  and  remove_multi_edges . source"},{"id":61,"pagetitle":"GNNGraph","title":"GNNGraphs.set_edge_weight","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.set_edge_weight-Tuple{GNNGraph, AbstractVector}","content":" GNNGraphs.set_edge_weight  —  Method set_edge_weight(g::GNNGraph, w::AbstractVector) Set  w  as edge weights in the returned graph.  source"},{"id":62,"pagetitle":"GNNGraph","title":"GNNGraphs.to_bidirected","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.to_bidirected-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}}","content":" GNNGraphs.to_bidirected  —  Method to_bidirected(g) Adds a reverse edge for each edge in the graph, then calls   remove_multi_edges  with  mean  aggregation to simplify the graph.  See also  is_bidirected .  Examples julia> s, t = [1, 2, 3, 3, 4], [2, 3, 4, 4, 4];\n\njulia> w = [1.0, 2.0, 3.0, 4.0, 5.0];\n\njulia> e = [10.0, 20.0, 30.0, 40.0, 50.0];\n\njulia> g = GNNGraph(s, t, w, edata = e)\nGNNGraph:\n  num_nodes: 4\n  num_edges: 5\n  edata:\n        e = 5-element Vector{Float64}\n\njulia> g2 = to_bidirected(g)\nGNNGraph:\n  num_nodes: 4\n  num_edges: 7\n  edata:\n        e = 7-element Vector{Float64}\n\njulia> edge_index(g2)\n([1, 2, 2, 3, 3, 4, 4], [2, 1, 3, 2, 4, 3, 4])\n\njulia> get_edge_weight(g2)\n7-element Vector{Float64}:\n 1.0\n 1.0\n 2.0\n 2.0\n 3.5\n 3.5\n 5.0\n\njulia> g2.edata.e\n7-element Vector{Float64}:\n 10.0\n 10.0\n 20.0\n 20.0\n 35.0\n 35.0\n 50.0 source"},{"id":63,"pagetitle":"GNNGraph","title":"GNNGraphs.to_unidirected","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.to_unidirected-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}}","content":" GNNGraphs.to_unidirected  —  Method to_unidirected(g::GNNGraph) Return a graph that for each multiple edge between two nodes in  g  keeps only an edge in one direction. source"},{"id":64,"pagetitle":"GNNGraph","title":"MLUtils.batch","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/gnngraph/#MLUtils.batch-Tuple{AbstractVector{<:GNNGraph}}","content":" MLUtils.batch  —  Method batch(gs::Vector{<:GNNGraph}) Batch together multiple  GNNGraph s into a single one  containing the total number of original nodes and edges. Equivalent to  SparseArrays.blockdiag . See also  MLUtils.unbatch . Examples julia> g1 = rand_graph(4, 4, ndata=ones(Float32, 3, 4))\nGNNGraph:\n  num_nodes: 4\n  num_edges: 4\n  ndata:\n        x = 3×4 Matrix{Float32}\n\njulia> g2 = rand_graph(5, 4, ndata=zeros(Float32, 3, 5))\nGNNGraph:\n  num_nodes: 5\n  num_edges: 4\n  ndata:\n        x = 3×5 Matrix{Float32}\n\njulia> g12 = MLUtils.batch([g1, g2])\nGNNGraph:\n  num_nodes: 9\n  num_edges: 8\n  num_graphs: 2\n  ndata:\n        x = 3×9 Matrix{Float32}\n\njulia> g12.ndata.x\n3×9 Matrix{Float32}:\n 1.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0\n 1.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0\n 1.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0 source"},{"id":65,"pagetitle":"GNNGraph","title":"MLUtils.unbatch","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/gnngraph/#MLUtils.unbatch-Union{Tuple{GNNGraph{T}}, Tuple{T}} where T<:(Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}})","content":" MLUtils.unbatch  —  Method unbatch(g::GNNGraph) Opposite of the  MLUtils.batch  operation, returns  an array of the individual graphs batched together in  g . See also  MLUtils.batch  and  getgraph . Examples julia> using MLUtils\n\njulia> gbatched = MLUtils.batch([rand_graph(5, 6), rand_graph(10, 8), rand_graph(4,2)])\nGNNGraph:\n  num_nodes: 19\n  num_edges: 16\n  num_graphs: 3\n\njulia> MLUtils.unbatch(gbatched)\n3-element Vector{GNNGraph{Tuple{Vector{Int64}, Vector{Int64}, Nothing}}}:\n GNNGraph(5, 6) with no data\n GNNGraph(10, 8) with no data\n GNNGraph(4, 2) with no data source"},{"id":66,"pagetitle":"GNNGraph","title":"SparseArrays.blockdiag","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/gnngraph/#SparseArrays.blockdiag-Tuple{GNNGraph, Vararg{GNNGraph}}","content":" SparseArrays.blockdiag  —  Method blockdiag(xs::GNNGraph...) Equivalent to  MLUtils.batch . source"},{"id":67,"pagetitle":"GNNGraph","title":"Utils","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/gnngraph/#Utils","content":" Utils"},{"id":68,"pagetitle":"GNNGraph","title":"GNNGraphs.sort_edge_index","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.sort_edge_index","content":" GNNGraphs.sort_edge_index  —  Function sort_edge_index(ei::Tuple) -> u', v'\nsort_edge_index(u, v) -> u', v' Return a sorted version of the tuple of vectors  ei = (u, v) , applying a common permutation to  u  and  v . The sorting is lexycographic, that is the pairs  (ui, vi)   are sorted first according to the  ui  and then according to  vi .  source"},{"id":69,"pagetitle":"GNNGraph","title":"GNNGraphs.color_refinement","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.color_refinement","content":" GNNGraphs.color_refinement  —  Function color_refinement(g::GNNGraph, [x0]) -> x, num_colors, niters The color refinement algorithm for graph coloring.  Given a graph  g  and an initial coloring  x0 , the algorithm  iteratively refines the coloring until a fixed point is reached. At each iteration the algorithm computes a hash of the coloring and the sorted list of colors of the neighbors of each node. This hash is used to determine if the coloring has changed. math x_i' = hashmap((x_i, sort([x_j for j \\in N(i)]))). ` This algorithm is related to the 1-Weisfeiler-Lehman algorithm for graph isomorphism testing. Arguments g::GNNGraph : The graph to color. x0::AbstractVector{<:Integer} : The initial coloring. If not provided, all nodes are colored with 1. Returns x::AbstractVector{<:Integer} : The final coloring. num_colors::Int : The number of colors used. niters::Int : The number of iterations until convergence. source"},{"id":70,"pagetitle":"GNNGraph","title":"Generate","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/gnngraph/#Generate","content":" Generate"},{"id":71,"pagetitle":"GNNGraph","title":"GNNGraphs.knn_graph","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.knn_graph-Tuple{AbstractMatrix, Int64}","content":" GNNGraphs.knn_graph  —  Method knn_graph(points::AbstractMatrix, \n          k::Int; \n          graph_indicator = nothing,\n          self_loops = false, \n          dir = :in, \n          kws...) Create a  k -nearest neighbor graph where each node is linked  to its  k  closest  points .   Arguments points : A num features × num nodes matrix storing the Euclidean positions of the nodes. k : The number of neighbors considered in the kNN algorithm. graph_indicator : Either nothing or a vector containing the graph assignment of each node,                     in which case the returned graph will be a batch of graphs.  self_loops : If  true , consider the node itself among its  k  nearest neighbors, in which               case the graph will contain self-loops.  dir : The direction of the edges. If  dir=:in  edges go from the  k          neighbors to the central node. If  dir=:out  we have the opposite        direction. kws : Further keyword arguments will be passed to the  GNNGraph  constructor. Examples julia> n, k = 10, 3;\n\njulia> x = rand(Float32, 3, n);\n\njulia> g = knn_graph(x, k)\nGNNGraph:\n    num_nodes = 10\n    num_edges = 30\n\njulia> graph_indicator = [1,1,1,1,1,2,2,2,2,2];\n\njulia> g = knn_graph(x, k; graph_indicator)\nGNNGraph:\n    num_nodes = 10\n    num_edges = 30\n    num_graphs = 2 source"},{"id":72,"pagetitle":"GNNGraph","title":"GNNGraphs.radius_graph","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.radius_graph-Tuple{AbstractMatrix, AbstractFloat}","content":" GNNGraphs.radius_graph  —  Method radius_graph(points::AbstractMatrix, \n             r::AbstractFloat; \n             graph_indicator = nothing,\n             self_loops = false, \n             dir = :in, \n             kws...) Create a graph where each node is linked  to its neighbors within a given distance  r .   Arguments points : A num features × num nodes matrix storing the Euclidean positions of the nodes. r : The radius. graph_indicator : Either nothing or a vector containing the graph assignment of each node,                     in which case the returned graph will be a batch of graphs.  self_loops : If  true , consider the node itself among its neighbors, in which               case the graph will contain self-loops.  dir : The direction of the edges. If  dir=:in  edges go from the        neighbors to the central node. If  dir=:out  we have the opposite        direction. kws : Further keyword arguments will be passed to the  GNNGraph  constructor. Examples julia> n, r = 10, 0.75;\n\njulia> x = rand(Float32, 3, n);\n\njulia> g = radius_graph(x, r)\nGNNGraph:\n    num_nodes = 10\n    num_edges = 46\n\njulia> graph_indicator = [1,1,1,1,1,2,2,2,2,2];\n\njulia> g = radius_graph(x, r; graph_indicator)\nGNNGraph:\n    num_nodes = 10\n    num_edges = 20\n    num_graphs = 2 References Section B paragraphs 1 and 2 of the paper  Dynamic Hidden-Variable Network Models source"},{"id":73,"pagetitle":"GNNGraph","title":"GNNGraphs.rand_graph","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.rand_graph-Tuple{Integer, Integer}","content":" GNNGraphs.rand_graph  —  Method rand_graph([rng,] n, m; bidirected=true, edge_weight = nothing, kws...) Generate a random (Erdós-Renyi)  GNNGraph  with  n  nodes and  m  edges. If  bidirected=true  the reverse edge of each edge will be present. If  bidirected=false  instead,  m  unrelated edges are generated. In any case, the output graph will contain no self-loops or multi-edges. A vector can be passed  as  edge_weight . Its length has to be equal to  m  in the directed case, and  m÷2  in the bidirected one. Pass a random number generator as the first argument to make the generation reproducible. Additional keyword arguments will be passed to the  GNNGraph  constructor. Examples julia> g = rand_graph(5, 4, bidirected=false)\nGNNGraph:\n  num_nodes: 5\n  num_edges: 4\n\njulia> edge_index(g)\n([4, 3, 2, 1], [5, 4, 3, 2])\n\n# In the bidirected case, edge data will be duplicated on the reverse edges if needed.\njulia> g = rand_graph(5, 4, edata=rand(Float32, 16, 2))\nGNNGraph:\n  num_nodes: 5\n  num_edges: 4\n  edata:\n        e = 16×4 Matrix{Float32}\n\n# Each edge has a reverse\njulia> edge_index(g)\n([1, 1, 5, 3], [5, 3, 1, 1]) source"},{"id":74,"pagetitle":"GNNGraph","title":"Operators","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/gnngraph/#Operators","content":" Operators"},{"id":75,"pagetitle":"GNNGraph","title":"Base.intersect","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/gnngraph/#Base.intersect","content":" Base.intersect  —  Function intersect(g1::GNNGraph, g2::GNNGraph) Intersect two graphs by keeping only the common edges. source"},{"id":76,"pagetitle":"GNNGraph","title":"Sampling","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/gnngraph/#Sampling","content":" Sampling"},{"id":77,"pagetitle":"GNNGraph","title":"GNNGraphs.sample_neighbors","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.sample_neighbors","content":" GNNGraphs.sample_neighbors  —  Function sample_neighbors(g, nodes, K=-1; dir=:in, replace=false, dropnodes=false) Sample neighboring edges of the given nodes and return the induced subgraph. For each node, a number of inbound (or outbound when  dir = :out ) edges will be randomly chosen.  If dropnodes=false`, the graph returned will then contain all the nodes in the original graph,  but only the sampled edges. The returned graph will contain an edge feature  EID  corresponding to the id of the edge in the original graph. If  dropnodes=true , it will also contain a node feature  NID  with the node ids in the original graph. Arguments g . The graph. nodes . A list of node IDs to sample neighbors from. K . The maximum number of edges to be sampled for each node.      If -1, all the neighboring edges will be selected. dir . Determines whether to sample inbound ( :in ) or outbound (` :out ) edges (Default  :in ). replace . If  true , sample with replacement. dropnodes . If  true , the resulting subgraph will contain only the nodes involved in the sampled edges. Examples julia> g = rand_graph(20, 100)\nGNNGraph:\n    num_nodes = 20\n    num_edges = 100\n\njulia> sample_neighbors(g, 2:3)\nGNNGraph:\n    num_nodes = 20\n    num_edges = 9\n    edata:\n        EID => (9,)\n\njulia> sg = sample_neighbors(g, 2:3, dropnodes=true)\nGNNGraph:\n    num_nodes = 10\n    num_edges = 9\n    ndata:\n        NID => (10,)\n    edata:\n        EID => (9,)\n\njulia> sg.ndata.NID\n10-element Vector{Int64}:\n  2\n  3\n 17\n 14\n 18\n 15\n 16\n 20\n  7\n 10\n\njulia> sample_neighbors(g, 2:3, 5, replace=true)\nGNNGraph:\n    num_nodes = 20\n    num_edges = 10\n    edata:\n        EID => (10,) source"},{"id":78,"pagetitle":"GNNGraph","title":"Graphs.induced_subgraph","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/gnngraph/#Graphs.induced_subgraph-Tuple{GNNGraph, Vector{Int64}}","content":" Graphs.induced_subgraph  —  Method induced_subgraph(graph, nodes) Generates a subgraph from the original graph using the provided  nodes .  The function includes the nodes' neighbors and creates edges between nodes that are connected in the original graph.  If a node has no neighbors, an isolated node will be added to the subgraph.  Returns A new  GNNGraph  containing the subgraph with the specified nodes and their features. Arguments graph . The original GNNGraph containing nodes, edges, and node features. nodes `. A vector of node indices to include in the subgraph. Examples julia> s = [1, 2]\n2-element Vector{Int64}:\n 1\n 2\n\njulia> t = [2, 3]\n2-element Vector{Int64}:\n 2\n 3\n\njulia> graph = GNNGraph((s, t), ndata = (; x=rand(Float32, 32, 3), y=rand(Float32, 3)), edata = rand(Float32, 2))\nGNNGraph:\n  num_nodes: 3\n  num_edges: 2\n  ndata:\n        y = 3-element Vector{Float32}\n        x = 32×3 Matrix{Float32}\n  edata:\n        e = 2-element Vector{Float32}\n\njulia> nodes = [1, 2]\n2-element Vector{Int64}:\n 1\n 2\n\njulia> subgraph = Graphs.induced_subgraph(graph, nodes)\nGNNGraph:\n  num_nodes: 2\n  num_edges: 1\n  ndata:\n        y = 2-element Vector{Float32}\n        x = 32×2 Matrix{Float32}\n  edata:\n        e = 1-element Vector{Float32} source"},{"id":81,"pagetitle":"GNNHeteroGraph","title":"Heterogeneous Graphs","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/heterograph/#Heterogeneous-Graphs","content":" Heterogeneous Graphs"},{"id":82,"pagetitle":"GNNHeteroGraph","title":"GNNHeteroGraph","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/heterograph/#GNNHeteroGraph","content":" GNNHeteroGraph Documentation page for the type  GNNHeteroGraph  representing heterogeneous graphs, where  nodes and edges can have different types."},{"id":83,"pagetitle":"GNNHeteroGraph","title":"GNNGraphs.GNNHeteroGraph","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/heterograph/#GNNGraphs.GNNHeteroGraph","content":" GNNGraphs.GNNHeteroGraph  —  Type GNNHeteroGraph(data; [ndata, edata, gdata, num_nodes])\nGNNHeteroGraph(pairs...; [ndata, edata, gdata, num_nodes]) A type representing a heterogeneous graph structure. It is similar to  GNNGraph  but nodes and edges are of different types. Constructor Arguments data : A dictionary or an iterable object that maps  (source_type, edge_type, target_type)          triples to  (source, target)  index vectors (or to  (source, target, weight)  if also edge weights are present). pairs : Passing multiple relations as pairs is equivalent to passing  data=Dict(pairs...) . ndata : Node features. A dictionary of arrays or named tuple of arrays.          The size of the last dimension of each array must be given by  g.num_nodes . edata : Edge features. A dictionary of arrays or named tuple of arrays. Default  nothing .          The size of the last dimension of each array must be given by  g.num_edges . Default  nothing . gdata : Graph features. An array or named tuple of arrays whose last dimension has size  num_graphs . Default  nothing . num_nodes : The number of nodes for each type. If not specified, inferred from  data . Default  nothing . Fields graph : A dictionary that maps (source type, edge type, target_type) triples to (source, target) index vectors. num_nodes : The number of nodes for each type. num_edges : The number of edges for each type. ndata : Node features. edata : Edge features. gdata : Graph features. ntypes : The node types. etypes : The edge types. Examples julia> using GNNGraphs\n\njulia> nA, nB = 10, 20;\n\njulia> num_nodes = Dict(:A => nA, :B => nB);\n\njulia> edges1 = (rand(1:nA, 20), rand(1:nB, 20))\n([4, 8, 6, 3, 4, 7, 2, 7, 3, 2, 3, 4, 9, 4, 2, 9, 10, 1, 3, 9], [6, 4, 20, 8, 16, 7, 12, 16, 5, 4, 6, 20, 11, 19, 17, 9, 12, 2, 18, 12])\n\njulia> edges2 = (rand(1:nB, 30), rand(1:nA, 30))\n([17, 5, 2, 4, 5, 3, 8, 7, 9, 7  …  19, 8, 20, 7, 16, 2, 9, 15, 8, 13], [1, 1, 3, 1, 1, 3, 2, 7, 4, 4  …  7, 10, 6, 3, 4, 9, 1, 5, 8, 5])\n\njulia> data = ((:A, :rel1, :B) => edges1, (:B, :rel2, :A) => edges2);\n\njulia> hg = GNNHeteroGraph(data; num_nodes)\nGNNHeteroGraph:\n  num_nodes: (:A => 10, :B => 20)\n  num_edges: ((:A, :rel1, :B) => 20, (:B, :rel2, :A) => 30)\n\njulia> hg.num_edges\nDict{Tuple{Symbol, Symbol, Symbol}, Int64} with 2 entries:\n(:A, :rel1, :B) => 20\n(:B, :rel2, :A) => 30\n\n# Let's add some node features\njulia> ndata = Dict(:A => (x = rand(2, nA), y = rand(3, num_nodes[:A])),\n                    :B => rand(10, nB));\n\njulia> hg = GNNHeteroGraph(data; num_nodes, ndata)\nGNNHeteroGraph:\n    num_nodes: (:A => 10, :B => 20)\n    num_edges: ((:A, :rel1, :B) => 20, (:B, :rel2, :A) => 30)\n    ndata:\n    :A  =>  (x = 2×10 Matrix{Float64}, y = 3×10 Matrix{Float64})\n    :B  =>  x = 10×20 Matrix{Float64}\n\n# Access features of nodes of type :A\njulia> hg.ndata[:A].x\n2×10 Matrix{Float64}:\n    0.825882  0.0797502  0.245813  0.142281  0.231253  0.685025  0.821457  0.888838  0.571347   0.53165\n    0.631286  0.316292   0.705325  0.239211  0.533007  0.249233  0.473736  0.595475  0.0623298  0.159307 See also  GNNGraph  for a homogeneous graph type and  rand_heterograph  for a function to generate random heterographs. source"},{"id":84,"pagetitle":"GNNHeteroGraph","title":"GNNGraphs.edge_type_subgraph","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/heterograph/#GNNGraphs.edge_type_subgraph-Tuple{GNNHeteroGraph, Tuple{Symbol, Symbol, Symbol}}","content":" GNNGraphs.edge_type_subgraph  —  Method edge_type_subgraph(g::GNNHeteroGraph, edge_ts) Return a subgraph of  g  that contains only the edges of type  edge_ts . Edge types can be specified as a single edge type (i.e. a tuple containing 3 symbols) or a vector of edge types. source"},{"id":85,"pagetitle":"GNNHeteroGraph","title":"GNNGraphs.num_edge_types","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/heterograph/#GNNGraphs.num_edge_types-Tuple{GNNGraph}","content":" GNNGraphs.num_edge_types  —  Method num_edge_types(g) Return the number of edge types in the graph. For  GNNGraph s, this is always 1. For  GNNHeteroGraph s, this is the number of unique edge types. source"},{"id":86,"pagetitle":"GNNHeteroGraph","title":"GNNGraphs.num_node_types","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/heterograph/#GNNGraphs.num_node_types-Tuple{GNNGraph}","content":" GNNGraphs.num_node_types  —  Method num_node_types(g) Return the number of node types in the graph. For  GNNGraph s, this is always 1. For  GNNHeteroGraph s, this is the number of unique node types. source"},{"id":87,"pagetitle":"GNNHeteroGraph","title":"Query","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/heterograph/#Query","content":" Query"},{"id":88,"pagetitle":"GNNHeteroGraph","title":"GNNGraphs.edge_index","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/heterograph/#GNNGraphs.edge_index-Tuple{GNNHeteroGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}, Tuple{Symbol, Symbol, Symbol}}","content":" GNNGraphs.edge_index  —  Method edge_index(g::GNNHeteroGraph, [edge_t]) Return a tuple containing two vectors, respectively storing the source and target nodes for each edges in  g  of type  edge_t = (src_t, rel_t, trg_t) . If  edge_t  is not provided, it will error if  g  has more than one edge type. source"},{"id":89,"pagetitle":"GNNHeteroGraph","title":"GNNGraphs.graph_indicator","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/heterograph/#GNNGraphs.graph_indicator-Tuple{GNNHeteroGraph}","content":" GNNGraphs.graph_indicator  —  Method graph_indicator(g::GNNHeteroGraph, [node_t]) Return a Dict of vectors containing the graph membership (an integer from  1  to  g.num_graphs ) of each node in the graph for each node type. If  node_t  is provided, return the graph membership of each node of type  node_t  instead. See also  batch . source"},{"id":90,"pagetitle":"GNNHeteroGraph","title":"Graphs.degree","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/heterograph/#Graphs.degree-Union{Tuple{TT}, Tuple{GNNHeteroGraph, Tuple{Symbol, Symbol, Symbol}}, Tuple{GNNHeteroGraph, Tuple{Symbol, Symbol, Symbol}, TT}} where TT<:Union{Nothing, Type{<:Number}}","content":" Graphs.degree  —  Method degree(g::GNNHeteroGraph, edge_type::EType; dir = :in) Return a vector containing the degrees of the nodes in  g  GNNHeteroGraph given  edge_type . Arguments g : A graph. edge_type : A tuple of symbols  (source_t, edge_t, target_t)  representing the edge type. T : Element type of the returned vector. If  nothing , is      chosen based on the graph type. Default  nothing . dir : For  dir = :out  the degree of a node is counted based on the outgoing edges.        For  dir = :in , the ingoing edges are used. If  dir = :both  we have the sum of the two.        Default  dir = :out . source"},{"id":91,"pagetitle":"GNNHeteroGraph","title":"Graphs.has_edge","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/heterograph/#Graphs.has_edge-Tuple{GNNHeteroGraph, Tuple{Symbol, Symbol, Symbol}, Integer, Integer}","content":" Graphs.has_edge  —  Method has_edge(g::GNNHeteroGraph, edge_t, i, j) Return  true  if there is an edge of type  edge_t  from node  i  to node  j  in  g . Examples julia> g = rand_bipartite_heterograph((2, 2), (4, 0), bidirected=false)\nGNNHeteroGraph:\n  num_nodes: Dict(:A => 2, :B => 2)\n  num_edges: Dict((:A, :to, :B) => 4, (:B, :to, :A) => 0)\n\njulia> has_edge(g, (:A,:to,:B), 1, 1)\ntrue\n\njulia> has_edge(g, (:B,:to,:A), 1, 1)\nfalse source"},{"id":92,"pagetitle":"GNNHeteroGraph","title":"Transform","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/heterograph/#Transform","content":" Transform"},{"id":93,"pagetitle":"GNNHeteroGraph","title":"GNNGraphs.add_edges","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/heterograph/#GNNGraphs.add_edges-Tuple{GNNHeteroGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}, Tuple{Symbol, Symbol, Symbol}, AbstractVector, AbstractVector}","content":" GNNGraphs.add_edges  —  Method add_edges(g::GNNHeteroGraph, edge_t, s, t; [edata, num_nodes])\nadd_edges(g::GNNHeteroGraph, edge_t => (s, t); [edata, num_nodes])\nadd_edges(g::GNNHeteroGraph, edge_t => (s, t, w); [edata, num_nodes]) Add to heterograph  g  edges of type  edge_t  with source node vector  s  and target node vector  t . Optionally, pass the  edge weights  w  or the features   edata  for the new edges.  edge_t  is a triplet of symbols  (src_t, rel_t, dst_t) .  If the edge type is not already present in the graph, it is added.  If it involves new node types, they are added to the graph as well. In this case, a dictionary or named tuple of  num_nodes  can be passed to specify the number of nodes of the new types, otherwise the number of nodes is inferred from the maximum node id in  s  and  t . source"},{"id":94,"pagetitle":"GNNHeteroGraph","title":"GNNGraphs.add_self_loops","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/heterograph/#GNNGraphs.add_self_loops-Tuple{GNNHeteroGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}, Tuple{Symbol, Symbol, Symbol}}","content":" GNNGraphs.add_self_loops  —  Method add_self_loops(g::GNNHeteroGraph, edge_t::EType)\nadd_self_loops(g::GNNHeteroGraph) If the source node type is the same as the destination node type in  edge_t , return a graph with the same features as  g  but also add self-loops  of the specified type,  edge_t . Otherwise, it returns  g  unchanged. Nodes with already existing self-loops of type  edge_t  will obtain  a second set of self-loops of the same type. If the graph has edge weights for edges of type  edge_t , the new edges will have weight 1. If no edges of type  edge_t  exist, or all existing edges have no weight,  then all new self loops will have no weight. If  edge_t  is not passed as argument, for the entire graph self-loop is added to each node for every edge type in the graph where the source and destination node types are the same.  This iterates over all edge types present in the graph, applying the self-loop addition logic to each applicable edge type. source"},{"id":95,"pagetitle":"GNNHeteroGraph","title":"Generate","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/heterograph/#Generate","content":" Generate"},{"id":96,"pagetitle":"GNNHeteroGraph","title":"GNNGraphs.rand_bipartite_heterograph","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/heterograph/#GNNGraphs.rand_bipartite_heterograph-Tuple{Any, Any}","content":" GNNGraphs.rand_bipartite_heterograph  —  Method rand_bipartite_heterograph([rng,] \n                           (n1, n2), (m12, m21); \n                           bidirected = true, \n                           node_t = (:A, :B), \n                           edge_t = :to, \n                           kws...) Construct an  GNNHeteroGraph  with random edges representing a bipartite graph. The graph will have two types of nodes, and edges will only connect nodes of different types. The first argument is a tuple  (n1, n2)  specifying the number of nodes of each type. The second argument is a tuple  (m12, m21)  specifying the number of edges connecting nodes of type  1  to nodes of type  2   and vice versa. The type of nodes and edges can be specified with the  node_t  and  edge_t  keyword arguments, which default to  (:A, :B)  and  :to  respectively. If  bidirected=true  (default), the reverse edge of each edge will be present. In this case  m12 == m21  is required. A random number generator can be passed as the first argument to make the generation reproducible. Additional keyword arguments will be passed to the  GNNHeteroGraph  constructor. See  rand_heterograph  for a more general version. Examples julia> g = rand_bipartite_heterograph((10, 15), 20)\nGNNHeteroGraph:\n  num_nodes: (:A => 10, :B => 15)\n  num_edges: ((:A, :to, :B) => 20, (:B, :to, :A) => 20)\n\njulia> g = rand_bipartite_heterograph((10, 15), (20, 0), node_t=(:user, :item), edge_t=:-, bidirected=false)\nGNNHeteroGraph:\n  num_nodes: Dict(:item => 15, :user => 10)\n  num_edges: Dict((:item, :-, :user) => 0, (:user, :-, :item) => 20) source"},{"id":97,"pagetitle":"GNNHeteroGraph","title":"GNNGraphs.rand_heterograph","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/heterograph/#GNNGraphs.rand_heterograph","content":" GNNGraphs.rand_heterograph  —  Function rand_heterograph([rng,] n, m; bidirected=false, kws...) Construct an  GNNHeteroGraph  with random edges and with number of nodes and edges  specified by  n  and  m  respectively.  n  and  m  can be any iterable of pairs specifing node/edge types and their numbers. Pass a random number generator as a first argument to make the generation reproducible. Setting  bidirected=true  will generate a bidirected graph, i.e. each edge will have a reverse edge. Therefore, for each edge type  (:A, :rel, :B)  a corresponding reverse edge type  (:B, :rel, :A)  will be generated. Additional keyword arguments will be passed to the  GNNHeteroGraph  constructor. Examples julia> g = rand_heterograph((:user => 10, :movie => 20),\n                            (:user, :rate, :movie) => 30)\nGNNHeteroGraph:\n  num_nodes: Dict(:movie => 20, :user => 10)\n  num_edges: Dict((:user, :rate, :movie) => 30) source"},{"id":100,"pagetitle":"Samplers","title":"Samplers","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/samplers/#Samplers","content":" Samplers"},{"id":101,"pagetitle":"Samplers","title":"GNNGraphs.NeighborLoader","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/samplers/#GNNGraphs.NeighborLoader","content":" GNNGraphs.NeighborLoader  —  Type NeighborLoader(graph; num_neighbors, input_nodes, num_layers, [batch_size]) A data structure for sampling neighbors from a graph for training Graph Neural Networks (GNNs).  It supports multi-layer sampling of neighbors for a batch of input nodes, useful for mini-batch training originally introduced in [\"Inductive Representation Learning on Large Graphs\"}(https://arxiv.org/abs/1706.02216) paper. Fields graph::GNNGraph : The input graph. num_neighbors::Vector{Int} : A vector specifying the number of neighbors to sample per node at each GNN layer. input_nodes::Vector{Int} : A vector containing the starting nodes for neighbor sampling. num_layers::Int : The number of layers for neighborhood expansion (how far to sample neighbors). batch_size::Union{Int, Nothing} : The size of the batch. If not specified, it defaults to the number of  input_nodes . Examples julia> loader = NeighborLoader(graph; num_neighbors=[10, 5], input_nodes=[1, 2, 3], num_layers=2)\n\njulia> batch_counter = 0\n\njulia> for mini_batch_gnn in loader\n            batch_counter += 1\n            println(\"Batch \", batch_counter, \": Nodes in mini-batch graph: \", nv(mini_batch_gnn))\n        end source"},{"id":104,"pagetitle":"TemporalSnapshotsGNNGraph","title":"Temporal Graphs","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/temporalgraph/#Temporal-Graphs","content":" Temporal Graphs"},{"id":105,"pagetitle":"TemporalSnapshotsGNNGraph","title":"TemporalSnapshotsGNNGraph","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/temporalgraph/#TemporalSnapshotsGNNGraph","content":" TemporalSnapshotsGNNGraph Documentation page for the graph type  TemporalSnapshotsGNNGraph  and related methods, representing time varying graphs with time varying features."},{"id":106,"pagetitle":"TemporalSnapshotsGNNGraph","title":"GNNGraphs.TemporalSnapshotsGNNGraph","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/temporalgraph/#GNNGraphs.TemporalSnapshotsGNNGraph","content":" GNNGraphs.TemporalSnapshotsGNNGraph  —  Type TemporalSnapshotsGNNGraph(snapshots) A type representing a time-varying graph as a sequence of snapshots, each snapshot being a  GNNGraph . The argument  snapshots  is a collection of  GNNGraph s with arbitrary  number of nodes and edges each.  Calling  tg  the temporal graph,  tg[t]  returns the  t -th snapshot. The snapshots can contain node/edge/graph features, while global features for the whole temporal sequence can be stored in  tg.tgdata . See  add_snapshot  and  remove_snapshot  for adding and removing snapshots. Examples julia> snapshots = [rand_graph(i , 2*i) for i in 10:10:50];\n\njulia> tg = TemporalSnapshotsGNNGraph(snapshots)\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 20, 30, 40, 50]\n  num_edges: [20, 40, 60, 80, 100]\n  num_snapshots: 5\n\njulia> tg.num_snapshots\n5\n\njulia> tg.num_nodes\n5-element Vector{Int64}:\n 10\n 20\n 30\n 40\n 50\n\njulia> tg[1]\nGNNGraph:\n  num_nodes: 10\n  num_edges: 20\n\njulia> tg[2:3]\nTemporalSnapshotsGNNGraph:\n  num_nodes: [20, 30]\n  num_edges: [40, 60]\n  num_snapshots: 2\n\njulia> tg[1] = rand_graph(10, 16)\nGNNGraph:\n  num_nodes: 10\n  num_edges: 16 source"},{"id":107,"pagetitle":"TemporalSnapshotsGNNGraph","title":"GNNGraphs.add_snapshot","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/temporalgraph/#GNNGraphs.add_snapshot-Tuple{TemporalSnapshotsGNNGraph, Int64, GNNGraph}","content":" GNNGraphs.add_snapshot  —  Method add_snapshot(tg::TemporalSnapshotsGNNGraph, t::Int, g::GNNGraph) Return a  TemporalSnapshotsGNNGraph  created starting from  tg  by adding the snapshot  g  at time index  t . Examples julia> snapshots = [rand_graph(10, 20) for i in 1:5];\n\njulia> tg = TemporalSnapshotsGNNGraph(snapshots)\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 10, 10, 10, 10]\n  num_edges: [20, 20, 20, 20, 20]\n  num_snapshots: 5\n\njulia> new_tg = add_snapshot(tg, 3, rand_graph(10, 16)) # add a new snapshot at time 3\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 10, 10, 10, 10, 10]\n  num_edges: [20, 20, 16, 20, 20, 20]\n  num_snapshots: 6 source"},{"id":108,"pagetitle":"TemporalSnapshotsGNNGraph","title":"GNNGraphs.remove_snapshot","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/temporalgraph/#GNNGraphs.remove_snapshot-Tuple{TemporalSnapshotsGNNGraph, Int64}","content":" GNNGraphs.remove_snapshot  —  Method remove_snapshot(tg::TemporalSnapshotsGNNGraph, t::Int) Return a  TemporalSnapshotsGNNGraph  created starting from  tg  by removing the snapshot at time index  t . Examples julia> using GNNGraphs\n\njulia> snapshots = [rand_graph(10,20), rand_graph(10,14), rand_graph(10,22)];\n\njulia> tg = TemporalSnapshotsGNNGraph(snapshots)\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 10, 10]\n  num_edges: [20, 14, 22]\n  num_snapshots: 3\n\njulia> new_tg = remove_snapshot(tg, 2) # remove snapshot at time 2\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 10]\n  num_edges: [20, 22]\n  num_snapshots: 2 source"},{"id":109,"pagetitle":"TemporalSnapshotsGNNGraph","title":"Random Generators","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/temporalgraph/#Random-Generators","content":" Random Generators"},{"id":110,"pagetitle":"TemporalSnapshotsGNNGraph","title":"GNNGraphs.rand_temporal_radius_graph","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/temporalgraph/#GNNGraphs.rand_temporal_radius_graph","content":" GNNGraphs.rand_temporal_radius_graph  —  Function rand_temporal_radius_graph(number_nodes::Int, \n                           number_snapshots::Int,\n                           speed::AbstractFloat,\n                           r::AbstractFloat;\n                           self_loops = false,\n                           dir = :in,\n                           kws...) Create a random temporal graph given  number_nodes  nodes and  number_snapshots  snapshots. First, the positions of the nodes are randomly generated in the unit square. Two nodes are connected if their distance is less than a given radius  r . Each following snapshot is obtained by applying the same construction to new positions obtained as follows. For each snapshot, the new positions of the points are determined by applying random independent displacement vectors to the previous positions. The direction of the displacement is chosen uniformly at random and its length is chosen uniformly in  [0, speed] . Then the connections are recomputed. If a point happens to move outside the boundary, its position is updated as if it had bounced off the boundary. Arguments number_nodes : The number of nodes of each snapshot. number_snapshots : The number of snapshots. speed : The speed to update the nodes. r : The radius of connection. self_loops : If  true , consider the node itself among its neighbors, in which               case the graph will contain self-loops.  dir : The direction of the edges. If  dir=:in  edges go from the        neighbors to the central node. If  dir=:out  we have the opposite        direction. kws : Further keyword arguments will be passed to the  GNNGraph  constructor of each snapshot. Example julia> n, snaps, s, r = 10, 5, 0.1, 1.5;\n\njulia> tg = rand_temporal_radius_graph(n,snaps,s,r) # complete graph at each snapshot\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 10, 10, 10, 10]\n  num_edges: [90, 90, 90, 90, 90]\n  num_snapshots: 5 source"},{"id":111,"pagetitle":"TemporalSnapshotsGNNGraph","title":"GNNGraphs.rand_temporal_hyperbolic_graph","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/api/temporalgraph/#GNNGraphs.rand_temporal_hyperbolic_graph","content":" GNNGraphs.rand_temporal_hyperbolic_graph  —  Function rand_temporal_hyperbolic_graph(number_nodes::Int, \n                               number_snapshots::Int;\n                               α::Real,\n                               R::Real,\n                               speed::Real,\n                               ζ::Real=1,\n                               self_loop = false,\n                               kws...) Create a random temporal graph given  number_nodes  nodes and  number_snapshots  snapshots. First, the positions of the nodes are generated with a quasi-uniform distribution (depending on the parameter  α ) in hyperbolic space within a disk of radius  R . Two nodes are connected if their hyperbolic distance is less than  R . Each following snapshot is created in order to keep the same initial distribution. Arguments number_nodes : The number of nodes of each snapshot. number_snapshots : The number of snapshots. α : The parameter that controls the position of the points. If  α=ζ , the points are uniformly distributed on the disk of radius  R . If  α>ζ , the points are more concentrated in the center of the disk. If  α<ζ , the points are more concentrated at the boundary of the disk. R : The radius of the disk and of connection. speed : The speed to update the nodes. ζ : The parameter that controls the curvature of the disk. self_loops : If  true , consider the node itself among its neighbors, in which               case the graph will contain self-loops. kws : Further keyword arguments will be passed to the  GNNGraph  constructor of each snapshot. Example julia> n, snaps, α, R, speed, ζ = 10, 5, 1.0, 4.0, 0.1, 1.0;\n\njulia> thg = rand_temporal_hyperbolic_graph(n, snaps; α, R, speed, ζ)\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 10, 10, 10, 10]\n  num_edges: [44, 46, 48, 42, 38]\n  num_snapshots: 5 References Section D of the paper  Dynamic Hidden-Variable Network Models  and the paper   Hyperbolic Geometry of Complex Networks source"},{"id":114,"pagetitle":"Datasets","title":"Datasets","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/guides/datasets/#Datasets","content":" Datasets GNNGraphs.jl doesn't come with its own datasets, but leverages those available in the Julia (and non-Julia) ecosystem. In particular, the  examples in the GraphNeuralNetworks.jl repository  make use of the  MLDatasets.jl  package. There you will find common graph datasets such as Cora, PubMed, Citeseer, TUDataset and  many others . For graphs with static structures and temporal features, datasets such as METRLA, PEMSBAY, ChickenPox, and WindMillEnergy are available. For graphs featuring both temporal structures and temporal features, the TemporalBrains dataset is suitable. GraphNeuralNetworks.jl provides the  mldataset2gnngraph  method for interfacing with MLDatasets.jl."},{"id":117,"pagetitle":"Graphs","title":"Static Graphs","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/guides/gnngraph/#Static-Graphs","content":" Static Graphs The fundamental graph type in GNNGraphs.jl is the  GNNGraph . A GNNGraph  g  is a directed graph with nodes labeled from 1 to  g.num_nodes . The underlying implementation allows for efficient application of graph neural network operators, gpu movement, and storage of node/edge/graph related feature arrays. GNNGraph  inherits from  Graphs.jl 's  AbstractGraph , therefore it supports most functionality from that library. "},{"id":118,"pagetitle":"Graphs","title":"Graph Creation","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/guides/gnngraph/#Graph-Creation","content":" Graph Creation A GNNGraph can be created from several different data sources encoding the graph topology: using GNNGraphs, Graphs, SparseArrays\n\n\n# Construct a GNNGraph from from a Graphs.jl's graph\nlg = erdos_renyi(10, 30)\ng = GNNGraph(lg)\n\n# Same as above using convenience method rand_graph\ng = rand_graph(10, 60)\n\n# From an adjacency matrix\nA = sprand(10, 10, 0.3)\ng = GNNGraph(A)\n\n# From an adjacency list\nadjlist = [[2,3], [1,3], [1,2,4], [3]]\ng = GNNGraph(adjlist)\n\n# From COO representation\nsource = [1,1,2,2,3,3,3,4]\ntarget = [2,3,1,3,1,2,4,3]\ng = GNNGraph(source, target) See also the related methods  Graphs.adjacency_matrix ,  edge_index , and  adjacency_list ."},{"id":119,"pagetitle":"Graphs","title":"Basic Queries","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/guides/gnngraph/#Basic-Queries","content":" Basic Queries julia> source = [1,1,2,2,3,3,3,4];\n\njulia> target = [2,3,1,3,1,2,4,3];\n\njulia> g = GNNGraph(source, target)\nGNNGraph:\n  num_nodes: 4\n  num_edges: 8\n\n\njulia> @assert g.num_nodes == 4   # number of nodes\n\njulia> @assert g.num_edges == 8   # number of edges\n\njulia> @assert g.num_graphs == 1  # number of subgraphs (a GNNGraph can batch many graphs together)\n\njulia> is_directed(g)      # a GNNGraph is always directed\ntrue\n\njulia> is_bidirected(g)      # for each edge, also the reverse edge is present\ntrue\n\njulia> has_self_loops(g)\nfalse\n\njulia> has_multi_edges(g)      \nfalse"},{"id":120,"pagetitle":"Graphs","title":"Data Features","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/guides/gnngraph/#Data-Features","content":" Data Features One or more arrays can be associated to nodes, edges, and (sub)graphs of a  GNNGraph . They will be stored in the fields  g.ndata ,  g.edata , and  g.gdata  respectively. The data fields are  DataStore  objects.  DataStore s conveniently offer an interface similar to both dictionaries and named tuples. Similarly to dictionaries, DataStores support addition of new features after creation time. The array contained in the datastores have last dimension equal to  num_nodes  (in  ndata ),  num_edges  (in  edata ), or  num_graphs  (in  gdata ) respectively. # Create a graph with a single feature array `x` associated to nodes\ng = rand_graph(10,  60, ndata = (; x = rand(Float32, 32, 10)))\n\ng.ndata.x  # access the features\n\n# Equivalent definition passing directly the array\ng = rand_graph(10,  60, ndata = rand(Float32, 32, 10))\n\ng.ndata.x  # `:x` is the default name for node features\n\ng.ndata.z = rand(Float32, 3, 10)  # add new feature array `z`\n\n# For convenience, we can access the features through the shortcut\ng.x \n\n# You can have multiple feature arrays\ng = rand_graph(10,  60, ndata = (; x=rand(Float32, 32, 10), y=rand(Float32, 10)))\n\ng.ndata.y, g.ndata.x   # or g.x, g.y\n\n# Attach an array with edge features.\n# Since `GNNGraph`s are directed, the number of edges\n# will be double that of the original Graphs' undirected graph.\ng = GNNGraph(erdos_renyi(10,  30), edata = rand(Float32, 60))\n@assert g.num_edges == 60\n\ng.edata.e  # or g.e\n\n# If we pass only half of the edge features, they will be copied\n# on the reversed edges.\ng = GNNGraph(erdos_renyi(10,  30), edata = rand(Float32, 30))\n\n\n# Create a new graph from previous one, inheriting edge data\n# but replacing node data\ng′ = GNNGraph(g, ndata =(; z = ones(Float32, 16, 10)))\n\ng′.z\ng′.e"},{"id":121,"pagetitle":"Graphs","title":"Edge weights","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/guides/gnngraph/#Edge-weights","content":" Edge weights It is common to denote scalar edge features as edge weights. The  GNNGraph  has specific support for edge weights: they can be stored as part of internal representations of the graph (COO or adjacency matrix). Some graph convolutional layers, most notably the  GCNConv , can use the edge weights to perform weighted sums over the nodes' neighborhoods. julia> source = [1, 1, 2, 2, 3, 3];\n\njulia> target = [2, 3, 1, 3, 1, 2];\n\njulia> weight = [1.0, 0.5, 2.1, 2.3, 4, 4.1];\n\njulia> g = GNNGraph(source, target, weight)\nGNNGraph:\n  num_nodes: 3\n  num_edges: 6\n\njulia> get_edge_weight(g)\n6-element Vector{Float64}:\n 1.0\n 0.5\n 2.1\n 2.3\n 4.0\n 4.1"},{"id":122,"pagetitle":"Graphs","title":"Batches and Subgraphs","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/guides/gnngraph/#Batches-and-Subgraphs","content":" Batches and Subgraphs Multiple  GNNGraph s can be batched together into a single graph that contains the total number of the original nodes  and where the original graphs are disjoint subgraphs. using MLUtils\n\ndata = [rand_graph(10, 30, ndata=rand(Float32, 3, 10)) for _ in 1:160]\ngall = MLUtils.batch(data)\n\n# gall is a GNNGraph containing many graphs\n@assert gall.num_graphs == 160 \n@assert gall.num_nodes == 1600   # 10 nodes x 160 graphs\n@assert gall.num_edges == 4800  # 30 undirected edges x 160 graphs\n\n# Let's create a mini-batch from gall\ng23 = getgraph(gall, 2:3)\n@assert g23.num_graphs == 2\n@assert g23.num_nodes == 20   # 10 nodes x 2 graphs\n@assert g23.num_edges == 60  # 30 undirected edges X 2 graphs\n\n# We can pass a GNNGraph to MLUtils' DataLoader\ntrain_loader = DataLoader(gall, batchsize=16, shuffle=true)\n\nfor g in train_loader\n    @assert g.num_graphs == 16\n    @assert g.num_nodes == 160\n    @assert size(g.ndata.x) = (3, 160)    \n    # .....\nend\n\n# Access the nodes' graph memberships \ngraph_indicator(gall)"},{"id":123,"pagetitle":"Graphs","title":"DataLoader and mini-batch iteration","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/guides/gnngraph/#DataLoader-and-mini-batch-iteration","content":" DataLoader and mini-batch iteration While constructing a batched graph and passing it to the  DataLoader  is always  an option for mini-batch iteration, the recommended way for better performance is to pass an array of graphs directly and set the  collate  option to  true : using MLUtils: DataLoader\n\ndata = [rand_graph(10, 30, ndata=rand(Float32, 3, 10)) for _ in 1:320]\n\ntrain_loader = DataLoader(data, batchsize=16, shuffle=true, collate=true)\n\nfor g in train_loader\n    @assert g.num_graphs == 16\n    @assert g.num_nodes == 160\n    @assert size(g.ndata.x) = (3, 160)    \n    # .....\nend"},{"id":124,"pagetitle":"Graphs","title":"Graph Manipulation","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/guides/gnngraph/#Graph-Manipulation","content":" Graph Manipulation g′ = add_self_loops(g)\ng′ = remove_self_loops(g)\ng′ = add_edges(g, [1, 2], [2, 3]) # add edges 1->2 and 2->3"},{"id":125,"pagetitle":"Graphs","title":"GPU movement","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/guides/gnngraph/#GPU-movement","content":" GPU movement Move a  GNNGraph  to a CUDA device using  Flux.gpu  method.  using Flux, CUDA # or using Metal or using AMDGPU \n\ng_gpu = g |> Flux.gpu"},{"id":126,"pagetitle":"Graphs","title":"Integration with Graphs.jl","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/guides/gnngraph/#Integration-with-Graphs.jl","content":" Integration with Graphs.jl Since  GNNGraph <: Graphs.AbstractGraph , we can use any functionality from  Graphs.jl  for querying and analyzing the graph structure.  Moreover, a  GNNGraph  can be easily constructed from a  Graphs.Graph  or a  Graphs.DiGraph : julia> import Graphs\n\njulia> using GNNGraphs\n\n# A Graphs.jl undirected graph\njulia> gu = Graphs.erdos_renyi(10, 20)    \n{10, 20} undirected simple Int64 graph\n\n# Since GNNGraphs are undirected, the edges are doubled when converting \n# to GNNGraph\njulia> GNNGraph(gu)\nGNNGraph:\n  num_nodes: 10\n  num_edges: 40\n\n# A Graphs.jl directed graph\njulia> gd = Graphs.erdos_renyi(10, 20, is_directed=true)\n{10, 20} directed simple Int64 graph\n\njulia> GNNGraph(gd)\nGNNGraph:\n  num_nodes: 10\n  num_edges: 20"},{"id":129,"pagetitle":"Heterogeneous Graphs","title":"Heterogeneous Graphs","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/guides/heterograph/#Heterogeneous-Graphs","content":" Heterogeneous Graphs Heterogeneous graphs (also called heterographs), are graphs where each node has a type, that we denote with symbols such as  :user  and  :movie . Relations such as  :rate  or  :like  can connect nodes of different types. We call a triplet  (source_node_type, relation_type, target_node_type)  the type of a edge, e.g.  (:user, :rate, :movie) . Different node/edge types can store different groups of features and this makes heterographs a very flexible modeling tools  and data containers. In GNNGraphs.jl heterographs are implemented in  the type  GNNHeteroGraph ."},{"id":130,"pagetitle":"Heterogeneous Graphs","title":"Creating a Heterograph","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/guides/heterograph/#Creating-a-Heterograph","content":" Creating a Heterograph A heterograph can be created empty or by passing pairs  edge_type => data  to the constructor. julia> using GNNGraphs\n\njulia> g = GNNHeteroGraph()\nGNNHeteroGraph:\n  num_nodes: Dict()\n  num_edges: Dict()\n  \njulia> g = GNNHeteroGraph((:user, :like, :actor) => ([1,2,2,3], [1,3,2,9]),\n                          (:user, :rate, :movie) => ([1,1,2,3], [7,13,5,7]))\nGNNHeteroGraph:\n  num_nodes: Dict(:actor => 9, :movie => 13, :user => 3)\n  num_edges: Dict((:user, :like, :actor) => 4, (:user, :rate, :movie) => 4)\n\njulia> g = GNNHeteroGraph((:user, :rate, :movie) => ([1,1,2,3], [7,13,5,7]))\nGNNHeteroGraph:\n  num_nodes: Dict(:movie => 13, :user => 3)\n  num_edges: Dict((:user, :rate, :movie) => 4) New relations, possibly with new node types, can be added with the function  add_edges . julia> g = add_edges(g, (:user, :like, :actor) => ([1,2,3,3,3], [3,5,1,9,4]))\nGNNHeteroGraph:\n  num_nodes: Dict(:actor => 9, :movie => 13, :user => 3)\n  num_edges: Dict((:user, :like, :actor) => 5, (:user, :rate, :movie) => 4) See  rand_heterograph ,  rand_bipartite_heterograph  for generating random heterographs.  julia> g = rand_bipartite_heterograph((10, 15), 20)\nGNNHeteroGraph:\n  num_nodes: Dict(:A => 10, :B => 15)\n  num_edges: Dict((:A, :to, :B) => 20, (:B, :to, :A) => 20)"},{"id":131,"pagetitle":"Heterogeneous Graphs","title":"Basic Queries","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/guides/heterograph/#Basic-Queries","content":" Basic Queries Basic queries are similar to those for homogeneous graphs: julia> g = GNNHeteroGraph((:user, :rate, :movie) => ([1,1,2,3], [7,13,5,7]))\nGNNHeteroGraph:\n  num_nodes: Dict(:movie => 13, :user => 3)\n  num_edges: Dict((:user, :rate, :movie) => 4)\n\njulia> g.num_nodes\nDict{Symbol, Int64} with 2 entries:\n  :user  => 3\n  :movie => 13\n\njulia> g.num_edges\nDict{Tuple{Symbol, Symbol, Symbol}, Int64} with 1 entry:\n  (:user, :rate, :movie) => 4\n\njulia> edge_index(g, (:user, :rate, :movie)) # source and target node for a given relation\n([1, 1, 2, 3], [7, 13, 5, 7])\n\njulia> g.ntypes  # node types\n2-element Vector{Symbol}:\n :user\n :movie\n\njulia> g.etypes  # edge types\n1-element Vector{Tuple{Symbol, Symbol, Symbol}}:\n (:user, :rate, :movie)"},{"id":132,"pagetitle":"Heterogeneous Graphs","title":"Data Features","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/guides/heterograph/#Data-Features","content":" Data Features Node, edge, and graph features can be added at construction time or later using: # equivalent to g.ndata[:user][:x] = ...\njulia> g[:user].x = rand(Float32, 64, 3);\n\njulia> g[:movie].z = rand(Float32, 64, 13);\n\n# equivalent to g.edata[(:user, :rate, :movie)][:e] = ...\njulia> g[:user, :rate, :movie].e = rand(Float32, 64, 4);\n\njulia> g\nGNNHeteroGraph:\n  num_nodes: Dict(:movie => 13, :user => 3)\n  num_edges: Dict((:user, :rate, :movie) => 4)\n  ndata:\n        :movie  =>  DataStore(z = [64×13 Matrix{Float32}])\n        :user  =>  DataStore(x = [64×3 Matrix{Float32}])\n  edata:\n        (:user, :rate, :movie)  =>  DataStore(e = [64×4 Matrix{Float32}])"},{"id":133,"pagetitle":"Heterogeneous Graphs","title":"Batching","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/guides/heterograph/#Batching","content":" Batching Similarly to graphs, also heterographs can be batched together. julia> gs = [rand_bipartite_heterograph((5, 10), 20) for _ in 1:32];\n\njulia> MLUtils.batch(gs)\nGNNHeteroGraph:\n  num_nodes: Dict(:A => 160, :B => 320)\n  num_edges: Dict((:A, :to, :B) => 640, (:B, :to, :A) => 640)\n  num_graphs: 32 Batching is automatically performed by the  DataLoader  iterator when the  collate  option is set to  true . using MLUtils: DataLoader\n\ndata = [rand_bipartite_heterograph((5, 10), 20, \n            ndata=Dict(:A=>rand(Float32, 3, 5))) \n        for _ in 1:320];\n\ntrain_loader = DataLoader(data, batchsize=16, shuffle=true, collate=true)\n\nfor g in train_loader\n    @assert g.num_graphs == 16\n    @assert g.num_nodes[:A] == 80\n    @assert size(g.ndata[:A].x) == (3, 80)    \n    # ...\nend"},{"id":134,"pagetitle":"Heterogeneous Graphs","title":"Graph convolutions on heterographs","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/guides/heterograph/#Graph-convolutions-on-heterographs","content":" Graph convolutions on heterographs See  HeteroGraphConv  for how to perform convolutions on heterogeneous graphs."},{"id":137,"pagetitle":"Temporal Graphs","title":"Temporal Graphs","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/guides/temporalgraph/#Temporal-Graphs","content":" Temporal Graphs Temporal graphs are graphs with time-varying topologies and features. In GNNGraphs.jl, they are represented by the  TemporalSnapshotsGNNGraph  type."},{"id":138,"pagetitle":"Temporal Graphs","title":"Creating a TemporalSnapshotsGNNGraph","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/guides/temporalgraph/#Creating-a-TemporalSnapshotsGNNGraph","content":" Creating a TemporalSnapshotsGNNGraph A temporal graph can be created by passing a list of snapshots to the constructor. Each snapshot is a  GNNGraph .  julia> using GNNGraphs\n\njulia> snapshots = [rand_graph(10, 20) for i in 1:5];\n\njulia> tg = TemporalSnapshotsGNNGraph(snapshots)\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 10, 10, 10, 10]\n  num_edges: [20, 20, 20, 20, 20]\n  num_snapshots: 5 A new temporal graph can be created by adding or removing snapshots to an existing temporal graph.  julia> new_tg = add_snapshot(tg, 3, rand_graph(10, 16)) # add a new snapshot at time 3\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 10, 10, 10, 10, 10]\n  num_edges: [20, 20, 16, 20, 20, 20]\n  num_snapshots: 6 julia> snapshots = [rand_graph(10,20), rand_graph(10,14), rand_graph(10,22)];\n\njulia> tg = TemporalSnapshotsGNNGraph(snapshots)\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 10, 10]\n  num_edges: [20, 14, 22]\n  num_snapshots: 3\n\njulia> new_tg = remove_snapshot(tg, 2) # remove snapshot at time 2\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 10]\n  num_edges: [20, 22]\n  num_snapshots: 2 See  rand_temporal_radius_graph  and  rand_temporal_hyperbolic_graph  for generating random temporal graphs.  julia> tg = rand_temporal_radius_graph(10, 3, 0.1, 0.5)\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 10, 10]\n  num_edges: [32, 30, 34]\n  num_snapshots: 3"},{"id":139,"pagetitle":"Temporal Graphs","title":"Indexing","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/guides/temporalgraph/#Indexing","content":" Indexing Snapshots in a temporal graph can be accessed using indexing: julia> snapshots = [rand_graph(10, 20), rand_graph(10, 14), rand_graph(10, 22)];\n\njulia> tg = TemporalSnapshotsGNNGraph(snapshots)\n\njulia> tg[1] # first snapshot\nGNNGraph:\n  num_nodes: 10\n  num_edges: 20\n\njulia> tg[2:3] # snapshots 2 and 3\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 10]\n  num_edges: [14, 22]\n  num_snapshots: 2 A snapshot can be modified by assigning a new snapshot to the temporal graph: julia> tg[1] = rand_graph(10, 16) # replace first snapshot\nGNNGraph:\n  num_nodes: 10\n  num_edges: 16"},{"id":140,"pagetitle":"Temporal Graphs","title":"Iteration and Broadcasting","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/guides/temporalgraph/#Iteration-and-Broadcasting","content":" Iteration and Broadcasting Iteration and broadcasting over a temporal graph is similar to that of a vector of snapshots: julia> snapshots = [rand_graph(10, 20), rand_graph(10, 14), rand_graph(10, 22)];\n\njulia> tg = TemporalSnapshotsGNNGraph(snapshots);\n\njulia> [g for g in tg] # iterate over snapshots\n3-element Vector{GNNGraph{Tuple{Vector{Int64}, Vector{Int64}, Nothing}}}:\n GNNGraph(10, 20) with no data\n GNNGraph(10, 14) with no data\n GNNGraph(10, 22) with no data\n\njulia> f(g) = g isa GNNGraph;\n\njulia> f.(tg) # broadcast over snapshots\n3-element BitVector:\n 1\n 1\n 1"},{"id":141,"pagetitle":"Temporal Graphs","title":"Basic Queries","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/guides/temporalgraph/#Basic-Queries","content":" Basic Queries Basic queries are similar to those for  GNNGraph s: julia> snapshots = [rand_graph(10,20), rand_graph(12,14), rand_graph(14,22)];\n\njulia> tg = TemporalSnapshotsGNNGraph(snapshots)\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 12, 14]\n  num_edges: [20, 14, 22]\n  num_snapshots: 3\n\njulia> tg.num_nodes         # number of nodes in each snapshot\n3-element Vector{Int64}:\n 10\n 12\n 14\n\njulia> tg.num_edges         # number of edges in each snapshot\n3-element Vector{Int64}:\n 20\n 14\n 22\n\njulia> tg.num_snapshots     # number of snapshots\n3\n\njulia> tg.snapshots         # list of snapshots\n3-element Vector{GNNGraph{Tuple{Vector{Int64}, Vector{Int64}, Nothing}}}:\n GNNGraph(10, 20) with no data\n GNNGraph(12, 14) with no data\n GNNGraph(14, 22) with no data\n\njulia> tg.snapshots[1]      # first snapshot, same as tg[1]\nGNNGraph:\n  num_nodes: 10\n  num_edges: 20"},{"id":142,"pagetitle":"Temporal Graphs","title":"Data Features","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNGraphs/guides/temporalgraph/#Data-Features","content":" Data Features A temporal graph can store global feature for the entire time series in the  tgdata  field. Also, each snapshot can store node, edge, and graph features in the  ndata ,  edata , and  gdata  fields, respectively.  julia> snapshots = [rand_graph(10, 20; ndata = rand(Float32, 3, 10)), \n                    rand_graph(10, 14; ndata = rand(Float32, 4, 10)), \n                    rand_graph(10, 22; ndata = rand(Float32, 5, 10))]; # node features at construction time\n\njulia> tg = TemporalSnapshotsGNNGraph(snapshots);\n\njulia> tg.tgdata.y = rand(Float32, 3, 1); # add global features after construction\n\njulia> tg\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 10, 10]\n  num_edges: [20, 14, 22]\n  num_snapshots: 3\n  tgdata:\n        y = 3×1 Matrix{Float32}\n\njulia> tg.ndata # vector of DataStore containing node features for each snapshot\n3-element Vector{DataStore}:\n DataStore(10) with 1 element:\n  x = 3×10 Matrix{Float32}\n DataStore(10) with 1 element:\n  x = 4×10 Matrix{Float32}\n DataStore(10) with 1 element:\n  x = 5×10 Matrix{Float32}\n\njulia> [ds.x for ds in tg.ndata]; # vector containing the x feature of each snapshot\n\njulia> [g.x for g in tg.snapshots]; # same vector as above, now accessing \n                                   # the x feature directly from the snapshots"},{"id":145,"pagetitle":"GNNlib.jl","title":"GNNlib.jl","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNlib/#GNNlib.jl","content":" GNNlib.jl GNNlib.jl is a package that provides the implementation of the basic message passing functions and  functional implementation of graph convolutional layers, which are used to build graph neural networks in both the  Flux.jl  and  Lux.jl  machine learning frameworks, created in the GraphNeuralNetworks.jl and GNNLux.jl packages, respectively. This package depends on GNNGraphs.jl and NNlib.jl, and is primarily intended for developers looking to create new GNN architectures. For most users, the higher-level GraphNeuralNetworks.jl and GNNLux.jl packages are recommended."},{"id":146,"pagetitle":"GNNlib.jl","title":"Installation","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNlib/#Installation","content":" Installation The package can be installed with the Julia package manager. From the Julia REPL, type  ]  to enter the Pkg REPL mode and run: pkg> add GNNlib"},{"id":149,"pagetitle":"Message Passing","title":"Message Passing","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNlib/api/messagepassing/#Message-Passing","content":" Message Passing"},{"id":150,"pagetitle":"Message Passing","title":"Interface","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNlib/api/messagepassing/#Interface","content":" Interface"},{"id":151,"pagetitle":"Message Passing","title":"GNNlib.apply_edges","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNlib/api/messagepassing/#GNNlib.apply_edges","content":" GNNlib.apply_edges  —  Function apply_edges(fmsg, g; [xi, xj, e])\napply_edges(fmsg, g, xi, xj, e=nothing) Returns the message from node  j  to node  i  applying the message function  fmsg  on the edges in graph  g . In the message-passing scheme, the incoming messages  from the neighborhood of  i  will later be aggregated in order to update the features of node  i  (see  aggregate_neighbors ). The function  fmsg  operates on batches of edges, therefore  xi ,  xj , and  e  are tensors whose last dimension is the batch size, or can be named tuples of  such tensors. Arguments g : An  AbstractGNNGraph . xi : An array or a named tuple containing arrays whose last dimension's size        is  g.num_nodes . It will be appropriately materialized on the       target node of each edge (see also  edge_index ). xj : As  xi , but now to be materialized on each edge's source node.  e : An array or a named tuple containing arrays whose last dimension's size is  g.num_edges . fmsg : A function that takes as inputs the edge-materialized  xi ,  xj , and  e .      These are arrays (or named tuples of arrays) whose last dimension' size is the size of      a batch of edges. The output of  f  has to be an array (or a named tuple of arrays)      with the same batch size. If also  layer  is passed to propagate,     the signature of  fmsg  has to be  fmsg(layer, xi, xj, e)       instead of  fmsg(xi, xj, e) . See also  propagate  and  aggregate_neighbors . source"},{"id":152,"pagetitle":"Message Passing","title":"GNNlib.aggregate_neighbors","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNlib/api/messagepassing/#GNNlib.aggregate_neighbors","content":" GNNlib.aggregate_neighbors  —  Function aggregate_neighbors(g, aggr, m) Given a graph  g , edge features  m , and an aggregation operator  aggr  (e.g  +, min, max, mean ), returns the new node features  \\[\\mathbf{x}_i = \\square_{j \\in \\mathcal{N}(i)} \\mathbf{m}_{j\\to i}\\] Neighborhood aggregation is the second step of  propagate ,  where it comes after  apply_edges . source"},{"id":153,"pagetitle":"Message Passing","title":"GNNlib.propagate","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNlib/api/messagepassing/#GNNlib.propagate","content":" GNNlib.propagate  —  Function propagate(fmsg, g, aggr; [xi, xj, e])\npropagate(fmsg, g, aggr xi, xj, e=nothing) Performs message passing on graph  g . Takes care of materializing the node features on each edge,  applying the message function  fmsg , and returning an aggregated message  $\\bar{\\mathbf{m}}$   (depending on the return value of  fmsg , an array or a named tuple of  arrays with last dimension's size  g.num_nodes ). It can be decomposed in two steps: m = apply_edges(fmsg, g, xi, xj, e)\nm̄ = aggregate_neighbors(g, aggr, m) GNN layers typically call  propagate  in their forward pass, providing as input  f  a closure.   Arguments g : A  GNNGraph . xi : An array or a named tuple containing arrays whose last dimension's size        is  g.num_nodes . It will be appropriately materialized on the       target node of each edge (see also  edge_index ). xj : As  xj , but to be materialized on edges' sources.  e : An array or a named tuple containing arrays whose last dimension's size is  g.num_edges . fmsg : A generic function that will be passed over to  apply_edges .      Has to take as inputs the edge-materialized  xi ,  xj , and  e       (arrays or named tuples of arrays whose last dimension' size is the size of      a batch of edges). Its output has to be an array or a named tuple of arrays     with the same batch size. If also  layer  is passed to propagate,     the signature of  fmsg  has to be  fmsg(layer, xi, xj, e)       instead of  fmsg(xi, xj, e) . aggr : Neighborhood aggregation operator. Use  + ,  mean ,  max , or  min .  Examples using GraphNeuralNetworks, Flux\n\nstruct GNNConv <: GNNLayer\n    W\n    b\n    σ\nend\n\nFlux.@layer GNNConv\n\nfunction GNNConv(ch::Pair{Int,Int}, σ=identity)\n    in, out = ch\n    W = Flux.glorot_uniform(out, in)\n    b = zeros(Float32, out)\n    GNNConv(W, b, σ)\nend\n\nfunction (l::GNNConv)(g::GNNGraph, x::AbstractMatrix)\n    message(xi, xj, e) = l.W * xj\n    m̄ = propagate(message, g, +, xj=x)\n    return l.σ.(m̄ .+ l.bias)\nend\n\nl = GNNConv(10 => 20)\nl(g, x) See also  apply_edges  and  aggregate_neighbors . source"},{"id":154,"pagetitle":"Message Passing","title":"Built-in message functions","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNlib/api/messagepassing/#Built-in-message-functions","content":" Built-in message functions"},{"id":155,"pagetitle":"Message Passing","title":"GNNlib.copy_xi","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNlib/api/messagepassing/#GNNlib.copy_xi","content":" GNNlib.copy_xi  —  Function copy_xi(xi, xj, e) = xi source"},{"id":156,"pagetitle":"Message Passing","title":"GNNlib.copy_xj","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNlib/api/messagepassing/#GNNlib.copy_xj","content":" GNNlib.copy_xj  —  Function copy_xj(xi, xj, e) = xj source"},{"id":157,"pagetitle":"Message Passing","title":"GNNlib.xi_dot_xj","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNlib/api/messagepassing/#GNNlib.xi_dot_xj","content":" GNNlib.xi_dot_xj  —  Function xi_dot_xj(xi, xj, e) = sum(xi .* xj, dims=1) source"},{"id":158,"pagetitle":"Message Passing","title":"GNNlib.xi_sub_xj","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNlib/api/messagepassing/#GNNlib.xi_sub_xj","content":" GNNlib.xi_sub_xj  —  Function xi_sub_xj(xi, xj, e) = xi .- xj source"},{"id":159,"pagetitle":"Message Passing","title":"GNNlib.xj_sub_xi","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNlib/api/messagepassing/#GNNlib.xj_sub_xi","content":" GNNlib.xj_sub_xi  —  Function xj_sub_xi(xi, xj, e) = xj .- xi source"},{"id":160,"pagetitle":"Message Passing","title":"GNNlib.e_mul_xj","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNlib/api/messagepassing/#GNNlib.e_mul_xj","content":" GNNlib.e_mul_xj  —  Function e_mul_xj(xi, xj, e) = reshape(e, (...)) .* xj Reshape  e  into a broadcast compatible shape with  xj  (by prepending singleton dimensions) then perform broadcasted multiplication. source"},{"id":161,"pagetitle":"Message Passing","title":"GNNlib.w_mul_xj","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNlib/api/messagepassing/#GNNlib.w_mul_xj","content":" GNNlib.w_mul_xj  —  Function w_mul_xj(xi, xj, w) = reshape(w, (...)) .* xj Similar to  e_mul_xj  but specialized on scalar edge features (weights). source"},{"id":164,"pagetitle":"Other Operators","title":"Utility Functions","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNlib/api/utils/#Utility-Functions","content":" Utility Functions"},{"id":165,"pagetitle":"Other Operators","title":"Graph-wise operations","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNlib/api/utils/#Graph-wise-operations","content":" Graph-wise operations"},{"id":166,"pagetitle":"Other Operators","title":"GNNlib.reduce_nodes","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNlib/api/utils/#GNNlib.reduce_nodes","content":" GNNlib.reduce_nodes  —  Function reduce_nodes(aggr, g, x) For a batched graph  g , return the graph-wise aggregation of the node features  x . The aggregation operator  aggr  can be  + ,  mean ,  max , or  min . The returned array will have last dimension  g.num_graphs . See also:  reduce_edges . source reduce_nodes(aggr, indicator::AbstractVector, x) Return the graph-wise aggregation of the node features  x  given the graph indicator  indicator . The aggregation operator  aggr  can be  + ,  mean ,  max , or  min . See also  graph_indicator . source"},{"id":167,"pagetitle":"Other Operators","title":"GNNlib.reduce_edges","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNlib/api/utils/#GNNlib.reduce_edges","content":" GNNlib.reduce_edges  —  Function reduce_edges(aggr, g, e) For a batched graph  g , return the graph-wise aggregation of the edge features  e . The aggregation operator  aggr  can be  + ,  mean ,  max , or  min . The returned array will have last dimension  g.num_graphs . source"},{"id":168,"pagetitle":"Other Operators","title":"GNNlib.softmax_nodes","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNlib/api/utils/#GNNlib.softmax_nodes","content":" GNNlib.softmax_nodes  —  Function softmax_nodes(g, x) Graph-wise softmax of the node features  x . source"},{"id":169,"pagetitle":"Other Operators","title":"GNNlib.softmax_edges","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNlib/api/utils/#GNNlib.softmax_edges","content":" GNNlib.softmax_edges  —  Function softmax_edges(g, e) Graph-wise softmax of the edge features  e . source"},{"id":170,"pagetitle":"Other Operators","title":"GNNlib.broadcast_nodes","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNlib/api/utils/#GNNlib.broadcast_nodes","content":" GNNlib.broadcast_nodes  —  Function broadcast_nodes(g, x) Graph-wise broadcast array  x  of size  (*, g.num_graphs)   to size  (*, g.num_nodes) . source"},{"id":171,"pagetitle":"Other Operators","title":"GNNlib.broadcast_edges","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNlib/api/utils/#GNNlib.broadcast_edges","content":" GNNlib.broadcast_edges  —  Function broadcast_edges(g, x) Graph-wise broadcast array  x  of size  (*, g.num_graphs)   to size  (*, g.num_edges) . source"},{"id":172,"pagetitle":"Other Operators","title":"Neighborhood operations","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNlib/api/utils/#Neighborhood-operations","content":" Neighborhood operations"},{"id":173,"pagetitle":"Other Operators","title":"GNNlib.softmax_edge_neighbors","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNlib/api/utils/#GNNlib.softmax_edge_neighbors","content":" GNNlib.softmax_edge_neighbors  —  Function softmax_edge_neighbors(g, e) Softmax over each node's neighborhood of the edge features  e . \\[\\mathbf{e}'_{j\\to i} = \\frac{e^{\\mathbf{e}_{j\\to i}}}\n                    {\\sum_{j'\\in N(i)} e^{\\mathbf{e}_{j'\\to i}}}.\\] source"},{"id":174,"pagetitle":"Other Operators","title":"NNlib's gather and scatter functions","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNlib/api/utils/#NNlib's-gather-and-scatter-functions","content":" NNlib's gather and scatter functions Primitive functions for message passing implemented in  NNlib.jl : gather! gather scatter! scatter"},{"id":177,"pagetitle":"Message Passing","title":"Message Passing","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNlib/guides/messagepassing/#Message-Passing","content":" Message Passing A generic message passing on graph takes the form \\[\\begin{aligned}\n\\mathbf{m}_{j\\to i} &= \\phi(\\mathbf{x}_i, \\mathbf{x}_j, \\mathbf{e}_{j\\to i}) \\\\\n\\bar{\\mathbf{m}}_{i} &= \\square_{j\\in N(i)}  \\mathbf{m}_{j\\to i} \\\\\n\\mathbf{x}_{i}' &= \\gamma_x(\\mathbf{x}_{i}, \\bar{\\mathbf{m}}_{i})\\\\\n\\mathbf{e}_{j\\to i}^\\prime &=  \\gamma_e(\\mathbf{e}_{j \\to i},\\mathbf{m}_{j \\to i})\n\\end{aligned}\\] where we refer to  $\\phi$  as to the message function,  and to  $\\gamma_x$  and  $\\gamma_e$  as to the node update and edge update function respectively. The aggregation  $\\square$  is over the neighborhood  $N(i)$  of node  $i$ ,  and it is usually equal either to  $\\sum$ , to  max  or to a  mean  operation.  In GNNlib.jl, the message passing mechanism is exposed by the  propagate  function.  propagate  takes care of materializing the node features on each edge, applying the message function, performing the aggregation, and returning  $\\bar{\\mathbf{m}}$ .  It is then left to the user to perform further node and edge updates, manipulating arrays of size  $D_{node} \\times num\\_nodes$  and     $D_{edge} \\times num\\_edges$ . propagate  is composed of two steps, also available as two independent methods: apply_edges  materializes node features on edges and applies the message function.  aggregate_neighbors  applies a reduction operator on the messages coming from the neighborhood of each node. The whole propagation mechanism internally relies on the  NNlib.gather   and  NNlib.scatter  methods."},{"id":178,"pagetitle":"Message Passing","title":"Examples","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNlib/guides/messagepassing/#Examples","content":" Examples"},{"id":179,"pagetitle":"Message Passing","title":"Basic use of apply_edges and propagate","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNlib/guides/messagepassing/#Basic-use-of-apply_edges-and-propagate","content":" Basic use of apply_edges and propagate The function  apply_edges  can be used to broadcast node data on each edge and produce new edge data. julia> using GNNlib, Graphs, Statistics\n\njulia> g = rand_graph(10, 20)\nGNNGraph:\n    num_nodes = 10\n    num_edges = 20\n\njulia> x = ones(2,10);\n\njulia> z = 2ones(2,10);\n\n# Return an edge features arrays (D × num_edges)\njulia> apply_edges((xi, xj, e) -> xi .+ xj, g, xi=x, xj=z)\n2×20 Matrix{Float64}:\n 3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0\n 3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0\n\n# now returning a named tuple\njulia> apply_edges((xi, xj, e) -> (a=xi .+ xj, b=xi .- xj), g, xi=x, xj=z)\n(a = [3.0 3.0 … 3.0 3.0; 3.0 3.0 … 3.0 3.0], b = [-1.0 -1.0 … -1.0 -1.0; -1.0 -1.0 … -1.0 -1.0])\n\n# Here we provide a named tuple input\njulia> apply_edges((xi, xj, e) -> xi.a + xi.b .* xj, g, xi=(a=x,b=z), xj=z)\n2×20 Matrix{Float64}:\n 5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0\n 5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0 The function  propagate  instead performs the  apply_edges  operation but then also applies a reduction over each node's neighborhood (see  aggregate_neighbors ). julia> propagate((xi, xj, e) -> xi .+ xj, g, +, xi=x, xj=z)\n2×10 Matrix{Float64}:\n 3.0  6.0  9.0  9.0  0.0  6.0  6.0  3.0  15.0  3.0\n 3.0  6.0  9.0  9.0  0.0  6.0  6.0  3.0  15.0  3.0\n\n# Previous output can be understood by looking at the degree\njulia> degree(g)\n10-element Vector{Int64}:\n 1\n 2\n 3\n 3\n 0\n 2\n 2\n 1\n 5\n 1"},{"id":180,"pagetitle":"Message Passing","title":"Implementing a custom Graph Convolutional Layer using Flux.jl","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNlib/guides/messagepassing/#Implementing-a-custom-Graph-Convolutional-Layer-using-Flux.jl","content":" Implementing a custom Graph Convolutional Layer using Flux.jl Let's implement a simple graph convolutional layer using the message passing framework using the machine learning framework Flux.jl. The convolution reads  \\[\\mathbf{x}'_i = W \\cdot \\sum_{j \\in N(i)}  \\mathbf{x}_j\\] We will also add a bias and an activation function. using Flux, Graphs, GraphNeuralNetworks\n\nstruct GCN{A<:AbstractMatrix, B, F} <: GNNLayer\n    weight::A\n    bias::B\n    σ::F\nend\n\nFlux.@layer GCN # allow gpu movement, select trainable params etc...\n\nfunction GCN(ch::Pair{Int,Int}, σ=identity)\n    in, out = ch\n    W = Flux.glorot_uniform(out, in)\n    b = zeros(Float32, out)\n    GCN(W, b, σ)\nend\n\nfunction (l::GCN)(g::GNNGraph, x::AbstractMatrix{T}) where T\n    @assert size(x, 2) == g.num_nodes\n\n    # Computes messages from source/neighbour nodes (j) to target/root nodes (i).\n    # The message function will have to handle matrices of size (*, num_edges).\n    # In this simple case we just let the neighbor features go through.\n    message(xi, xj, e) = xj \n\n    # The + operator gives the sum aggregation.\n    # `mean`, `max`, `min`, and `*` are other possibilities.\n    x = propagate(message, g, +, xj=x) \n\n    return l.σ.(l.weight * x .+ l.bias)\nend See the  GATConv  implementation  here  for a more complex example."},{"id":181,"pagetitle":"Message Passing","title":"Built-in message functions","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/GNNlib/guides/messagepassing/#Built-in-message-functions","content":" Built-in message functions In order to exploit optimized specializations of the  propagate , it is recommended  to use built-in message functions such as  copy_xj  whenever possible. "},{"id":184,"pagetitle":"Basic layers","title":"Basic Layers","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/api/basic/#Basic-Layers","content":" Basic Layers"},{"id":185,"pagetitle":"Basic layers","title":"GraphNeuralNetworks.DotDecoder","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/api/basic/#GraphNeuralNetworks.DotDecoder","content":" GraphNeuralNetworks.DotDecoder  —  Type DotDecoder() A graph neural network layer that  for given input graph  g  and node features  x , returns the dot product  x_i ⋅ xj  on each edge.  Examples julia> g = rand_graph(5, 6)\nGNNGraph:\n    num_nodes = 5\n    num_edges = 6\n\njulia> dotdec = DotDecoder()\nDotDecoder()\n\njulia> dotdec(g, rand(2, 5))\n1×6 Matrix{Float64}:\n 0.345098  0.458305  0.106353  0.345098  0.458305  0.106353 source"},{"id":186,"pagetitle":"Basic layers","title":"GraphNeuralNetworks.GNNChain","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/api/basic/#GraphNeuralNetworks.GNNChain","content":" GraphNeuralNetworks.GNNChain  —  Type GNNChain(layers...)\nGNNChain(name = layer, ...) Collects multiple layers / functions to be called in sequence on given input graph and input node features.  It allows to compose layers in a sequential fashion as  Flux.Chain  does, propagating the output of each layer to the next one. In addition,  GNNChain  handles the input graph as well, providing it  as a first argument only to layers subtyping the  GNNLayer  abstract type.  GNNChain  supports indexing and slicing,  m[2]  or  m[1:end-1] , and if names are given,  m[:name] == m[1]  etc. Examples julia> using Flux, GraphNeuralNetworks\n\njulia> m = GNNChain(GCNConv(2=>5), \n                    BatchNorm(5), \n                    x -> relu.(x), \n                    Dense(5, 4))\nGNNChain(GCNConv(2 => 5), BatchNorm(5), #7, Dense(5 => 4))\n\njulia> x = randn(Float32, 2, 3);\n\njulia> g = rand_graph(3, 6)\nGNNGraph:\n    num_nodes = 3\n    num_edges = 6\n\njulia> m(g, x)\n4×3 Matrix{Float32}:\n    -0.795592  -0.795592  -0.795592\n    -0.736409  -0.736409  -0.736409\n    0.994925   0.994925   0.994925\n    0.857549   0.857549   0.857549\n\njulia> m2 = GNNChain(enc = m, \n                     dec = DotDecoder())\nGNNChain(enc = GNNChain(GCNConv(2 => 5), BatchNorm(5), #7, Dense(5 => 4)), dec = DotDecoder())\n\njulia> m2(g, x)\n1×6 Matrix{Float32}:\n 2.90053  2.90053  2.90053  2.90053  2.90053  2.90053\n\njulia> m2[:enc](g, x) == m(g, x)\ntrue source"},{"id":187,"pagetitle":"Basic layers","title":"GraphNeuralNetworks.GNNLayer","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/api/basic/#GraphNeuralNetworks.GNNLayer","content":" GraphNeuralNetworks.GNNLayer  —  Type abstract type GNNLayer end An abstract type from which graph neural network layers are derived. See also  GNNChain . source"},{"id":188,"pagetitle":"Basic layers","title":"GraphNeuralNetworks.WithGraph","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/api/basic/#GraphNeuralNetworks.WithGraph","content":" GraphNeuralNetworks.WithGraph  —  Type WithGraph(model, g::GNNGraph; traingraph=false) A type wrapping the  model  and tying it to the graph  g . In the forward pass, can only take feature arrays as inputs, returning  model(g, x...; kws...) . If  traingraph=false , the graph's parameters won't be part of  the  trainable  parameters in the gradient updates. Examples g = GNNGraph([1,2,3], [2,3,1])\nx = rand(Float32, 2, 3)\nmodel = SAGEConv(2 => 3)\nwg = WithGraph(model, g)\n# No need to feed the graph to `wg`\n@assert wg(x) == model(g, x)\n\ng2 = GNNGraph([1,1,2,3], [2,4,1,1])\nx2 = rand(Float32, 2, 4)\n# WithGraph will ignore the internal graph if fed with a new one. \n@assert wg(g2, x2) == model(g2, x2) source"},{"id":191,"pagetitle":"Convolutional layers","title":"Convolutional Layers","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/api/conv/#Convolutional-Layers","content":" Convolutional Layers Many different types of graphs convolutional layers have been proposed in the literature. Choosing the right layer for your application could involve a lot of exploration.  Some of the most commonly used layers are the  GCNConv  and the  GATv2Conv . Multiple graph convolutional layers are typically stacked together to create a graph neural network model (see  GNNChain ). The table below lists all graph convolutional layers implemented in the  GraphNeuralNetworks.jl . It also highlights the presence of some additional capabilities with respect to basic message passing: Sparse Ops : implements message passing as multiplication by sparse adjacency matrix instead of the gather/scatter mechanism. This can lead to better CPU performances but it is not supported on GPU yet.  Edge Weight : supports scalar weights (or equivalently scalar features) on edges.  Edge Features : supports feature vectors on edges. Heterograph : supports heterogeneous graphs (see  GNNHeteroGraph ). TemporalSnapshotsGNNGraphs : supports temporal graphs (see  TemporalSnapshotsGNNGraph ) by applying the convolution layers to each snapshot independently. Layer Sparse Ops Edge Weight Edge Features Heterograph TemporalSnapshotsGNNGraphs AGNNConv ✓ CGConv ✓ ✓ ✓ ChebConv ✓ EGNNConv ✓ EdgeConv ✓ GATConv ✓ ✓ ✓ GATv2Conv ✓ ✓ ✓ GatedGraphConv ✓ ✓ GCNConv ✓ ✓ ✓ GINConv ✓ ✓ ✓ GMMConv ✓ GraphConv ✓ ✓ ✓ MEGNetConv ✓ NNConv ✓ ResGatedGraphConv ✓ ✓ SAGEConv ✓ ✓ ✓ SGConv ✓ ✓ TransformerConv ✓"},{"id":192,"pagetitle":"Convolutional layers","title":"GraphNeuralNetworks.AGNNConv","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/api/conv/#GraphNeuralNetworks.AGNNConv","content":" GraphNeuralNetworks.AGNNConv  —  Type AGNNConv(; init_beta=1.0f0, trainable=true, add_self_loops=true) Attention-based Graph Neural Network layer from paper  Attention-based Graph Neural Network for Semi-Supervised Learning . The forward pass is given by \\[\\mathbf{x}_i' = \\sum_{j \\in N(i)} \\alpha_{ij} \\mathbf{x}_j\\] where the attention coefficients  $\\alpha_{ij}$  are given by \\[\\alpha_{ij} =\\frac{e^{\\beta \\cos(\\mathbf{x}_i, \\mathbf{x}_j)}}\n                  {\\sum_{j'}e^{\\beta \\cos(\\mathbf{x}_i, \\mathbf{x}_{j'})}}\\] with the cosine distance defined by \\[\\cos(\\mathbf{x}_i, \\mathbf{x}_j) = \n  \\frac{\\mathbf{x}_i \\cdot \\mathbf{x}_j}{\\lVert\\mathbf{x}_i\\rVert \\lVert\\mathbf{x}_j\\rVert}\\] and  $\\beta$  a trainable parameter if  trainable=true . Arguments init_beta : The initial value of  $\\beta$ . Default 1.0f0. trainable : If true,  $\\beta$  is trainable. Default  true . add_self_loops : Add self loops to the graph before performing the convolution. Default  true . Examples: # create data\ns = [1,1,2,3]\nt = [2,3,1,1]\ng = GNNGraph(s, t)\n\n# create layer\nl = AGNNConv(init_beta=2.0f0)\n\n# forward pass\ny = l(g, x)    source"},{"id":193,"pagetitle":"Convolutional layers","title":"GraphNeuralNetworks.CGConv","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/api/conv/#GraphNeuralNetworks.CGConv","content":" GraphNeuralNetworks.CGConv  —  Type CGConv((in, ein) => out, act=identity; bias=true, init=glorot_uniform, residual=false)\nCGConv(in => out, ...) The crystal graph convolutional layer from the paper  Crystal Graph Convolutional Neural Networks for an Accurate and Interpretable Prediction of Material Properties . Performs the operation \\[\\mathbf{x}_i' = \\mathbf{x}_i + \\sum_{j\\in N(i)}\\sigma(W_f \\mathbf{z}_{ij} + \\mathbf{b}_f)\\, act(W_s \\mathbf{z}_{ij} + \\mathbf{b}_s)\\] where  $\\mathbf{z}_{ij}$   is the node and edge features concatenation   $[\\mathbf{x}_i; \\mathbf{x}_j; \\mathbf{e}_{j\\to i}]$   and  $\\sigma$  is the sigmoid function. The residual  $\\mathbf{x}_i$  is added only if  residual=true  and the output size is the same  as the input size. Arguments in : The dimension of input node features. ein : The dimension of input edge features.  If  ein  is not given, assumes that no edge features are passed as input in the forward pass. out : The dimension of output node features. act : Activation function. bias : Add learnable bias. init : Weights' initializer. residual : Add a residual connection. Examples g = rand_graph(5, 6)\nx = rand(Float32, 2, g.num_nodes)\ne = rand(Float32, 3, g.num_edges)\n\nl = CGConv((2, 3) => 4, tanh)\ny = l(g, x, e)    # size: (4, num_nodes)\n\n# No edge features\nl = CGConv(2 => 4, tanh)\ny = l(g, x)    # size: (4, num_nodes) source"},{"id":194,"pagetitle":"Convolutional layers","title":"GraphNeuralNetworks.ChebConv","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/api/conv/#GraphNeuralNetworks.ChebConv","content":" GraphNeuralNetworks.ChebConv  —  Type ChebConv(in => out, k; bias=true, init=glorot_uniform) Chebyshev spectral graph convolutional layer from paper  Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering . Implements \\[X' = \\sum^{K-1}_{k=0}  W^{(k)} Z^{(k)}\\] where  $Z^{(k)}$  is the  $k$ -th term of Chebyshev polynomials, and can be calculated by the following recursive form: \\[\\begin{aligned}\nZ^{(0)} &= X \\\\\nZ^{(1)} &= \\hat{L} X \\\\\nZ^{(k)} &= 2 \\hat{L} Z^{(k-1)} - Z^{(k-2)}\n\\end{aligned}\\] with  $\\hat{L}$  the  scaled_laplacian . Arguments in : The dimension of input features. out : The dimension of output features. k : The order of Chebyshev polynomial. bias : Add learnable bias. init : Weights' initializer. Examples # create data\ns = [1,1,2,3]\nt = [2,3,1,1]\ng = GNNGraph(s, t)\nx = randn(Float32, 3, g.num_nodes)\n\n# create layer\nl = ChebConv(3 => 5, 5) \n\n# forward pass\ny = l(g, x)       # size:  5 × num_nodes source"},{"id":195,"pagetitle":"Convolutional layers","title":"GraphNeuralNetworks.DConv","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/api/conv/#GraphNeuralNetworks.DConv","content":" GraphNeuralNetworks.DConv  —  Type DConv(ch::Pair{Int, Int}, k::Int; init = glorot_uniform, bias = true) Diffusion convolution layer from the paper  Diffusion Convolutional Recurrent Neural Networks: Data-Driven Traffic Forecasting . Arguments ch : Pair of input and output dimensions. k : Number of diffusion steps. init : Weights' initializer. Default  glorot_uniform . bias : Add learnable bias. Default  true . Examples julia> g = GNNGraph(rand(10, 10), ndata = rand(Float32, 2, 10));\n\njulia> dconv = DConv(2 => 4, 4)\nDConv(2 => 4, 4)\n\njulia> y = dconv(g, g.ndata.x);\n\njulia> size(y)\n(4, 10) source"},{"id":196,"pagetitle":"Convolutional layers","title":"GraphNeuralNetworks.EGNNConv","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/api/conv/#GraphNeuralNetworks.EGNNConv","content":" GraphNeuralNetworks.EGNNConv  —  Type EGNNConv((in, ein) => out; hidden_size=2in, residual=false)\nEGNNConv(in => out; hidden_size=2in, residual=false) Equivariant Graph Convolutional Layer from  E(n) Equivariant Graph Neural Networks . The layer performs the following operation: \\[\\begin{aligned}\n\\mathbf{m}_{j\\to i} &=\\phi_e(\\mathbf{h}_i, \\mathbf{h}_j, \\lVert\\mathbf{x}_i-\\mathbf{x}_j\\rVert^2, \\mathbf{e}_{j\\to i}),\\\\\n\\mathbf{x}_i' &= \\mathbf{x}_i + C_i\\sum_{j\\in\\mathcal{N}(i)}(\\mathbf{x}_i-\\mathbf{x}_j)\\phi_x(\\mathbf{m}_{j\\to i}),\\\\\n\\mathbf{m}_i &= C_i\\sum_{j\\in\\mathcal{N}(i)} \\mathbf{m}_{j\\to i},\\\\\n\\mathbf{h}_i' &= \\mathbf{h}_i + \\phi_h(\\mathbf{h}_i, \\mathbf{m}_i)\n\\end{aligned}\\] where  $\\mathbf{h}_i$ ,  $\\mathbf{x}_i$ ,  $\\mathbf{e}_{j\\to i}$  are invariant node features, equivariant node features, and edge features respectively.  $\\phi_e$ ,  $\\phi_h$ , and  $\\phi_x$  are two-layer MLPs.  C  is a constant for normalization, computed as  $1/|\\mathcal{N}(i)|$ . Constructor Arguments in : Number of input features for  h . out : Number of output features for  h . ein : Number of input edge features. hidden_size : Hidden representation size. residual : If  true , add a residual connection. Only possible if  in == out . Default  false . Forward Pass l(g, x, h, e=nothing) Forward Pass Arguments: g  : The graph. x  : Matrix of equivariant node coordinates. h  : Matrix of invariant node features. e  : Matrix of invariant edge features. Default  nothing . Returns updated  h  and  x . Examples g = rand_graph(10, 10)\nh = randn(Float32, 5, g.num_nodes)\nx = randn(Float32, 3, g.num_nodes)\negnn = EGNNConv(5 => 6, 10)\nhnew, xnew = egnn(g, h, x) source"},{"id":197,"pagetitle":"Convolutional layers","title":"GraphNeuralNetworks.EdgeConv","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/api/conv/#GraphNeuralNetworks.EdgeConv","content":" GraphNeuralNetworks.EdgeConv  —  Type EdgeConv(nn; aggr=max) Edge convolutional layer from paper  Dynamic Graph CNN for Learning on Point Clouds . Performs the operation \\[\\mathbf{x}_i' = \\square_{j \\in N(i)}\\, nn([\\mathbf{x}_i; \\mathbf{x}_j - \\mathbf{x}_i])\\] where  nn  generally denotes a learnable function, e.g. a linear layer or a multi-layer perceptron. Arguments nn : A (possibly learnable) function.  aggr : Aggregation operator for the incoming messages (e.g.  + ,  * ,  max ,  min , and  mean ). Examples: # create data\ns = [1,1,2,3]\nt = [2,3,1,1]\nin_channel = 3\nout_channel = 5\ng = GNNGraph(s, t)\n\n# create layer\nl = EdgeConv(Dense(2 * in_channel, out_channel), aggr = +)\n\n# forward pass\ny = l(g, x) source"},{"id":198,"pagetitle":"Convolutional layers","title":"GraphNeuralNetworks.GATConv","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/api/conv/#GraphNeuralNetworks.GATConv","content":" GraphNeuralNetworks.GATConv  —  Type GATConv(in => out, [σ; heads, concat, init, bias, negative_slope, add_self_loops])\nGATConv((in, ein) => out, ...) Graph attentional layer from the paper  Graph Attention Networks . Implements the operation \\[\\mathbf{x}_i' = \\sum_{j \\in N(i) \\cup \\{i\\}} \\alpha_{ij} W \\mathbf{x}_j\\] where the attention coefficients  $\\alpha_{ij}$  are given by \\[\\alpha_{ij} = \\frac{1}{z_i} \\exp(LeakyReLU(\\mathbf{a}^T [W \\mathbf{x}_i; W \\mathbf{x}_j]))\\] with  $z_i$  a normalization factor.  In case  ein > 0  is given, edge features of dimension  ein  will be expected in the forward pass  and the attention coefficients will be calculated as   \\[\\alpha_{ij} = \\frac{1}{z_i} \\exp(LeakyReLU(\\mathbf{a}^T [W_e \\mathbf{e}_{j\\to i}; W \\mathbf{x}_i; W \\mathbf{x}_j]))\\] Arguments in : The dimension of input node features. ein : The dimension of input edge features. Default 0 (i.e. no edge features passed in the forward). out : The dimension of output node features. σ : Activation function. Default  identity . bias : Learn the additive bias if true. Default  true . heads : Number attention heads. Default  1 . concat : Concatenate layer output or not. If not, layer output is averaged over the heads. Default  true . negative_slope : The parameter of LeakyReLU.Default  0.2 . add_self_loops : Add self loops to the graph before performing the convolution. Default  true . dropout : Dropout probability on the normalized attention coefficient. Default  0.0 . Examples # create data\ns = [1,1,2,3]\nt = [2,3,1,1]\nin_channel = 3\nout_channel = 5\ng = GNNGraph(s, t)\nx = randn(Float32, 3, g.num_nodes)\n\n# create layer\nl = GATConv(in_channel => out_channel, add_self_loops = false, bias = false; heads=2, concat=true)\n\n# forward pass\ny = l(g, x)        source"},{"id":199,"pagetitle":"Convolutional layers","title":"GraphNeuralNetworks.GATv2Conv","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/api/conv/#GraphNeuralNetworks.GATv2Conv","content":" GraphNeuralNetworks.GATv2Conv  —  Type GATv2Conv(in => out, [σ; heads, concat, init, bias, negative_slope, add_self_loops])\nGATv2Conv((in, ein) => out, ...) GATv2 attentional layer from the paper  How Attentive are Graph Attention Networks? . Implements the operation \\[\\mathbf{x}_i' = \\sum_{j \\in N(i) \\cup \\{i\\}} \\alpha_{ij} W_1 \\mathbf{x}_j\\] where the attention coefficients  $\\alpha_{ij}$  are given by \\[\\alpha_{ij} = \\frac{1}{z_i} \\exp(\\mathbf{a}^T LeakyReLU(W_2 \\mathbf{x}_i + W_1 \\mathbf{x}_j))\\] with  $z_i$  a normalization factor. In case  ein > 0  is given, edge features of dimension  ein  will be expected in the forward pass  and the attention coefficients will be calculated as   \\[\\alpha_{ij} = \\frac{1}{z_i} \\exp(\\mathbf{a}^T LeakyReLU(W_3 \\mathbf{e}_{j\\to i} + W_2 \\mathbf{x}_i + W_1 \\mathbf{x}_j)).\\] Arguments in : The dimension of input node features. ein : The dimension of input edge features. Default 0 (i.e. no edge features passed in the forward). out : The dimension of output node features. σ : Activation function. Default  identity . bias : Learn the additive bias if true. Default  true . heads : Number attention heads. Default  1 . concat : Concatenate layer output or not. If not, layer output is averaged over the heads. Default  true . negative_slope : The parameter of LeakyReLU.Default  0.2 . add_self_loops : Add self loops to the graph before performing the convolution. Default  true . dropout : Dropout probability on the normalized attention coefficient. Default  0.0 . Examples # create data\ns = [1,1,2,3]\nt = [2,3,1,1]\nin_channel = 3\nout_channel = 5\nein = 3\ng = GNNGraph(s, t)\nx = randn(Float32, 3, g.num_nodes)\n\n# create layer\nl = GATv2Conv((in_channel, ein) => out_channel, add_self_loops = false)\n\n# edge features\ne = randn(Float32, ein, length(s))\n\n# forward pass\ny = l(g, x, e)     source"},{"id":200,"pagetitle":"Convolutional layers","title":"GraphNeuralNetworks.GCNConv","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/api/conv/#GraphNeuralNetworks.GCNConv","content":" GraphNeuralNetworks.GCNConv  —  Type GCNConv(in => out, σ=identity; [bias, init, add_self_loops, use_edge_weight]) Graph convolutional layer from paper  Semi-supervised Classification with Graph Convolutional Networks . Performs the operation \\[\\mathbf{x}'_i = \\sum_{j\\in N(i)} a_{ij} W \\mathbf{x}_j\\] where  $a_{ij} = 1 / \\sqrt{|N(i)||N(j)|}$  is a normalization factor computed from the node degrees.  If the input graph has weighted edges and  use_edge_weight=true , than  $a_{ij}$  will be computed as \\[a_{ij} = \\frac{e_{j\\to i}}{\\sqrt{\\sum_{j \\in N(i)}  e_{j\\to i}} \\sqrt{\\sum_{i \\in N(j)}  e_{i\\to j}}}\\] The input to the layer is a node feature array  X  of size  (num_features, num_nodes)  and optionally an edge weight vector. Arguments in : Number of input features. out : Number of output features. σ : Activation function. Default  identity . bias : Add learnable bias. Default  true . init : Weights' initializer. Default  glorot_uniform . add_self_loops : Add self loops to the graph before performing the convolution. Default  false . use_edge_weight : If  true , consider the edge weights in the input graph (if available).                    If  add_self_loops=true  the new weights will be set to 1.                     This option is ignored if the  edge_weight  is explicitly provided in the forward pass.                    Default  false . Forward (::GCNConv)(g::GNNGraph, x, edge_weight = nothing; norm_fn = d -> 1 ./ sqrt.(d), conv_weight = nothing) -> AbstractMatrix Takes as input a graph  g , a node feature matrix  x  of size  [in, num_nodes] , and optionally an edge weight vector. Returns a node feature matrix of size   [out, num_nodes] . The  norm_fn  parameter allows for custom normalization of the graph convolution operation by passing a function as argument.  By default, it computes  $\\frac{1}{\\sqrt{d}}$  i.e the inverse square root of the degree ( d ) of each node in the graph.  If  conv_weight  is an  AbstractMatrix  of size  [out, in] , then the convolution is performed using that weight matrix instead of the weights stored in the model. Examples # create data\ns = [1,1,2,3]\nt = [2,3,1,1]\ng = GNNGraph(s, t)\nx = randn(Float32, 3, g.num_nodes)\n\n# create layer\nl = GCNConv(3 => 5) \n\n# forward pass\ny = l(g, x)       # size:  5 × num_nodes\n\n# convolution with edge weights and custom normalization function\nw = [1.1, 0.1, 2.3, 0.5]\ncustom_norm_fn(d) = 1 ./ sqrt.(d + 1)  # Custom normalization function\ny = l(g, x, w; norm_fn = custom_norm_fn)\n\n# Edge weights can also be embedded in the graph.\ng = GNNGraph(s, t, w)\nl = GCNConv(3 => 5, use_edge_weight=true) \ny = l(g, x) # same as l(g, x, w)  source"},{"id":201,"pagetitle":"Convolutional layers","title":"GraphNeuralNetworks.GINConv","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/api/conv/#GraphNeuralNetworks.GINConv","content":" GraphNeuralNetworks.GINConv  —  Type GINConv(f, ϵ; aggr=+) Graph Isomorphism convolutional layer from paper  How Powerful are Graph Neural Networks? . Implements the graph convolution \\[\\mathbf{x}_i' = f_\\Theta\\left((1 + \\epsilon) \\mathbf{x}_i + \\sum_{j \\in N(i)} \\mathbf{x}_j \\right)\\] where  $f_\\Theta$  typically denotes a learnable function, e.g. a linear layer or a multi-layer perceptron. Arguments f : A (possibly learnable) function acting on node features.  ϵ : Weighting factor. Examples: # create data\ns = [1,1,2,3]\nt = [2,3,1,1]\nin_channel = 3\nout_channel = 5\ng = GNNGraph(s, t)\n\n# create dense layer\nnn = Dense(in_channel, out_channel)\n\n# create layer\nl = GINConv(nn, 0.01f0, aggr = mean)\n\n# forward pass\ny = l(g, x)   source"},{"id":202,"pagetitle":"Convolutional layers","title":"GraphNeuralNetworks.GMMConv","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/api/conv/#GraphNeuralNetworks.GMMConv","content":" GraphNeuralNetworks.GMMConv  —  Type GMMConv((in, ein) => out, σ=identity; K=1, bias=true, init=glorot_uniform, residual=false) Graph mixture model convolution layer from the paper  Geometric deep learning on graphs and manifolds using mixture model CNNs  Performs the operation \\[\\mathbf{x}_i' = \\mathbf{x}_i + \\frac{1}{|N(i)|} \\sum_{j\\in N(i)}\\frac{1}{K}\\sum_{k=1}^K \\mathbf{w}_k(\\mathbf{e}_{j\\to i}) \\odot \\Theta_k \\mathbf{x}_j\\] where  $w^a_{k}(e^a)$  for feature  a  and kernel  k  is given by \\[w^a_{k}(e^a) = \\exp(-\\frac{1}{2}(e^a - \\mu^a_k)^T (\\Sigma^{-1})^a_k(e^a - \\mu^a_k))\\] $\\Theta_k, \\mu^a_k, (\\Sigma^{-1})^a_k$  are learnable parameters. The input to the layer is a node feature array  x  of size  (num_features, num_nodes)  and edge pseudo-coordinate array  e  of size  (num_features, num_edges)  The residual  $\\mathbf{x}_i$  is added only if  residual=true  and the output size is the same  as the input size. Arguments in : Number of input node features. ein : Number of input edge features. out : Number of output features. σ : Activation function. Default  identity . K : Number of kernels. Default  1 . bias : Add learnable bias. Default  true . init : Weights' initializer. Default  glorot_uniform . residual : Residual conncetion. Default  false . Examples # create data\ns = [1,1,2,3]\nt = [2,3,1,1]\ng = GNNGraph(s,t)\nnin, ein, out, K = 4, 10, 7, 8 \nx = randn(Float32, nin, g.num_nodes)\ne = randn(Float32, ein, g.num_edges)\n\n# create layer\nl = GMMConv((nin, ein) => out, K=K)\n\n# forward pass\nl(g, x, e) source"},{"id":203,"pagetitle":"Convolutional layers","title":"GraphNeuralNetworks.GatedGraphConv","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/api/conv/#GraphNeuralNetworks.GatedGraphConv","content":" GraphNeuralNetworks.GatedGraphConv  —  Type GatedGraphConv(out, num_layers; aggr=+, init=glorot_uniform) Gated graph convolution layer from  Gated Graph Sequence Neural Networks . Implements the recursion \\[\\begin{aligned}\n\\mathbf{h}^{(0)}_i &= [\\mathbf{x}_i; \\mathbf{0}] \\\\\n\\mathbf{h}^{(l)}_i &= GRU(\\mathbf{h}^{(l-1)}_i, \\square_{j \\in N(i)} W \\mathbf{h}^{(l-1)}_j)\n\\end{aligned}\\] where  $\\mathbf{h}^{(l)}_i$  denotes the  $l$ -th hidden variables passing through GRU. The dimension of input  $\\mathbf{x}_i$  needs to be less or equal to  out . Arguments out : The dimension of output features. num_layers : The number of recursion steps. aggr : Aggregation operator for the incoming messages (e.g.  + ,  * ,  max ,  min , and  mean ). init : Weight initialization function. Examples: # create data\ns = [1,1,2,3]\nt = [2,3,1,1]\nout_channel = 5\nnum_layers = 3\ng = GNNGraph(s, t)\n\n# create layer\nl = GatedGraphConv(out_channel, num_layers)\n\n# forward pass\ny = l(g, x)    source"},{"id":204,"pagetitle":"Convolutional layers","title":"GraphNeuralNetworks.GraphConv","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/api/conv/#GraphNeuralNetworks.GraphConv","content":" GraphNeuralNetworks.GraphConv  —  Type GraphConv(in => out, σ=identity; aggr=+, bias=true, init=glorot_uniform) Graph convolution layer from Reference:  Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks . Performs: \\[\\mathbf{x}_i' = W_1 \\mathbf{x}_i + \\square_{j \\in \\mathcal{N}(i)} W_2 \\mathbf{x}_j\\] where the aggregation type is selected by  aggr . Arguments in : The dimension of input features. out : The dimension of output features. σ : Activation function. aggr : Aggregation operator for the incoming messages (e.g.  + ,  * ,  max ,  min , and  mean ). bias : Add learnable bias. init : Weights' initializer. Examples # create data\ns = [1,1,2,3]\nt = [2,3,1,1]\nin_channel = 3\nout_channel = 5\ng = GNNGraph(s, t)\nx = randn(Float32, 3, g.num_nodes)\n\n# create layer\nl = GraphConv(in_channel => out_channel, relu, bias = false, aggr = mean)\n\n# forward pass\ny = l(g, x)        source"},{"id":205,"pagetitle":"Convolutional layers","title":"GraphNeuralNetworks.MEGNetConv","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/api/conv/#GraphNeuralNetworks.MEGNetConv","content":" GraphNeuralNetworks.MEGNetConv  —  Type MEGNetConv(ϕe, ϕv; aggr=mean)\nMEGNetConv(in => out; aggr=mean) Convolution from  Graph Networks as a Universal Machine Learning Framework for Molecules and Crystals  paper. In the forward pass, takes as inputs node features  x  and edge features  e  and returns updated features  x'  and  e'  according to  \\[\\begin{aligned}\n\\mathbf{e}_{i\\to j}'  = \\phi_e([\\mathbf{x}_i;\\,  \\mathbf{x}_j;\\,  \\mathbf{e}_{i\\to j}]),\\\\\n\\mathbf{x}_{i}'  = \\phi_v([\\mathbf{x}_i;\\, \\square_{j\\in \\mathcal{N}(i)}\\,\\mathbf{e}_{j\\to i}']).\n\\end{aligned}\\] aggr  defines the aggregation to be performed. If the neural networks  ϕe  and   ϕv  are not provided, they will be constructed from the  in  and  out  arguments instead as multi-layer perceptron with one hidden layer and  relu   activations. Examples g = rand_graph(10, 30)\nx = randn(Float32, 3, 10)\ne = randn(Float32, 3, 30)\nm = MEGNetConv(3 => 3)\nx′, e′ = m(g, x, e) source"},{"id":206,"pagetitle":"Convolutional layers","title":"GraphNeuralNetworks.NNConv","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/api/conv/#GraphNeuralNetworks.NNConv","content":" GraphNeuralNetworks.NNConv  —  Type NNConv(in => out, f, σ=identity; aggr=+, bias=true, init=glorot_uniform) The continuous kernel-based convolutional operator from the   Neural Message Passing for Quantum Chemistry  paper.  This convolution is also known as the edge-conditioned convolution from the   Dynamic Edge-Conditioned Filters in Convolutional Neural Networks on Graphs  paper. Performs the operation \\[\\mathbf{x}_i' = W \\mathbf{x}_i + \\square_{j \\in N(i)} f_\\Theta(\\mathbf{e}_{j\\to i})\\,\\mathbf{x}_j\\] where  $f_\\Theta$   denotes a learnable function (e.g. a linear layer or a multi-layer perceptron). Given an input of batched edge features  e  of size  (num_edge_features, num_edges) ,  the function  f  will return an batched matrices array whose size is  (out, in, num_edges) . For convenience, also functions returning a single  (out*in, num_edges)  matrix are allowed. Arguments in : The dimension of input node features. out : The dimension of output node features. f : A (possibly learnable) function acting on edge features. aggr : Aggregation operator for the incoming messages (e.g.  + ,  * ,  max ,  min , and  mean ). σ : Activation function. bias : Add learnable bias. init : Weights' initializer. Examples: n_in = 3\nn_in_edge = 10\nn_out = 5\n\n# create data\ns = [1,1,2,3]\nt = [2,3,1,1]\ng = GNNGraph(s, t)\n\n# create dense layer\nnn = Dense(n_in_edge => n_out * n_in)\n\n# create layer\nl = NNConv(n_in => n_out, nn, tanh, bias = true, aggr = +)\n\nx = randn(Float32, n_in, g.num_nodes)\ne = randn(Float32, n_in_edge, g.num_edges)\n\n# forward pass\ny = l(g, x, e)   source"},{"id":207,"pagetitle":"Convolutional layers","title":"GraphNeuralNetworks.ResGatedGraphConv","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/api/conv/#GraphNeuralNetworks.ResGatedGraphConv","content":" GraphNeuralNetworks.ResGatedGraphConv  —  Type ResGatedGraphConv(in => out, act=identity; init=glorot_uniform, bias=true) The residual gated graph convolutional operator from the  Residual Gated Graph ConvNets  paper. The layer's forward pass is given by \\[\\mathbf{x}_i' = act\\big(U\\mathbf{x}_i + \\sum_{j \\in N(i)} \\eta_{ij} V \\mathbf{x}_j\\big),\\] where the edge gates  $\\eta_{ij}$  are given by \\[\\eta_{ij} = sigmoid(A\\mathbf{x}_i + B\\mathbf{x}_j).\\] Arguments in : The dimension of input features. out : The dimension of output features. act : Activation function. init : Weight matrices' initializing function.  bias : Learn an additive bias if true. Examples: # create data\ns = [1,1,2,3]\nt = [2,3,1,1]\nin_channel = 3\nout_channel = 5\ng = GNNGraph(s, t)\n\n# create layer\nl = ResGatedGraphConv(in_channel => out_channel, tanh, bias = true)\n\n# forward pass\ny = l(g, x)   source"},{"id":208,"pagetitle":"Convolutional layers","title":"GraphNeuralNetworks.SAGEConv","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/api/conv/#GraphNeuralNetworks.SAGEConv","content":" GraphNeuralNetworks.SAGEConv  —  Type SAGEConv(in => out, σ=identity; aggr=mean, bias=true, init=glorot_uniform) GraphSAGE convolution layer from paper  Inductive Representation Learning on Large Graphs . Performs: \\[\\mathbf{x}_i' = W \\cdot [\\mathbf{x}_i; \\square_{j \\in \\mathcal{N}(i)} \\mathbf{x}_j]\\] where the aggregation type is selected by  aggr . Arguments in : The dimension of input features. out : The dimension of output features. σ : Activation function. aggr : Aggregation operator for the incoming messages (e.g.  + ,  * ,  max ,  min , and  mean ). bias : Add learnable bias. init : Weights' initializer. Examples: # create data\ns = [1,1,2,3]\nt = [2,3,1,1]\nin_channel = 3\nout_channel = 5\ng = GNNGraph(s, t)\n\n# create layer\nl = SAGEConv(in_channel => out_channel, tanh, bias = false, aggr = +)\n\n# forward pass\ny = l(g, x)    source"},{"id":209,"pagetitle":"Convolutional layers","title":"GraphNeuralNetworks.SGConv","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/api/conv/#GraphNeuralNetworks.SGConv","content":" GraphNeuralNetworks.SGConv  —  Type SGConv(int => out, k=1; [bias, init, add_self_loops, use_edge_weight]) SGC layer from  Simplifying Graph Convolutional Networks  Performs operation \\[H^{K} = (\\tilde{D}^{-1/2} \\tilde{A} \\tilde{D}^{-1/2})^K X \\Theta\\] where  $\\tilde{A}$  is  $A + I$ . Arguments in : Number of input features. out : Number of output features. k  : Number of hops k. Default  1 . bias : Add learnable bias. Default  true . init : Weights' initializer. Default  glorot_uniform . add_self_loops : Add self loops to the graph before performing the convolution. Default  false . use_edge_weight : If  true , consider the edge weights in the input graph (if available).                    If  add_self_loops=true  the new weights will be set to 1. Default  false . Examples # create data\ns = [1,1,2,3]\nt = [2,3,1,1]\ng = GNNGraph(s, t)\nx = randn(Float32, 3, g.num_nodes)\n\n# create layer\nl = SGConv(3 => 5; add_self_loops = true) \n\n# forward pass\ny = l(g, x)       # size:  5 × num_nodes\n\n# convolution with edge weights\nw = [1.1, 0.1, 2.3, 0.5]\ny = l(g, x, w)\n\n# Edge weights can also be embedded in the graph.\ng = GNNGraph(s, t, w)\nl = SGConv(3 => 5, add_self_loops = true, use_edge_weight=true) \ny = l(g, x) # same as l(g, x, w)  source"},{"id":210,"pagetitle":"Convolutional layers","title":"GraphNeuralNetworks.TAGConv","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/api/conv/#GraphNeuralNetworks.TAGConv","content":" GraphNeuralNetworks.TAGConv  —  Type TAGConv(in => out, k=3; bias=true, init=glorot_uniform, add_self_loops=true, use_edge_weight=false) TAGConv layer from  Topology Adaptive Graph Convolutional Networks . This layer extends the idea of graph convolutions by applying filters that adapt to the topology of the data.  It performs the operation: \\[H^{K} = {\\sum}_{k=0}^K (D^{-1/2} A D^{-1/2})^{k} X {\\Theta}_{k}\\] where  A  is the adjacency matrix of the graph,  D  is the degree matrix,  X  is the input feature matrix, and  ${\\Theta}_{k}$  is a unique weight matrix for each hop  k . Arguments in : Number of input features. out : Number of output features. k : Maximum number of hops to consider. Default is  3 . bias : Whether to include a learnable bias term. Default is  true . init : Initialization function for the weights. Default is  glorot_uniform . add_self_loops : Whether to add self-loops to the adjacency matrix. Default is  true . use_edge_weight : If  true , edge weights are considered in the computation (if available). Default is  false . Examples # Example graph data\ns = [1, 1, 2, 3]\nt = [2, 3, 1, 1]\ng = GNNGraph(s, t)  # Create a graph\nx = randn(Float32, 3, g.num_nodes)  # Random features for each node\n\n# Create a TAGConv layer\nl = TAGConv(3 => 5, k=3; add_self_loops=true)\n\n# Apply the TAGConv layer\ny = l(g, x)  # Output size: 5 × num_nodes source"},{"id":211,"pagetitle":"Convolutional layers","title":"GraphNeuralNetworks.TransformerConv","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/api/conv/#GraphNeuralNetworks.TransformerConv","content":" GraphNeuralNetworks.TransformerConv  —  Type TransformerConv((in, ein) => out; [heads, concat, init, add_self_loops, bias_qkv,\n    bias_root, root_weight, gating, skip_connection, batch_norm, ff_channels])) The transformer-like multi head attention convolutional operator from the   Masked Label Prediction: Unified Message Passing Model for Semi-Supervised  Classification  paper, which also considers  edge features. It further contains options to also be configured as the transformer-like convolutional operator from the   Attention, Learn to Solve Routing Problems!  paper, including a successive feed-forward network as well as skip layers and batch normalization. The layer's basic forward pass is given by \\[x_i' = W_1x_i + \\sum_{j\\in N(i)} \\alpha_{ij} (W_2 x_j + W_6e_{ij})\\] where the attention scores are \\[\\alpha_{ij} = \\mathrm{softmax}\\left(\\frac{(W_3x_i)^T(W_4x_j+\nW_6e_{ij})}{\\sqrt{d}}\\right).\\] Optionally, a combination of the aggregated value with transformed root node features  by a gating mechanism via \\[x'_i = \\beta_i W_1 x_i + (1 - \\beta_i) \\underbrace{\\left(\\sum_{j \\in \\mathcal{N}(i)}\n\\alpha_{i,j} W_2 x_j \\right)}_{=m_i}\\] with \\[\\beta_i = \\textrm{sigmoid}(W_5^{\\top} [ W_1 x_i, m_i, W_1 x_i - m_i ]).\\] can be performed. Arguments in : Dimension of input features, which also corresponds to the dimension of    the output features. ein : Dimension of the edge features; if 0, no edge features will be used. out : Dimension of the output. heads : Number of heads in output. Default  1 . concat : Concatenate layer output or not. If not, layer output is averaged   over the heads. Default  true . init : Weight matrices' initializing function. Default  glorot_uniform . add_self_loops : Add self loops to the input graph. Default  false . bias_qkv : If set, bias is used in the key, query and value transformations for nodes.   Default  true . bias_root : If set, the layer will also learn an additive bias for the root when root    weight is used. Default  true . root_weight : If set, the layer will add the transformed root node features   to the output. Default  true . gating : If set, will combine aggregation and transformed root node features by a   gating mechanism. Default  false . skip_connection : If set, a skip connection will be made from the input and    added to the output. Default  false . batch_norm : If set, a batch normalization will be applied to the output. Default  false . ff_channels : If positive, a feed-forward NN is appended, with the first having the given   number of hidden nodes; this NN also gets a skip connection and batch normalization    if the respective parameters are set. Default:  0 . Examples N, in_channel, out_channel = 4, 3, 5\nein, heads = 2, 3\ng = GNNGraph([1,1,2,4], [2,3,1,1])\nl = TransformerConv((in_channel, ein) => in_channel; heads, gating = true, bias_qkv = true)\nx = rand(Float32, in_channel, N)\ne = rand(Float32, ein, g.num_edges)\nl(g, x, e) source"},{"id":214,"pagetitle":"Hetero Convolutional layers","title":"Hetero Graph-Convolutional Layers","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/api/heteroconv/#Hetero-Graph-Convolutional-Layers","content":" Hetero Graph-Convolutional Layers Heterogeneous graph convolutions are implemented in the type  HeteroGraphConv .  HeteroGraphConv  relies on standard graph convolutional layers to perform message passing on the different relations."},{"id":215,"pagetitle":"Hetero Convolutional layers","title":"GraphNeuralNetworks.HeteroGraphConv","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/api/heteroconv/#GraphNeuralNetworks.HeteroGraphConv","content":" GraphNeuralNetworks.HeteroGraphConv  —  Type HeteroGraphConv(itr; aggr = +)\nHeteroGraphConv(pairs...; aggr = +) A convolutional layer for heterogeneous graphs. The  itr  argument is an iterator of  pairs  of the form  edge_t => layer , where  edge_t  is a 3-tuple of the form  (src_node_type, edge_type, dst_node_type) , and  layer  is a  convolutional layers for homogeneous graphs.  Each convolution is applied to the corresponding relation.  Since a node type can be involved in multiple relations, the single convolution outputs  have to be aggregated using the  aggr  function. The default is to sum the outputs. Forward Arguments g::GNNHeteroGraph : The input graph. x::Union{NamedTuple,Dict} : The input node features. The keys are node types and the values are node feature tensors. Examples julia> g = rand_bipartite_heterograph((10, 15), 20)\nGNNHeteroGraph:\n  num_nodes: Dict(:A => 10, :B => 15)\n  num_edges: Dict((:A, :to, :B) => 20, (:B, :to, :A) => 20)\n\njulia> x = (A = rand(Float32, 64, 10), B = rand(Float32, 64, 15));\n\njulia> layer = HeteroGraphConv((:A, :to, :B) => GraphConv(64 => 32, relu),\n                               (:B, :to, :A) => GraphConv(64 => 32, relu));\n\njulia> y = layer(g, x); # output is a named tuple\n\njulia> size(y.A) == (32, 10) && size(y.B) == (32, 15)\ntrue source"},{"id":218,"pagetitle":"Pooling layers","title":"Pooling Layers","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/api/pool/#Pooling-Layers","content":" Pooling Layers"},{"id":219,"pagetitle":"Pooling layers","title":"Index","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/api/pool/#Index","content":" Index GraphNeuralNetworks.GlobalAttentionPool GraphNeuralNetworks.GlobalPool GraphNeuralNetworks.Set2Set GraphNeuralNetworks.TopKPool"},{"id":220,"pagetitle":"Pooling layers","title":"GraphNeuralNetworks.GlobalAttentionPool","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/api/pool/#GraphNeuralNetworks.GlobalAttentionPool","content":" GraphNeuralNetworks.GlobalAttentionPool  —  Type GlobalAttentionPool(fgate, ffeat=identity) Global soft attention layer from the  Gated Graph Sequence Neural Networks  paper \\[\\mathbf{u}_V = \\sum_{i\\in V} \\alpha_i\\, f_{feat}(\\mathbf{x}_i)\\] where the coefficients  $\\alpha_i$  are given by a  softmax_nodes  operation: \\[\\alpha_i = \\frac{e^{f_{gate}(\\mathbf{x}_i)}}\n                {\\sum_{i'\\in V} e^{f_{gate}(\\mathbf{x}_{i'})}}.\\] Arguments fgate : The function  $f_{gate}: \\mathbb{R}^{D_{in}} \\to \\mathbb{R}$ .           It is tipically expressed by a neural network. ffeat : The function  $f_{feat}: \\mathbb{R}^{D_{in}} \\to \\mathbb{R}^{D_{out}}$ .           It is tipically expressed by a neural network. Examples chin = 6\nchout = 5    \n\nfgate = Dense(chin, 1)\nffeat = Dense(chin, chout)\npool = GlobalAttentionPool(fgate, ffeat)\n\ng = Flux.batch([GNNGraph(random_regular_graph(10, 4), \n                         ndata=rand(Float32, chin, 10)) \n                for i=1:3])\n\nu = pool(g, g.ndata.x)\n\n@assert size(u) == (chout, g.num_graphs) source"},{"id":221,"pagetitle":"Pooling layers","title":"GraphNeuralNetworks.GlobalPool","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/api/pool/#GraphNeuralNetworks.GlobalPool","content":" GraphNeuralNetworks.GlobalPool  —  Type GlobalPool(aggr) Global pooling layer for graph neural networks. Takes a graph and feature nodes as inputs and performs the operation \\[\\mathbf{u}_V = \\square_{i \\in V} \\mathbf{x}_i\\] where  $V$  is the set of nodes of the input graph and  the type of aggregation represented by  $\\square$  is selected by the  aggr  argument.  Commonly used aggregations are  mean ,  max , and  + . See also  reduce_nodes . Examples using Flux, GraphNeuralNetworks, Graphs\n\npool = GlobalPool(mean)\n\ng = GNNGraph(erdos_renyi(10, 4))\nX = rand(32, 10)\npool(g, X) # => 32x1 matrix\n\n\ng = Flux.batch([GNNGraph(erdos_renyi(10, 4)) for _ in 1:5])\nX = rand(32, 50)\npool(g, X) # => 32x5 matrix source"},{"id":222,"pagetitle":"Pooling layers","title":"GraphNeuralNetworks.Set2Set","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/api/pool/#GraphNeuralNetworks.Set2Set","content":" GraphNeuralNetworks.Set2Set  —  Type Set2Set(n_in, n_iters, n_layers = 1) Set2Set layer from the paper  Order Matters: Sequence to sequence for sets . For each graph in the batch, the layer computes an output vector of size  2*n_in  by iterating the following steps  n_iters  times: \\[\\mathbf{q} = \\mathrm{LSTM}(\\mathbf{q}_{t-1}^*)\n\\alpha_{i} = \\frac{\\exp(\\mathbf{q}^T \\mathbf{x}_i)}{\\sum_{j=1}^N \\exp(\\mathbf{q}^T \\mathbf{x}_j)} \n\\mathbf{r} = \\sum_{i=1}^N \\alpha_{i} \\mathbf{x}_i\n\\mathbf{q}^*_t = [\\mathbf{q}; \\mathbf{r}]\\] where  N  is the number of nodes in the graph,  LSTM  is a Long-Short-Term-Memory network with  n_layers  layers,  input size  2*n_in  and output size  n_in . Given a batch of graphs  g  and node features  x , the layer returns a matrix of size  (2*n_in, n_graphs) . ``` source"},{"id":223,"pagetitle":"Pooling layers","title":"GraphNeuralNetworks.TopKPool","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/api/pool/#GraphNeuralNetworks.TopKPool","content":" GraphNeuralNetworks.TopKPool  —  Type TopKPool(adj, k, in_channel) Top-k pooling layer. Arguments adj : Adjacency matrix  of a graph. k : Top-k nodes are selected to pool together. in_channel : The dimension of input channel. source"},{"id":226,"pagetitle":"Temporal Convolutional layers","title":"Temporal Graph-Convolutional Layers","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/api/temporalconv/#Temporal-Graph-Convolutional-Layers","content":" Temporal Graph-Convolutional Layers Convolutions for time-varying graphs (temporal graphs) such as the  TemporalSnapshotsGNNGraph ."},{"id":227,"pagetitle":"Temporal Convolutional layers","title":"GraphNeuralNetworks.DCGRUCell","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/api/temporalconv/#GraphNeuralNetworks.DCGRUCell","content":" GraphNeuralNetworks.DCGRUCell  —  Type DCGRUCell(in => out, k; [bias, init]) Diffusion Convolutional Recurrent Neural Network (DCGRU) cell from the paper   Diffusion Convolutional Recurrent Neural Network: Data-driven Traffic Forecasting . Applyis a  DConv  layer to model spatial dependencies,  in combination with a Gated Recurrent Unit (GRU) cell to model temporal dependencies. Arguments in : Number of input node features. out : Number of output node features. k : Diffusion step for the  DConv . bias : Add learnable bias. Default  true . init : Convolution weights' initializer. Default  glorot_uniform . Forward cell(g::GNNGraph, x, [h]) g : The input graph. x : The node features. It should be a matrix of size  in x num_nodes . h : The current state of the GRU cell. It is a matrix of size  out x num_nodes .      If not provided, it is assumed to be a matrix of zeros. Performs one recurrence step and returns a tuple  (h, h) , where  h  is the updated hidden state of the GRU cell. Examples julia> using GraphNeuralNetworks, Flux\n\njulia> num_nodes, num_edges = 5, 10;\n\njulia> d_in, d_out = 2, 3;\n\njulia> timesteps = 5;\n\njulia> g = rand_graph(num_nodes, num_edges);\n\njulia> x = [rand(Float32, d_in, num_nodes) for t in 1:timesteps];\n\njulia> cell = DCGRUCell(d_in => d_out, 2);\n\njulia> state = Flux.initialstates(cell);\n\njulia> y = state;\n\njulia> for xt in x\n           y, state = cell(g, xt, state)\n       end\n\njulia> size(y) # (d_out, num_nodes)\n(3, 5) source"},{"id":228,"pagetitle":"Temporal Convolutional layers","title":"GraphNeuralNetworks.EvolveGCNOCell","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/api/temporalconv/#GraphNeuralNetworks.EvolveGCNOCell","content":" GraphNeuralNetworks.EvolveGCNOCell  —  Type \"     EvolveGCNOCell(in => out; bias = true, init = glorot_uniform) Evolving Graph Convolutional Network cell of type \"-O\" from the paper   EvolveGCN: Evolving Graph Convolutional Networks for Dynamic Graphs . Uses a  GCNConv  layer to model spatial dependencies, and an  LSTMCell  to model temporal dependencies. Can work with time-varying graphs and node features. Arguments in => out : A pair where  in  is the number of input node features and  out   is the number of output node features. bias : Add learnable bias for the convolution and the lstm cell. Default  true . init : Weights' initializer for the convolution. Default  glorot_uniform . Forward cell(g::GNNGraph, x, [state]) -> x, state g : The input graph. x : The node features. It should be a matrix of size  in x num_nodes . state : The current state of the cell.   A state is a tuple  (weight, lstm)  where  weight  is the convolution's weight and  lstm  is the lstm's state.   If not provided, it is generated by calling  Flux.initialstates(cell) . Returns the updated node features  x  and the updated state. julia> using GraphNeuralNetworks, Flux\n\njulia> num_nodes, num_edges = 5, 10;\n\njulia> d_in, d_out = 2, 3;\n\njulia> timesteps = 5;\n\njulia> g = [rand_graph(num_nodes, num_edges) for t in 1:timesteps];\n\njulia> x = [rand(Float32, d_in, num_nodes) for t in 1:timesteps];\n\njulia> cell1 = EvolveGCNOCell(d_in => d_out)\nEvolveGCNOCell(2 => 3)  # 321 parameters\n\njulia> cell2 = EvolveGCNOCell(d_out => d_out)\nEvolveGCNOCell(3 => 3)  # 696 parameters\n\njulia> state1 = Flux.initialstates(cell1);\n\njulia> state2 = Flux.initialstates(cell2);\n\njulia> outputs = [];\n\njulia> for t in 1:timesteps\n           zt, state1 = cell1(g[t], x[t], state1)\n           yt, state2 = cell2(g[t], zt, state2)\n           outputs = vcat(outputs, [yt])\n       end\n\njulia> size(outputs[end]) # (d_out, num_nodes)\n(3, 5) source"},{"id":229,"pagetitle":"Temporal Convolutional layers","title":"GraphNeuralNetworks.GConvGRUCell","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/api/temporalconv/#GraphNeuralNetworks.GConvGRUCell","content":" GraphNeuralNetworks.GConvGRUCell  —  Type GConvGRUCell(in => out, k; [bias, init]) Graph Convolutional Gated Recurrent Unit (GConvGRU) recurrent cell from the paper   Structured Sequence Modeling with Graph Convolutional Recurrent Networks . Uses  ChebConv  to model spatial dependencies,  followed by a Gated Recurrent Unit (GRU) cell to model temporal dependencies. Arguments in => out : A pair  where  in  is the number of input node features and  out   is the number of output node features. k : Chebyshev polynomial order. bias : Add learnable bias. Default  true . init : Weights' initializer. Default  glorot_uniform . Forward cell(g::GNNGraph, x, [h]) g : The input graph. x : The node features. It should be a matrix of size  in x num_nodes . h : The current hidden state of the GRU cell. If given, it is a matrix of size  out x num_nodes .      If not provided, it is assumed to be a matrix of zeros. Performs one recurrence step and returns a tuple  (h, h) ,  where  h  is the updated hidden state of the GRU cell. Examples julia> using GraphNeuralNetworks, Flux\n\njulia> num_nodes, num_edges = 5, 10;\n\njulia> d_in, d_out = 2, 3;\n\njulia> timesteps = 5;\n\njulia> g = rand_graph(num_nodes, num_edges);\n\njulia> x = [rand(Float32, d_in, num_nodes) for t in 1:timesteps];\n\njulia> cell = GConvGRUCell(d_in => d_out, 2);\n\njulia> state = Flux.initialstates(cell);\n\njulia> y = state;\n\njulia> for xt in x\n           y, state = cell(g, xt, state)\n       end\n\njulia> size(y) # (d_out, num_nodes)\n(3, 5) source"},{"id":230,"pagetitle":"Temporal Convolutional layers","title":"GraphNeuralNetworks.GConvLSTMCell","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/api/temporalconv/#GraphNeuralNetworks.GConvLSTMCell","content":" GraphNeuralNetworks.GConvLSTMCell  —  Type GConvLSTMCell(in => out, k; [bias, init]) Graph Convolutional LSTM recurrent cell from the paper   Structured Sequence Modeling with Graph Convolutional Recurrent Networks . Uses  ChebConv  to model spatial dependencies,  followed by a Long Short-Term Memory (LSTM) cell to model temporal dependencies. Arguments in => out : A pair  where  in  is the number of input node features and  out   is the number of output node features. k : Chebyshev polynomial order. bias : Add learnable bias. Default  true . init : Weights' initializer. Default  glorot_uniform . Forward cell(g::GNNGraph, x, [state]) g : The input graph. x : The node features. It should be a matrix of size  in x num_nodes . state : The current state of the LSTM cell.        If given, it is a tuple  (h, c)  where both  h  and  c  are arrays of size  out x num_nodes .      If not provided, it is assumed to be a tuple of matrices of zeros. Performs one recurrence step and returns a tuple  (output, state) ,  where  output  is the updated hidden state  h  of the LSTM cell and  state  is the updated tuple  (h, c) . Examples julia> using GraphNeuralNetworks, Flux\n\njulia> num_nodes, num_edges = 5, 10;\n\njulia> d_in, d_out = 2, 3;\n\njulia> timesteps = 5;\n\njulia> g = rand_graph(num_nodes, num_edges);\n\njulia> x = [rand(Float32, d_in, num_nodes) for t in 1:timesteps];\n\njulia> cell = GConvLSTMCell(d_in => d_out, 2);\n\njulia> state = Flux.initialstates(cell);\n\njulia> y = state[1];\n\njulia> for xt in x\n           y, state = cell(g, xt, state)\n       end\n\njulia> size(y) # (d_out, num_nodes)\n(3, 5) source"},{"id":231,"pagetitle":"Temporal Convolutional layers","title":"GraphNeuralNetworks.GNNRecurrence","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/api/temporalconv/#GraphNeuralNetworks.GNNRecurrence","content":" GraphNeuralNetworks.GNNRecurrence  —  Type GNNRecurrence(cell) Construct a recurrent layer that applies the graph recurrent  cell  forward multiple times to process an entire temporal sequence of node features at once. The  cell  has to satisfy the following interface for the forward pass:   yt, state = cell(g, xt, state) , where  xt  is the input node features,  yt  is the updated node features,  state  is the cell state to be updated. Forward layer(g, x, [state]) Applies the recurrent cell to each timestep of the input sequence. Arguments g : The input  GNNGraph  or  TemporalSnapshotsGNNGraph . If  GNNGraph , the same graph is used for all timesteps. If  TemporalSnapshotsGNNGraph , a different graph is used for each timestep. Not all cells support this. x : The time-varying node features.  If  g  is  GNNGraph , it is an array of size  in x timesteps x num_nodes . If  g  is  TemporalSnapshotsGNNGraph , it is an vector of length  timesteps , with element  t  of size  in x num_nodes_t . state : The initial state for the cell.   If not provided, it is generated by calling  Flux.initialstates(cell) . Return Returns the updated node features: If  g  is  GNNGraph , returns an array of size  out_features x timesteps x num_nodes . If  g  is  TemporalSnapshotsGNNGraph , returns a vector of length  timesteps ,  with element  t  of size  out_features x num_nodes_t . Examples The following example considers a static graph and a time-varying node features. julia> num_nodes, num_edges = 5, 10;\n\njulia> d_in, d_out = 2, 3;\n\njulia> timesteps = 5;\n\njulia> g = rand_graph(num_nodes, num_edges);\nGNNGraph:\n  num_nodes: 5\n  num_edges: 10\n\njulia> x = rand(Float32, d_in, timesteps, num_nodes);\n\njulia> cell = GConvLSTMCell(d_in => d_out, 2)\nGConvLSTMCell(2 => 3, 2)  # 168 parameters\n\njulia> layer = GNNRecurrence(cell)\nGNNRecurrence(\n  GConvLSTMCell(2 => 3, 2),             # 168 parameters\n)                   # Total: 24 arrays, 168 parameters, 2.023 KiB.\n\njulia> y = layer(g, x);\n\njulia> size(y) # (d_out, timesteps, num_nodes)\n(3, 5, 5) Now consider a time-varying graph and time-varying node features. julia> d_in, d_out = 2, 3;\n\njulia> timesteps = 5;\n\njulia> num_nodes = [10, 10, 10, 10, 10];\n\njulia> num_edges = [10, 12, 14, 16, 18];\n\njulia> snapshots = [rand_graph(n, m) for (n, m) in zip(num_nodes, num_edges)];\n\njulia> tg = TemporalSnapshotsGNNGraph(snapshots)\n\njulia> x = [rand(Float32, d_in, n) for n in num_nodes];\n\njulia> cell = EvolveGCNOCell(d_in => d_out)\nEvolveGCNOCell(2 => 3)  # 321 parameters\n\njulia> layer = GNNRecurrence(cell)\nGNNRecurrence(\n  EvolveGCNOCell(2 => 3),               # 321 parameters\n)                   # Total: 5 arrays, 321 parameters, 1.535 KiB.\n\njulia> y = layer(tg, x);\n\njulia> length(y)    # timesteps\n5\n\njulia> size(y[end]) # (d_out, num_nodes[end])\n(3, 10) source"},{"id":232,"pagetitle":"Temporal Convolutional layers","title":"GraphNeuralNetworks.TGCNCell","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/api/temporalconv/#GraphNeuralNetworks.TGCNCell","content":" GraphNeuralNetworks.TGCNCell  —  Type TGCNCell(in => out; kws...) Recurrent graph convolutional cell from the paper  T-GCN: A Temporal Graph Convolutional Network for Traffic Prediction . Uses two stacked  GCNConv  layers to model spatial dependencies, and a GRU mechanism to model temporal dependencies. in  and  out  are the number of input and output node features, respectively. The keyword arguments are passed to the  GCNConv  constructor. Forward cell(g::GNNGraph, x, [state]) g : The input graph. x : The node features. It should be a matrix of size  in x num_nodes . state : The current state of the cell.   If not provided, it is generated by calling  Flux.initialstates(cell) .   The state is a matrix of size  out x num_nodes . Returns the updated node features and the updated state. Examples julia> using GraphNeuralNetworks, Flux\n\njulia> num_nodes, num_edges = 5, 10;\n\njulia> d_in, d_out = 2, 3;\n\njulia> timesteps = 5;\n\njulia> g = rand_graph(num_nodes, num_edges);\n\njulia> x = [rand(Float32, d_in, num_nodes) for t in 1:timesteps];\n\njulia> cell = DCGRUCell(d_in => d_out, 2);\n\njulia> state = Flux.initialstates(cell);\n\njulia> y = state;\n\njulia> for xt in x\n           y, state = cell(g, xt, state)\n       end\n\njulia> size(y) # (d_out, num_nodes)\n(3, 5) source"},{"id":233,"pagetitle":"Temporal Convolutional layers","title":"GraphNeuralNetworks.DCGRU","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/api/temporalconv/#GraphNeuralNetworks.DCGRU-Tuple","content":" GraphNeuralNetworks.DCGRU  —  Method DCGRU(args...; kws...) Construct a recurrent layer corresponding to the  DCGRUCell  cell. It can be used to process an entire temporal sequence of node features at once. The arguments are passed to the  DCGRUCell  constructor. See  GNNRecurrence  for more details. Examples julia> num_nodes, num_edges = 5, 10;\n\njulia> d_in, d_out = 2, 3;\n\njulia> timesteps = 5;\n\njulia> g = rand_graph(num_nodes, num_edges);\n\njulia> x = rand(Float32, d_in, timesteps, num_nodes);\n\njulia> layer = DCGRU(d_in => d_out, 2)\nGNNRecurrence(\n  DCGRUCell(2 => 3, 2),                 # 189 parameters\n)                   # Total: 6 arrays, 189 parameters, 1.184 KiB.\n\njulia> y = layer(g, x);\n\njulia> size(y) # (d_out, timesteps, num_nodes)\n(3, 5, 5) source"},{"id":234,"pagetitle":"Temporal Convolutional layers","title":"GraphNeuralNetworks.EvolveGCNO","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/api/temporalconv/#GraphNeuralNetworks.EvolveGCNO-Tuple","content":" GraphNeuralNetworks.EvolveGCNO  —  Method EvolveGCNO(args...; kws...) Construct a recurrent layer corresponding to the  EvolveGCNOCell  cell. It can be used to process an entire temporal sequence of graphs and node features at once. The arguments are passed to the  EvolveGCNOCell  constructor. See  GNNRecurrence  for more details. Examples julia> d_in, d_out = 2, 3;\n\njulia> timesteps = 5;\n\njulia> num_nodes = [10, 10, 10, 10, 10];\n\njulia> num_edges = [10, 12, 14, 16, 18];\n\njulia> snapshots = [rand_graph(n, m) for (n, m) in zip(num_nodes, num_edges)];\n\njulia> tg = TemporalSnapshotsGNNGraph(snapshots)\n\njulia> x = [rand(Float32, d_in, n) for n in num_nodes];\n\njulia> cell = EvolveGCNO(d_in => d_out)\nGNNRecurrence(\n  EvolveGCNOCell(2 => 3),               # 321 parameters\n)                   # Total: 5 arrays, 321 parameters, 1.535 KiB.\n\njulia> y = layer(tg, x);\n\njulia> length(y)    # timesteps\n5 \n\njulia> size(y[end]) # (d_out, num_nodes[end])\n(3, 10) source"},{"id":235,"pagetitle":"Temporal Convolutional layers","title":"GraphNeuralNetworks.GConvGRU","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/api/temporalconv/#GraphNeuralNetworks.GConvGRU-Tuple","content":" GraphNeuralNetworks.GConvGRU  —  Method GConvGRU(args...; kws...) Construct a recurrent layer corresponding to the  GConvGRUCell  cell. It can be used to process an entire temporal sequence of node features at once. The arguments are passed to the  GConvGRUCell  constructor. See  GNNRecurrence  for more details. Examples julia> num_nodes, num_edges = 5, 10;\n\njulia> d_in, d_out = 2, 3;\n\njulia> timesteps = 5;\n\njulia> g = rand_graph(num_nodes, num_edges);\n\njulia> x = rand(Float32, d_in, timesteps, num_nodes);\n\njulia> layer = GConvGRU(d_in => d_out, 2)\nGConvGRU(\n  GConvGRUCell(2 => 3, 2),              # 108 parameters\n)                   # Total: 12 arrays, 108 parameters, 1.148 KiB.\n\njulia> y = layer(g, x);\n\njulia> size(y) # (d_out, timesteps, num_nodes)\n(3, 5, 5) source"},{"id":236,"pagetitle":"Temporal Convolutional layers","title":"GraphNeuralNetworks.GConvLSTM","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/api/temporalconv/#GraphNeuralNetworks.GConvLSTM-Tuple","content":" GraphNeuralNetworks.GConvLSTM  —  Method GConvLSTM(args...; kws...) Construct a recurrent layer corresponding to the  GConvLSTMCell  cell. It can be used to process an entire temporal sequence of node features at once. The arguments are passed to the  GConvLSTMCell  constructor. See  GNNRecurrence  for more details. Examples julia> num_nodes, num_edges = 5, 10;\n\njulia> d_in, d_out = 2, 3;\n\njulia> timesteps = 5;\n\njulia> g = rand_graph(num_nodes, num_edges);\n\njulia> x = rand(Float32, d_in, timesteps, num_nodes);\n\njulia> layer = GConvLSTM(d_in => d_out, 2)\nGNNRecurrence(\n  GConvLSTMCell(2 => 3, 2),             # 168 parameters\n)                   # Total: 24 arrays, 168 parameters, 2.023 KiB.\n\njulia> y = layer(g, x);\n\njulia> size(y) # (d_out, timesteps, num_nodes)\n(3, 5, 5) source"},{"id":237,"pagetitle":"Temporal Convolutional layers","title":"GraphNeuralNetworks.TGCN","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/api/temporalconv/#GraphNeuralNetworks.TGCN-Tuple","content":" GraphNeuralNetworks.TGCN  —  Method TGCN(args...; kws...) Construct a recurrent layer corresponding to the  TGCNCell  cell. The arguments are passed to the  TGCNCell  constructor. See  GNNRecurrence  for more details. Examples julia> num_nodes, num_edges = 5, 10;\n\njulia> d_in, d_out = 2, 3;\n\njulia> timesteps = 5;\n\njulia> g = rand_graph(num_nodes, num_edges);\n\njulia> x = rand(Float32, d_in, timesteps, num_nodes);\n\njulia> layer = TGCN(d_in => d_out)\n\njulia> y = layer(g, x);\n\njulia> size(y) # (d_out, timesteps, num_nodes)\n(3, 5, 5) source"},{"id":240,"pagetitle":"Developer guide","title":"Developer Notes","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/dev/#Developer-Notes","content":" Developer Notes"},{"id":241,"pagetitle":"Developer guide","title":"Development Enviroment","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/dev/#Development-Enviroment","content":" Development Enviroment GraphNeuralNetworks.jl is package hosted in a monorepo that contains multiple packages.  The GraphNeuralNetworks.jl package depends on GNNGraphs.jl and GNNlib.jl, also hosted in the same monorepo. In order  pkg> activate .\n\npkg> dev ./GNNGraphs"},{"id":242,"pagetitle":"Developer guide","title":"Add a New Layer","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/dev/#Add-a-New-Layer","content":" Add a New Layer To add a new graph convolutional layer and make it available in both the Flux-based frontend (GraphNeuralNetworks.jl) and the Lux-based frontend (GNNLux), you need to: Add the functional version to GNNlib Add the stateful version to GraphNeuralNetworks Add the stateless version to GNNLux Add the layer to the table in docs/api/conv.md We suggest to start with implementing a self-contained Flux layer in GraphNeuralNetworks.jl, add the corresponding tests, and then when everything is working, move the implementation of the forward pass to GNNlib.jl. At this point, you can add the stateless version to GNNLux.jl. It could also be convenient to use the  @structdef  macro from  Autostruct.jl  to simultaneously generate the struct and the constructor for the layer. For example, the Flux implementation of  MEGNetConv  layer can be written as follows: using Flux, GraphNeuralNetworks, AutoStructs\n\n@structdef function MEGNetConv(ch::Pair{Int, Int}; aggr = mean)\n    nin, nout = ch\n    ϕe = Chain(Dense(3nin, nout, relu),\n               Dense(nout, nout))\n\n    ϕv = Chain(Dense(nin + nout, nout, relu),\n               Dense(nout, nout))\n\n    return MEGNetConv(ϕe, ϕv, aggr)\nend\n\nFlux.@layer MEGNetConv\n\nfunction (l::MEGNetConv)(g::AbstractGraph, x::AbstractMatrix, e::AbstractMatrix)\n    ē = apply_edges(g, xi = x, xj = x, e = e) do xi, xj, e\n        l.ϕe(vcat(xi, xj, e))\n    end\n    xᵉ = aggregate_neighbors(g, l.aggr, ē)\n    x̄ = l.ϕv(vcat(x, xᵉ))\n    return x̄, ē\nend"},{"id":243,"pagetitle":"Developer guide","title":"Versions and Tagging","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/dev/#Versions-and-Tagging","content":" Versions and Tagging Each PR should update the version number in the Porject.toml file of each involved package if needed by semnatic versioning. For instance, when adding new features GNNGraphs could move from \"1.17.5\" to \"1.18.0-DEV\". The \"DEV\" will be removed when the package is tagged and released. Pay also attention to updating the compat bounds, e.g. GraphNeuralNetworks might require a newer version of GNNGraphs."},{"id":244,"pagetitle":"Developer guide","title":"Generate Documentation Locally","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/dev/#Generate-Documentation-Locally","content":" Generate Documentation Locally For generating the documentation locally cd docs\njulia (@v1.10) pkg> activate .\n  Activating project at `~/.julia/dev/GraphNeuralNetworks/docs`\n\n(docs) pkg> dev ../ ../GNNGraphs/\n   Resolving package versions...\n  No Changes to `~/.julia/dev/GraphNeuralNetworks/docs/Project.toml`\n  No Changes to `~/.julia/dev/GraphNeuralNetworks/docs/Manifest.toml`\n\njulia> include(\"make.jl\")"},{"id":245,"pagetitle":"Developer guide","title":"Benchmarking","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/dev/#Benchmarking","content":" Benchmarking You can benchmark the effect on performance of your commits using the script  perf/perf.jl . First, checkout and benchmark the master branch: julia> include(\"perf.jl\")\n\njulia> df = run_benchmarks()\n\n# observe results\njulia> for g in groupby(df, :layer); println(g, \"\\n\"); end\n\njulia> @save \"perf_master_20210803_mymachine.jld2\" dfmaster=df Now checkout your branch and do the same: julia> df = run_benchmarks()\n\njulia> @save \"perf_pr_20210803_mymachine.jld2\" dfpr=df Finally, compare the results: julia> @load \"perf_master_20210803_mymachine.jld2\"\n\njulia> @load \"perf_pr_20210803_mymachine.jld2\"\n\njulia> compare(dfpr, dfmaster)"},{"id":246,"pagetitle":"Developer guide","title":"Caching tutorials","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/dev/#Caching-tutorials","content":" Caching tutorials Tutorials in GraphNeuralNetworks.jl are written in Pluto and rendered using  DemoCards.jl  and  PlutoStaticHTML.jl . Rendering a Pluto notebook is time and resource-consuming, especially in a CI environment. So we use the  caching functionality  provided by PlutoStaticHTML.jl to reduce CI time. If you are contributing a new tutorial or making changes to the existing notebook, generate the docs locally before committing/pushing. For caching to work, the cache environment(your local) and the documenter CI should have the same Julia version (e.g. \"v1.9.1\", also the patch number must match). So use the  documenter CI Julia version  for generating docs locally. julia --version # check julia version before generating docs\njulia --project=docs docs/make.jl Note: Use  juliaup  for easy switching of Julia versions. During the doc generation process, DemoCards.jl stores the cache notebooks in docs/pluto_output. So add any changes made in this folder in your git commit. Remember that every file in this folder is machine-generated and should not be edited manually. git add docs/pluto_output # add generated cache Check the  documenter CI logs  to ensure that it used the local cache:"},{"id":249,"pagetitle":"Models","title":"Models","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/guides/models/#Models","content":" Models GraphNeuralNetworks.jl provides common graph convolutional layers by which you can assemble arbitrarily deep or complex models. GNN layers are compatible with  Flux.jl ones, therefore expert Flux users are promptly able to define and train  their models.  In what follows, we discuss two different styles for model creation: the  explicit modeling  style, more verbose but more flexible,  and the  implicit modeling  style based on  GraphNeuralNetworks.GNNChain , more concise but less flexible."},{"id":250,"pagetitle":"Models","title":"Explicit modeling","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/guides/models/#Explicit-modeling","content":" Explicit modeling In the explicit modeling style, the model is created according to the following steps: Define a new type for your model ( GNN  in the example below). Layers and submodels are fields. Apply  Flux.@layer  to the new type to make it Flux's compatible (parameters' collection, gpu movement, etc...) Optionally define a convenience constructor for your model. Define the forward pass by implementing the call method for your type. Instantiate the model.  Here is an example of this construction: using Flux, Graphs, GraphNeuralNetworks\n\nstruct GNN                                # step 1\n    conv1\n    bn\n    conv2\n    dropout\n    dense\nend\n\nFlux.@layer GNN                         # step 2\n\nfunction GNN(din::Int, d::Int, dout::Int) # step 3    \n    GNN(GraphConv(din => d),\n        BatchNorm(d),\n        GraphConv(d => d, relu),\n        Dropout(0.5),\n        Dense(d, dout))\nend\n\nfunction (model::GNN)(g::GNNGraph, x)     # step 4\n    x = model.conv1(g, x)\n    x = relu.(model.bn(x))\n    x = model.conv2(g, x)\n    x = model.dropout(x)\n    x = model.dense(x)\n    return x \nend\n\ndin, d, dout = 3, 4, 2 \nmodel = GNN(din, d, dout)                 # step 5\n\ng = rand_graph(10, 30)\nX = randn(Float32, din, 10) \n\ny = model(g, X)  # output size: (dout, g.num_nodes)\ngrad = gradient(model -> sum(model(g, X)), model)"},{"id":251,"pagetitle":"Models","title":"Implicit modeling with GNNChains","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/guides/models/#Implicit-modeling-with-GNNChains","content":" Implicit modeling with GNNChains While very flexible, the way in which we defined  GNN  model definition in last section is a bit verbose. In order to simplify things, we provide the  GraphNeuralNetworks.GNNChain  type. It is very similar  to Flux's well known  Chain . It allows to compose layers in a sequential fashion as Chain does, propagating the output of each layer to the next one. In addition,  GNNChain  propagates the input graph as well, providing it as a first argument to layers subtyping the  GraphNeuralNetworks.GNNLayer  abstract type.  Using  GNNChain , the model definition becomes more concise: model = GNNChain(GraphConv(din => d),\n                 BatchNorm(d),\n                 x -> relu.(x),\n                 GraphConv(d => d, relu),\n                 Dropout(0.5),\n                 Dense(d, dout)) The  GNNChain  only propagates the graph and the node features. More complex scenarios, e.g. when also edge features are updated, have to be handled using the explicit definition of the forward pass.  A  GNNChain  opportunely propagates the graph into the branches created by the  Flux.Parallel  layer: AddResidual(l) = Parallel(+, identity, l)  # implementing a skip/residual connection\n\nmodel = GNNChain( ResGatedGraphConv(din => d, relu),\n                  AddResidual(ResGatedGraphConv(d => d, relu)),\n                  AddResidual(ResGatedGraphConv(d => d, relu)),\n                  AddResidual(ResGatedGraphConv(d => d, relu)),\n                  GlobalPooling(mean),\n                  Dense(d, dout))\n\ny = model(g, X) # output size: (dout, g.num_graphs)"},{"id":252,"pagetitle":"Models","title":"Embedding a graph in the model","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/guides/models/#Embedding-a-graph-in-the-model","content":" Embedding a graph in the model Sometimes it is useful to consider a specific graph as a part of a model instead of  its input. GraphNeuralNetworks.jl provides the  WithGraph  type to deal with this scenario. chain = GNNChain(GCNConv(din => d, relu),\n                 GCNConv(d => d))\n\n\ng = rand_graph(10, 30)\n\nmodel = WithGraph(chain, g)\n\nX = randn(Float32, din, 10)\n\n# Pass only X as input, the model already contains the graph.\ny = model(X)  An example of  WithGraph  usage is given in the graph neural ODE script in the  examples  folder."},{"id":255,"pagetitle":"Hands on","title":"Implementing Graph Neural Networks","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/tutorials/gnn_intro_pluto/#Implementing-Graph-Neural-Networks","content":" Implementing Graph Neural Networks After learning about GraphNeuralNetworks.jl's data handling, it's time to implement our first Graph Neural Network! For this, we will use on of the most simple GNN operators, the  GCN layer  ( Kipf et al. (2017) ), which is defined as $$\\mathbf{x}_v^{(\\ell + 1)} = \\mathbf{W}^{(\\ell + 1)} \\sum_{w \\in \\mathcal{N}(v) \\, \\cup \\, \\{ v \\}} \\frac{1}{c_{w,v}} \\cdot \\mathbf{x}_w^{(\\ell)}$$ where  \\(\\mathbf{W}^{(\\ell + 1)}\\)  denotes a trainable weight matrix of shape  [num_output_features, num_input_features]  and  \\(c_{w,v}\\)  refers to a fixed normalization coefficient for each edge. GraphNeuralNetworks.jl implements this layer via  GCNConv , which can be executed by passing in the node feature representation  x  and the COO graph connectivity representation  edge_index . With this, we are ready to create our first Graph Neural Network by defining our network architecture: begin\n    struct GCN\n        layers::NamedTuple\n    end\n\n    Flux.@layer GCN # provides parameter collection, gpu movement and more\n\n    function GCN(num_features, num_classes)\n        layers = (conv1 = GCNConv(num_features => 4),\n                  conv2 = GCNConv(4 => 4),\n                  conv3 = GCNConv(4 => 2),\n                  classifier = Dense(2, num_classes))\n        return GCN(layers)\n    end\n\n    function (gcn::GCN)(g::GNNGraph, x::AbstractMatrix)\n        l = gcn.layers\n        x = l.conv1(g, x)\n        x = tanh.(x)\n        x = l.conv2(g, x)\n        x = tanh.(x)\n        x = l.conv3(g, x)\n        x = tanh.(x)  # Final GNN embedding space.\n        out = l.classifier(x)\n        # Apply a final (linear) classifier.\n        return out, x\n    end\nend Here, we first initialize all of our building blocks in the constructor and define the computation flow of our network in the call method. We first define and stack  three graph convolution layers , which corresponds to aggregating 3-hop neighborhood information around each node (all nodes up to 3 \"hops\" away). In addition, the  GCNConv  layers reduce the node feature dimensionality to  \\(2\\) ,  i.e. ,  \\(34 \\rightarrow 4 \\rightarrow 4 \\rightarrow 2\\) . Each  GCNConv  layer is enhanced by a  tanh  non-linearity. After that, we apply a single linear transformation ( Flux.Dense  that acts as a classifier to map our nodes to 1 out of the 4 classes/communities. We return both the output of the final classifier as well as the final node embeddings produced by our GNN. We proceed to initialize our final model via  GCN() , and printing our model produces a summary of all its used sub-modules. Embedding the Karate Club Network Let's take a look at the node embeddings produced by our GNN. Here, we pass in the initial node features  x  and the graph  information  g  to the model, and visualize its 2-dimensional embedding. begin\n    num_features = 34\n    num_classes = 4\n    gcn = GCN(num_features, num_classes)\nend GCN((conv1 = GCNConv(34 => 4), conv2 = GCNConv(4 => 4), conv3 = GCNConv(4 => 2), classifier = Dense(2 => 4)))  # 182 parameters _, h = gcn(g, g.ndata.x) (Float32[-0.0068139993 0.008728906 … 0.020461287 0.016271798; -0.0019973165 -0.0064561698 … -0.0044912496 -0.004174295; 0.1469301 0.13193016 … -0.06870474 -0.03323521; -0.022454038 -0.0069215773 … 0.025904683 0.018215057], Float32[-0.055850513 -0.03927876 … 0.03876325 0.023417776; -0.11278143 -0.11275233 … 0.03937418 0.014116553]) function visualize_embeddings(h; colors = nothing)\n    xs = h[1, :] |> vec\n    ys = h[2, :] |> vec\n    Makie.scatter(xs, ys, color = labels, markersize = 20)\nend visualize_embeddings (generic function with 1 method) visualize_embeddings(h, colors = labels) Remarkably, even before training the weights of our model, the model produces an embedding of nodes that closely resembles the community-structure of the graph. Nodes of the same color (community) are already closely clustered together in the embedding space, although the weights of our model are initialized  completely at random  and we have not yet performed any training so far! This leads to the conclusion that GNNs introduce a strong inductive bias, leading to similar embeddings for nodes that are close to each other in the input graph. Training on the Karate Club Network But can we do better? Let's look at an example on how to train our network parameters based on the knowledge of the community assignments of 4 nodes in the graph (one for each community). Since everything in our model is differentiable and parameterized, we can add some labels, train the model and observe how the embeddings react. Here, we make use of a semi-supervised or transductive learning procedure: we simply train against one node per class, but are allowed to make use of the complete input graph data. Training our model is very similar to any other Flux model. In addition to defining our network architecture, we define a loss criterion (here,  logitcrossentropy ), and initialize a stochastic gradient optimizer (here,  Adam ). After that, we perform multiple rounds of optimization, where each round consists of a forward and backward pass to compute the gradients of our model parameters w.r.t. to the loss derived from the forward pass. If you are not new to Flux, this scheme should appear familiar to you. Note that our semi-supervised learning scenario is achieved by the following line: loss = logitcrossentropy(ŷ[:,train_mask], y[:,train_mask]) While we compute node embeddings for all of our nodes, we  only make use of the training nodes for computing the loss . Here, this is implemented by filtering the output of the classifier  out  and ground-truth labels  data.y  to only contain the nodes in the  train_mask . Let us now start training and see how our node embeddings evolve over time (best experienced by explicitly running the code): begin\n    model = GCN(num_features, num_classes)\n    opt = Flux.setup(Adam(1e-2), model)\n    epochs = 2000\n\n    emb = h\n    function report(epoch, loss, h)\n        # p = visualize_embeddings(h)\n        @info (; epoch, loss)\n    end\n\n    report(0, 10.0, emb)\n    for epoch in 1:epochs\n        loss, grad = Flux.withgradient(model) do model\n            ŷ, emb = model(g, g.ndata.x)\n            logitcrossentropy(ŷ[:, train_mask], y[:, train_mask])\n        end\n\n        Flux.update!(opt, model, grad[1])\n        if epoch % 200 == 0\n            report(epoch, loss, emb)\n        end\n    end\nend ŷ, emb_final = model(g, g.ndata.x) (Float32[-8.871021 -6.288402 … 7.8817716 7.3984337; 7.873129 5.5748186 … -8.054153 -7.562167; 0.6939411 2.6538918 … 0.1978332 0.633129; 0.42380208 -1.7143326 … -0.14687762 -0.5542332], Float32[-0.99049056 -0.9905237 … 0.99305063 0.87260294; -0.9905631 -0.40585023 … 0.9999852 0.99999404]) # train accuracy\nmean(onecold(ŷ[:, train_mask]) .== onecold(y[:, train_mask])) 1.0 # test accuracy\nmean(onecold(ŷ[:, .!train_mask]) .== onecold(y[:, .!train_mask])) 0.8 visualize_embeddings(emb_final, colors = labels) As one can see, our 3-layer GCN model manages to linearly separating the communities and classifying most of the nodes correctly. Furthermore, we did this all with a few lines of code, thanks to the GraphNeuralNetworks.jl which helped us out with data handling and GNN implementations."},{"id":258,"pagetitle":"Graph classification","title":"Mini-batching of graphs","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/tutorials/graph_classification_pluto/#Mini-batching-of-graphs","content":" Mini-batching of graphs Since graphs in graph classification datasets are usually small, a good idea is to  batch the graphs  before inputting them into a Graph Neural Network to guarantee full GPU utilization. In the image or language domain, this procedure is typically achieved by  rescaling  or  padding  each example into a set of equally-sized shapes, and examples are then grouped in an additional dimension. The length of this dimension is then equal to the number of examples grouped in a mini-batch and is typically referred to as the  batchsize . However, for GNNs the two approaches described above are either not feasible or may result in a lot of unnecessary memory consumption. Therefore, GraphNeuralNetworks.jl opts for another approach to achieve parallelization across a number of examples. Here, adjacency matrices are stacked in a diagonal fashion (creating a giant graph that holds multiple isolated subgraphs), and node and target features are simply concatenated in the node dimension (the last dimension). This procedure has some crucial advantages over other batching procedures: GNN operators that rely on a message passing scheme do not need to be modified since messages are not exchanged between two nodes that belong to different graphs. There is no computational or memory overhead since adjacency matrices are saved in a sparse fashion holding only non-zero entries,  i.e. , the edges. GraphNeuralNetworks.jl can  batch multiple graphs into a single giant graph : vec_gs, _ = first(train_loader) (GNNGraph{Tuple{Vector{Int64}, Vector{Int64}, Nothing}}[GNNGraph(13, 28) with x: 7×13 data, GNNGraph(15, 34) with x: 7×15 data, GNNGraph(11, 22) with x: 7×11 data, GNNGraph(17, 38) with x: 7×17 data, GNNGraph(23, 54) with x: 7×23 data, GNNGraph(14, 30) with x: 7×14 data, GNNGraph(16, 34) with x: 7×16 data, GNNGraph(17, 38) with x: 7×17 data, GNNGraph(13, 28) with x: 7×13 data, GNNGraph(19, 40) with x: 7×19 data  …  GNNGraph(26, 56) with x: 7×26 data, GNNGraph(13, 28) with x: 7×13 data, GNNGraph(11, 22) with x: 7×11 data, GNNGraph(18, 38) with x: 7×18 data, GNNGraph(28, 66) with x: 7×28 data, GNNGraph(11, 22) with x: 7×11 data, GNNGraph(13, 28) with x: 7×13 data, GNNGraph(18, 40) with x: 7×18 data, GNNGraph(16, 36) with x: 7×16 data, GNNGraph(22, 50) with x: 7×22 data], Bool[1 0 … 1 0; 0 1 … 0 1]) MLUtils.batch(vec_gs) GNNGraph:\n  num_nodes: 569\n  num_edges: 1258\n  num_graphs: 32\n  ndata:\n\tx = 7×569 Matrix{Float32} Each batched graph object is equipped with a  graph_indicator  vector , which maps each node to its respective graph in the batch: $$\\textrm{graph\\_indicator} = [1, \\ldots, 1, 2, \\ldots, 2, 3, \\ldots ]$$"},{"id":259,"pagetitle":"Graph classification","title":"Training a Graph Neural Network (GNN)","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/tutorials/graph_classification_pluto/#Training-a-Graph-Neural-Network-(GNN)","content":" Training a Graph Neural Network (GNN) Training a GNN for graph classification usually follows a simple recipe: Embed each node by performing multiple rounds of message passing Aggregate node embeddings into a unified graph embedding ( readout layer ) Train a final classifier on the graph embedding There exists multiple  readout layers  in literature, but the most common one is to simply take the average of node embeddings: $$\\mathbf{x}_{\\mathcal{G}} = \\frac{1}{|\\mathcal{V}|} \\sum_{v \\in \\mathcal{V}} \\mathcal{x}^{(L)}_v$$ GraphNeuralNetworks.jl provides this functionality via  GlobalPool(mean) , which takes in the node embeddings of all nodes in the mini-batch and the assignment vector  graph_indicator  to compute a graph embedding of size  [hidden_channels, batchsize] . The final architecture for applying GNNs to the task of graph classification then looks as follows and allows for complete end-to-end training: function create_model(nin, nh, nout)\n    GNNChain(GCNConv(nin => nh, relu),\n             GCNConv(nh => nh, relu),\n             GCNConv(nh => nh),\n             GlobalPool(mean),\n             Dropout(0.5),\n             Dense(nh, nout))\nend create_model (generic function with 1 method) Here, we again make use of the  GCNConv  with  \\(\\mathrm{ReLU}(x) = \\max(x, 0)\\)  activation for obtaining localized node embeddings, before we apply our final classifier on top of a graph readout layer. Let's train our network for a few epochs to see how well it performs on the training as well as test set: function eval_loss_accuracy(model, data_loader, device)\n    loss = 0.0\n    acc = 0.0\n    ntot = 0\n    for (g, y) in data_loader\n        g, y = MLUtils.batch(g) |> device, y |> device\n        n = length(y)\n        ŷ = model(g, g.ndata.x)\n        loss += logitcrossentropy(ŷ, y) * n\n        acc += mean((ŷ .> 0) .== y) * n\n        ntot += n\n    end\n    return (loss = round(loss / ntot, digits = 4),\n            acc = round(acc * 100 / ntot, digits = 2))\nend eval_loss_accuracy (generic function with 1 method) function train!(model; epochs = 200, η = 1e-2, infotime = 10)\n    # device = Flux.gpu # uncomment this for GPU training\n    device = Flux.cpu\n    model = model |> device\n    opt = Flux.setup(Adam(1e-3), model)\n\n    function report(epoch)\n        train = eval_loss_accuracy(model, train_loader, device)\n        test = eval_loss_accuracy(model, test_loader, device)\n        @info (; epoch, train, test)\n    end\n\n    report(0)\n    for epoch in 1:epochs\n        for (g, y) in train_loader\n            g, y = MLUtils.batch(g) |> device, y |> device\n            grad = Flux.gradient(model) do model\n                ŷ = model(g, g.ndata.x)\n                logitcrossentropy(ŷ, y)\n            end\n            Flux.update!(opt, model, grad[1])\n        end\n        epoch % infotime == 0 && report(epoch)\n    end\nend train! (generic function with 1 method) begin\n    nin = 7\n    nh = 64\n    nout = 2\n    model = create_model(nin, nh, nout)\n    train!(model)\nend As one can see, our model reaches around  74% test accuracy . Reasons for the fluctuations in accuracy can be explained by the rather small dataset (only 38 test graphs), and usually disappear once one applies GNNs to larger datasets. (Optional) Exercise Can we do better than this? As multiple papers pointed out ( Xu et al. (2018) ,  Morris et al. (2018) ), applying  neighborhood normalization decreases the expressivity of GNNs in distinguishing certain graph structures . An alternative formulation ( Morris et al. (2018) ) omits neighborhood normalization completely and adds a simple skip-connection to the GNN layer in order to preserve central node information: $$\\mathbf{x}_i^{(\\ell+1)} = \\mathbf{W}^{(\\ell + 1)}_1 \\mathbf{x}_i^{(\\ell)} + \\mathbf{W}^{(\\ell + 1)}_2 \\sum_{j \\in \\mathcal{N}(i)} \\mathbf{x}_j^{(\\ell)}$$ This layer is implemented under the name  GraphConv  in GraphNeuralNetworks.jl. As an exercise, you are invited to complete the following code to the extent that it makes use of  GraphConv  rather than  GCNConv . This should bring you close to  82% test accuracy ."},{"id":260,"pagetitle":"Graph classification","title":"Conclusion","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/tutorials/graph_classification_pluto/#Conclusion","content":" Conclusion In this chapter, you have learned how to apply GNNs to the task of graph classification. You have learned how graphs can be batched together for better GPU utilization, and how to apply readout layers for obtaining graph embeddings rather than node embeddings."},{"id":263,"pagetitle":"Node classification","title":"Import","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/tutorials/node_classification_pluto/#Import","content":" Import Let us start off by importing some libraries. We will be using Flux.jl and  GraphNeuralNetworks.jl  for our tutorial. begin\n    using MLDatasets\n    using GraphNeuralNetworks\n    using Flux\n    using Flux: onecold, onehotbatch, logitcrossentropy\n    using Plots\n    using PlutoUI\n    using TSne\n    using Random\n    using Statistics\n\n    ENV[\"DATADEPS_ALWAYS_ACCEPT\"] = \"true\"\n    Random.seed!(17) # for reproducibility\nend;"},{"id":264,"pagetitle":"Node classification","title":"Visualize","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/tutorials/node_classification_pluto/#Visualize","content":" Visualize We want to visualize the outputs of the results using t-distributed stochastic neighbor embedding (tsne) to embed our output embeddings onto a 2D plane. function visualize_tsne(out, targets)\n    z = tsne(out, 2)\n    scatter(z[:, 1], z[:, 2], color = Int.(targets[1:size(z, 1)]), leg = false)\nend visualize_tsne (generic function with 1 method)"},{"id":265,"pagetitle":"Node classification","title":"Dataset: Cora","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/tutorials/node_classification_pluto/#Dataset:-Cora","content":" Dataset: Cora For our tutorial, we will be using the  Cora  dataset.  Cora  is a citation network of 2708 documents classified into one of seven classes and 5429 links. Each node represent articles/documents and the edges between these nodes if one of them cite each other. Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary. The dictionary consists of 1433 unique words. This dataset was first introduced by  Yang et al. (2016)  as one of the datasets of the  Planetoid  benchmark suite. We will be using  MLDatasets.jl  for an easy access to this dataset. dataset = Cora() dataset Cora:\n  metadata  =>    Dict{String, Any} with 3 entries\n  graphs    =>    1-element Vector{MLDatasets.Graph} Datasets in MLDatasets.jl have  metadata  containing information about the dataset itself. dataset.metadata Dict{String, Any} with 3 entries:\n  \"name\"        => \"cora\"\n  \"classes\"     => [1, 2, 3, 4, 5, 6, 7]\n  \"num_classes\" => 7 The  graphs  variable GraphDataset contains the graph. The  Cora  dataset contains only 1 graph. dataset.graphs 1-element Vector{MLDatasets.Graph}:\n Graph(2708, 10556) There is only one graph of the dataset. The  node_data  contains  features  indicating if certain words are present or not and  targets  indicating the class for each document. We convert the single-graph dataset to a  GNNGraph . g = mldataset2gnngraph(dataset) GNNGraph:\n  num_nodes: 2708\n  num_edges: 10556\n  ndata:\n\tval_mask = 2708-element BitVector\n\ttargets = 2708-element Vector{Int64}\n\ttest_mask = 2708-element BitVector\n\tfeatures = 1433×2708 Matrix{Float32}\n\ttrain_mask = 2708-element BitVector with_terminal() do\n    # Gather some statistics about the graph.\n    println(\"Number of nodes: $(g.num_nodes)\")\n    println(\"Number of edges: $(g.num_edges)\")\n    println(\"Average node degree: $(g.num_edges / g.num_nodes)\")\n    println(\"Number of training nodes: $(sum(g.ndata.train_mask))\")\n    println(\"Training node label rate: $(mean(g.ndata.train_mask))\")\n    # println(\"Has isolated nodes: $(has_isolated_nodes(g))\")\n    println(\"Has self-loops: $(has_self_loops(g))\")\n    println(\"Is undirected: $(is_bidirected(g))\")\nend Number of nodes: 2708\nNumber of edges: 10556\nAverage node degree: 3.8980797636632203\nNumber of training nodes: 140\nTraining node label rate: 0.051698670605613\nHas self-loops: false\nIs undirected: true\n Overall, this dataset is quite similar to the previously used  KarateClub  network. We can see that the  Cora  network holds 2,708 nodes and 10,556 edges, resulting in an average node degree of 3.9. For training this dataset, we are given the ground-truth categories of 140 nodes (20 for each class). This results in a training node label rate of only 5%. We can further see that this network is undirected, and that there exists no isolated nodes (each document has at least one citation). begin\n    x = g.ndata.features\n    # we onehot encode both the node labels (what we want to predict):\n    y = onehotbatch(g.ndata.targets, 1:7)\n    train_mask = g.ndata.train_mask\n    num_features = size(x)[1]\n    hidden_channels = 16\n    num_classes = dataset.metadata[\"num_classes\"]\nend;"},{"id":266,"pagetitle":"Node classification","title":"Multi-layer Perception Network (MLP)","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/tutorials/node_classification_pluto/#Multi-layer-Perception-Network-(MLP)","content":" Multi-layer Perception Network (MLP) In theory, we should be able to infer the category of a document solely based on its content,  i.e.  its bag-of-words feature representation, without taking any relational information into account. Let's verify that by constructing a simple MLP that solely operates on input node features (using shared weights across all nodes): begin\n    struct MLP\n        layers::NamedTuple\n    end\n\n    Flux.@layer :expand MLP\n\n    function MLP(num_features, num_classes, hidden_channels; drop_rate = 0.5)\n        layers = (hidden = Dense(num_features => hidden_channels),\n                  drop = Dropout(drop_rate),\n                  classifier = Dense(hidden_channels => num_classes))\n        return MLP(layers)\n    end\n\n    function (model::MLP)(x::AbstractMatrix)\n        l = model.layers\n        x = l.hidden(x)\n        x = relu(x)\n        x = l.drop(x)\n        x = l.classifier(x)\n        return x\n    end\nend Training a Multilayer Perceptron Our MLP is defined by two linear layers and enhanced by  ReLU  non-linearity and  Dropout . Here, we first reduce the 1433-dimensional feature vector to a low-dimensional embedding ( hidden_channels=16 ), while the second linear layer acts as a classifier that should map each low-dimensional node embedding to one of the 7 classes. Let's train our simple MLP by following a similar procedure as described in  the first part of this tutorial . We again make use of the  cross entropy loss  and  Adam optimizer . This time, we also define a  accuracy  function  to evaluate how well our final model performs on the test node set (which labels have not been observed during training). function train(model::MLP, data::AbstractMatrix, epochs::Int, opt)\n    Flux.trainmode!(model)\n\n    for epoch in 1:epochs\n        loss, grad = Flux.withgradient(model) do model\n            ŷ = model(data)\n            logitcrossentropy(ŷ[:, train_mask], y[:, train_mask])\n        end\n\n        Flux.update!(opt, model, grad[1])\n        if epoch % 200 == 0\n            @show epoch, loss\n        end\n    end\nend train (generic function with 1 method) function accuracy(model::MLP, x::AbstractMatrix, y::Flux.OneHotArray, mask::BitVector)\n    Flux.testmode!(model)\n    mean(onecold(model(x))[mask] .== onecold(y)[mask])\nend accuracy (generic function with 1 method) begin\n    mlp = MLP(num_features, num_classes, hidden_channels)\n    opt_mlp = Flux.setup(Adam(1e-3), mlp)\n    epochs = 2000\n    train(mlp, g.ndata.features, epochs, opt_mlp)\nend After training the model, we can call the  accuracy  function to see how well our model performs on unseen labels. Here, we are interested in the accuracy of the model,  i.e. , the ratio of correctly classified nodes: accuracy(mlp, g.ndata.features, y, .!train_mask) 0.45872274143302183 As one can see, our MLP performs rather bad with only about 47% test accuracy. But why does the MLP do not perform better? The main reason for that is that this model suffers from heavy overfitting due to only having access to a  small amount of training nodes , and therefore generalizes poorly to unseen node representations. It also fails to incorporate an important bias into the model:  Cited papers are very likely related to the category of a document . That is exactly where Graph Neural Networks come into play and can help to boost the performance of our model."},{"id":267,"pagetitle":"Node classification","title":"Training a Graph Convolutional Neural Network (GNN)","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/tutorials/node_classification_pluto/#Training-a-Graph-Convolutional-Neural-Network-(GNN)","content":" Training a Graph Convolutional Neural Network (GNN) Following-up on  the first part of this tutorial , we replace the  Dense  linear layers by the  GCNConv  module. To recap, the  GCN layer  ( Kipf et al. (2017) ) is defined as $$\\mathbf{x}_v^{(\\ell + 1)} = \\mathbf{W}^{(\\ell + 1)} \\sum_{w \\in \\mathcal{N}(v) \\, \\cup \\, \\{ v \\}} \\frac{1}{c_{w,v}} \\cdot \\mathbf{x}_w^{(\\ell)}$$ where  \\(\\mathbf{W}^{(\\ell + 1)}\\)  denotes a trainable weight matrix of shape  [num_output_features, num_input_features]  and  \\(c_{w,v}\\)  refers to a fixed normalization coefficient for each edge. In contrast, a single  Linear  layer is defined as $$\\mathbf{x}_v^{(\\ell + 1)} = \\mathbf{W}^{(\\ell + 1)} \\mathbf{x}_v^{(\\ell)}$$ which does not make use of neighboring node information. begin\n    struct GCN\n        layers::NamedTuple\n    end\n\n    Flux.@layer GCN # provides parameter collection, gpu movement and more\n\n    function GCN(num_features, num_classes, hidden_channels; drop_rate = 0.5)\n        layers = (conv1 = GCNConv(num_features => hidden_channels),\n                  drop = Dropout(drop_rate),\n                  conv2 = GCNConv(hidden_channels => num_classes))\n        return GCN(layers)\n    end\n\n    function (gcn::GCN)(g::GNNGraph, x::AbstractMatrix)\n        l = gcn.layers\n        x = l.conv1(g, x)\n        x = relu.(x)\n        x = l.drop(x)\n        x = l.conv2(g, x)\n        return x\n    end\nend Now let's visualize the node embeddings of our  untrained  GCN network. begin\n    gcn = GCN(num_features, num_classes, hidden_channels)\n    h_untrained = gcn(g, x) |> transpose\n    visualize_tsne(h_untrained, g.ndata.targets)\nend We certainly can do better by training our model. The training and testing procedure is once again the same, but this time we make use of the node features  x and  the graph  g  as input to our GCN model. function train(model::GCN, g::GNNGraph, x::AbstractMatrix, epochs::Int, opt)\n    Flux.trainmode!(model)\n\n    for epoch in 1:epochs\n        loss, grad = Flux.withgradient(model) do model\n            ŷ = model(g, x)\n            logitcrossentropy(ŷ[:, train_mask], y[:, train_mask])\n        end\n\n        Flux.update!(opt, model, grad[1])\n        if epoch % 200 == 0\n            @show epoch, loss\n        end\n    end\nend train (generic function with 2 methods) function accuracy(model::GCN, g::GNNGraph, x::AbstractMatrix, y::Flux.OneHotArray,\n                  mask::BitVector)\n    Flux.testmode!(model)\n    mean(onecold(model(g, x))[mask] .== onecold(y)[mask])\nend accuracy (generic function with 2 methods) begin\n    opt_gcn = Flux.setup(Adam(1e-2), gcn)\n    train(gcn, g, x, epochs, opt_gcn)\nend Now let's evaluate the loss of our trained GCN. with_terminal() do\n    train_accuracy = accuracy(gcn, g, g.ndata.features, y, train_mask)\n    test_accuracy = accuracy(gcn, g, g.ndata.features, y, .!train_mask)\n\n    println(\"Train accuracy: $(train_accuracy)\")\n    println(\"Test accuracy: $(test_accuracy)\")\nend Train accuracy: 1.0\nTest accuracy: 0.7706386292834891\n There it is!  By simply swapping the linear layers with GNN layers, we can reach  75.77% of test accuracy ! This is in stark contrast to the 59% of test accuracy obtained by our MLP, indicating that relational information plays a crucial role in obtaining better performance. We can also verify that once again by looking at the output embeddings of our trained model, which now produces a far better clustering of nodes of the same category. begin\n    Flux.testmode!(gcn) # inference mode\n\n    out_trained = gcn(g, x) |> transpose\n    visualize_tsne(out_trained, g.ndata.targets)\nend"},{"id":268,"pagetitle":"Node classification","title":"(Optional) Exercises","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/tutorials/node_classification_pluto/#(Optional)-Exercises","content":" (Optional) Exercises To achieve better model performance and to avoid overfitting, it is usually a good idea to select the best model based on an additional validation set. The  Cora  dataset provides a validation node set as  g.ndata.val_mask , but we haven't used it yet. Can you modify the code to select and test the model with the highest validation performance? This should bring test performance to  82% accuracy . How does  GCN  behave when increasing the hidden feature dimensionality or the number of layers? Does increasing the number of layers help at all? You can try to use different GNN layers to see how model performance changes. What happens if you swap out all  GCNConv  instances with  GATConv  layers that make use of attention? Try to write a 2-layer  GAT  model that makes use of 8 attention heads in the first layer and 1 attention head in the second layer, uses a  dropout  ratio of  0.6  inside and outside each  GATConv  call, and uses a  hidden_channels  dimensions of  8  per head."},{"id":269,"pagetitle":"Node classification","title":"Conclusion","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/tutorials/node_classification_pluto/#Conclusion","content":" Conclusion In this tutorial, we have seen how to apply GNNs to real-world problems, and, in particular, how they can effectively be used for boosting a model's performance. In the next tutorial, we will look into how GNNs can be used for the task of graph classification."},{"id":272,"pagetitle":"Temporal graph classification","title":"Import","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/tutorials/temporal_graph_classification_pluto/#Import","content":" Import We start by importing the necessary libraries. We use  GraphNeuralNetworks.jl ,  Flux.jl  and  MLDatasets.jl , among others. begin\n    using Flux\n    using GraphNeuralNetworks\n    using Statistics, Random\n    using LinearAlgebra\n    using MLDatasets: TemporalBrains\n    using CUDA\n    using cuDNN\nend"},{"id":273,"pagetitle":"Temporal graph classification","title":"Dataset: TemporalBrains","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/tutorials/temporal_graph_classification_pluto/#Dataset:-TemporalBrains","content":" Dataset: TemporalBrains The TemporalBrains dataset contains a collection of functional brain connectivity networks from 1000 subjects obtained from resting-state functional MRI data from the  Human Connectome Project (HCP) .  Functional connectivity is defined as the temporal dependence of neuronal activation patterns of anatomically separated brain regions. The graph nodes represent brain regions and their number is fixed at 102 for each of the 27 snapshots, while the edges, representing functional connectivity, change over time. For each snapshot, the feature of a node represents the average activation of the node during that snapshot. Each temporal graph has a label representing gender ('M' for male and 'F' for female) and age group (22-25, 26-30, 31-35, and 36+). The network's edge weights are binarized, and the threshold is set to 0.6 by default. brain_dataset = TemporalBrains() dataset TemporalBrains:\n  graphs  =>    1000-element Vector{MLDatasets.TemporalSnapshotsGraph} After loading the dataset from the MLDatasets.jl package, we see that there are 1000 graphs and we need to convert them to the  TemporalSnapshotsGNNGraph  format. So we create a function called  data_loader  that implements the latter and splits the dataset into the training set that will be used to train the model and the test set that will be used to test the performance of the model. function data_loader(brain_dataset)\n    graphs = brain_dataset.graphs\n    dataset = Vector{TemporalSnapshotsGNNGraph}(undef, length(graphs))\n    for i in 1:length(graphs)\n        graph = graphs[i]\n        dataset[i] = TemporalSnapshotsGNNGraph(GraphNeuralNetworks.mlgraph2gnngraph.(graph.snapshots))\n        # Add graph and node features\n        for t in 1:27\n            s = dataset[i].snapshots[t]\n            s.ndata.x = [I(102); s.ndata.x']\n        end\n        dataset[i].tgdata.g = Float32.(Flux.onehot(graph.graph_data.g, [\"F\", \"M\"]))\n    end\n    # Split the dataset into a 80% training set and a 20% test set\n    train_loader = dataset[1:200]\n    test_loader = dataset[201:250]\n    return train_loader, test_loader\nend; The first part of the  data_loader  function calls the  mlgraph2gnngraph  function for each snapshot, which takes the graph and converts it to a  GNNGraph . The vector of  GNNGraph s is then rewritten to a  TemporalSnapshotsGNNGraph . The second part adds the graph and node features to the temporal graphs, in particular it adds the one-hot encoding of the label of the graph (in this case we directly use the identity matrix) and appends the mean activation of the node of the snapshot (which is contained in the vector  dataset[i].snapshots[t].ndata.x , where  i  is the index indicating the subject and  t  is the snapshot). For the graph feature, it adds the one-hot encoding of gender. The last part splits the dataset."},{"id":274,"pagetitle":"Temporal graph classification","title":"Model","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/tutorials/temporal_graph_classification_pluto/#Model","content":" Model We now implement a simple model that takes a  TemporalSnapshotsGNNGraph  as input. It consists of a  GINConv  applied independently to each snapshot, a  GlobalPool  to get an embedding for each snapshot, a pooling on the time dimension to get an embedding for the whole temporal graph, and finally a  Dense  layer. First, we start by adapting the  GlobalPool  to the  TemporalSnapshotsGNNGraphs . function (l::GlobalPool)(g::TemporalSnapshotsGNNGraph, x::AbstractVector)\n    h = [reduce_nodes(l.aggr, g[i], x[i]) for i in 1:(g.num_snapshots)]\n    sze = size(h[1])\n    reshape(reduce(hcat, h), sze[1], length(h))\nend Then we implement the constructor of the model, which we call  GenderPredictionModel , and the foward pass. begin\n    struct GenderPredictionModel\n        gin::GINConv\n        mlp::Chain\n        globalpool::GlobalPool\n        f::Function\n        dense::Dense\n    end\n    \n    Flux.@layer GenderPredictionModel\n    \n    function GenderPredictionModel(; nfeatures = 103, nhidden = 128, activation = relu)\n        mlp = Chain(Dense(nfeatures, nhidden, activation), Dense(nhidden, nhidden, activation))\n        gin = GINConv(mlp, 0.5)\n        globalpool = GlobalPool(mean)\n        f = x -> mean(x, dims = 2)\n        dense = Dense(nhidden, 2)\n        GenderPredictionModel(gin, mlp, globalpool, f, dense)\n    end\n    \n    function (m::GenderPredictionModel)(g::TemporalSnapshotsGNNGraph)\n        h = m.gin(g, g.ndata.x)\n        h = m.globalpool(g, h)\n        h = m.f(h)\n        m.dense(h)\n    end\n    \nend"},{"id":275,"pagetitle":"Temporal graph classification","title":"Training","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/tutorials/temporal_graph_classification_pluto/#Training","content":" Training We train the model for 100 epochs, using the Adam optimizer with a learning rate of 0.001. We use the  logitbinarycrossentropy  as the loss function, which is typically used as the loss in two-class classification, where the labels are given in a one-hot format. The accuracy expresses the number of correct classifications.  lossfunction(ŷ, y) = Flux.logitbinarycrossentropy(ŷ, y); function eval_loss_accuracy(model, data_loader)\n    error = mean([lossfunction(model(g), g.tgdata.g) for g in data_loader])\n    acc = mean([round(100 * mean(Flux.onecold(model(g)) .==     Flux.onecold(g.tgdata.g)); digits = 2) for g in data_loader])\n    return (loss = error, acc = acc)\nend; function train(dataset; usecuda::Bool, kws...)\n\n    if usecuda && CUDA.functional() #check if GPU is available \n        my_device = gpu\n        @info \"Training on GPU\"\n    else\n        my_device = cpu\n        @info \"Training on CPU\"\n    end\n    \n    function report(epoch)\n        train_loss, train_acc = eval_loss_accuracy(model, train_loader)\n        test_loss, test_acc = eval_loss_accuracy(model, test_loader)\n        println(\"Epoch: $epoch  $((; train_loss, train_acc))  $((; test_loss, test_acc))\")\n        return (train_loss, train_acc, test_loss, test_acc)\n    end\n\n    model = GenderPredictionModel() |> my_device\n\n    opt = Flux.setup(Adam(1.0f-3), model)\n\n    train_loader, test_loader = data_loader(dataset)\n    train_loader = train_loader |> my_device\n    test_loader = test_loader |> my_device\n\n    report(0)\n    for epoch in 1:100\n        for g in train_loader\n            grads = Flux.gradient(model) do model\n                ŷ = model(g)\n                lossfunction(vec(ŷ), g.tgdata.g)\n            end\n            Flux.update!(opt, model, grads[1])\n        end\n        if  epoch % 10 == 0\n            report(epoch)\n        end\n    end\n    return model\nend;\n train(brain_dataset; usecuda = true) GenderPredictionModel(GINConv(Chain(Dense(103 => 128, relu), Dense(128 => 128, relu)), 0.5), Chain(Dense(103 => 128, relu), Dense(128 => 128, relu)), GlobalPool{typeof(mean)}(Statistics.mean), var\"#4#5\"(), Dense(128 => 2))  # 30_082 parameters, plus 29_824 non-trainable We set up the training on the GPU because training takes a lot of time, especially when working on the CPU."},{"id":276,"pagetitle":"Temporal graph classification","title":"Conclusions","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/tutorials/temporal_graph_classification_pluto/#Conclusions","content":" Conclusions In this tutorial, we implemented a very simple architecture to classify temporal graphs in the context of gender classification using brain data. We then trained the model on the GPU for 100 epochs on the TemporalBrains dataset. The accuracy of the model is approximately 75-80%, but can be improved by fine-tuning the parameters and training on more data."},{"id":279,"pagetitle":"Node autoregression","title":"Import","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/tutorials/traffic_prediction/#Import","content":" Import We start by importing the necessary libraries. We use  GraphNeuralNetworks.jl ,  Flux.jl  and  MLDatasets.jl , among others. begin\n    using GraphNeuralNetworks\n    using Flux\n    using Flux.Losses: mae\n    using MLDatasets: METRLA\n    using Statistics\n    using Plots\nend"},{"id":280,"pagetitle":"Node autoregression","title":"Dataset: METR-LA","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/tutorials/traffic_prediction/#Dataset:-METR-LA","content":" Dataset: METR-LA We use the  METR-LA  dataset from the paper  Diffusion Convolutional Recurrent Neural Network: Data-driven Traffic Forecasting , which contains traffic data from loop detectors in the highway of Los Angeles County. The dataset contains traffic speed data from March 1, 2012 to June 30, 2012. The data is collected every 5 minutes, resulting in 12 observations per hour, from 207 sensors. Each sensor is a node in the graph, and the edges represent the distances between the sensors. dataset_metrla = METRLA(; num_timesteps = 3) dataset METRLA:\n  graphs  =>    1-element Vector{MLDatasets.Graph}  g = dataset_metrla[1] Graph:\n  num_nodes   =>    207\n  num_edges   =>    1722\n  edge_index  =>    (\"1722-element Vector{Int64}\", \"1722-element Vector{Int64}\")\n  node_data   =>    (features = \"34269-element Vector{Any}\", targets = \"34269-element Vector{Any}\")\n  edge_data   =>    1722-element Vector{Float32} edge_data  contains the weights of the edges of the graph and  node_data  contains a node feature vector and a target vector. The latter vectors contain batches of dimension  num_timesteps , which means that they contain vectors with the node features and targets of  num_timesteps  time steps. Two consecutive batches are shifted by one-time step. The node features are the traffic speed of the sensors and the time of the day, and the targets are the traffic speed of the sensors in the next time step. Let's see some examples: size(g.node_data.features[1]) (2, 207, 3) The first dimension correspond to the two features (first line the speed value and the second line the time of the day), the second to the nodes and the third to the number of timestep  num_timesteps . size(g.node_data.targets[1]) (1, 207, 3) In the case of the targets the first dimension is 1 because they store just the speed value. g.node_data.features[1][:,1,:] 2×3 Matrix{Float32}:\n  1.17081    1.11647   1.15888\n -0.876741  -0.87663  -0.87652 g.node_data.features[2][:,1,:] 2×3 Matrix{Float32}:\n  1.11647   1.15888  -0.876741\n -0.87663  -0.87652  -0.87641 g.node_data.targets[1][:,1,:] 1×3 Matrix{Float32}:\n 1.11647  1.15888  -0.876741 function plot_data(data,sensor)\n    p = plot(legend=false, xlabel=\"Time (h)\", ylabel=\"Normalized speed\")\n    plotdata = []\n    for i in 1:3:length(data)\n        push!(plotdata,data[i][1,sensor,:])\n    end\n    plotdata = reduce(vcat,plotdata)\n    plot!(p, collect(1:length(data)), plotdata, color = :green, xticks =([i for i in 0:50:250], [\"$(i)\" for i in 0:4:24]))\n    return p\nend plot_data (generic function with 1 method) plot_data(g.node_data.features[1:288],1) Now let's construct the static graph, the temporal features and targets from the dataset. begin\n    graph = GNNGraph(g.edge_index; edata = g.edge_data, g.num_nodes)\n    features = g.node_data.features\n    targets = g.node_data.targets\nend;   Now let's construct the  train_loader  and  data_loader . begin\n    train_loader = zip(features[1:200], targets[1:200])\n    test_loader = zip(features[2001:2288], targets[2001:2288])\nend;"},{"id":281,"pagetitle":"Node autoregression","title":"Model: T-GCN","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/tutorials/traffic_prediction/#Model:-T-GCN","content":" Model: T-GCN We use the T-GCN model from the paper  T-GCN: A Temporal Graph Convolutional Network for Traffic Prediction , which consists of a graph convolutional network (GCN) and a gated recurrent unit (GRU). The GCN is used to capture spatial features from the graph, and the GRU is used to capture temporal features from the feature time series. model = GNNChain(TGCN(2 => 100), Dense(100, 1)) GNNChain(Recur(TGCNCell(2 => 100)), Dense(100 => 1))"},{"id":282,"pagetitle":"Node autoregression","title":"Training","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/tutorials/traffic_prediction/#Training","content":" Training We train the model for 100 epochs, using the Adam optimizer with a learning rate of 0.001. We use the mean absolute error (MAE) as the loss function. function train(graph, train_loader, model)\n\n    opt = Flux.setup(Adam(0.001), model)\n\n    for epoch in 1:100\n        for (x, y) in train_loader\n            x, y = (x, y)\n            grads = Flux.gradient(model) do model\n                ŷ = model(graph, x)\n                Flux.mae(ŷ, y) \n            end\n            Flux.update!(opt, model, grads[1])\n        end\n        \n        if epoch % 10 == 0\n            loss = mean([Flux.mae(model(graph,x), y) for (x, y) in train_loader])\n            @show epoch, loss\n        end\n    end\n    return model\nend train (generic function with 1 method) train(graph, train_loader, model) GNNChain(Recur(TGCNCell(2 => 100)), Dense(100 => 1)) function plot_predicted_data(graph,features,targets, sensor)\n    p = plot(xlabel=\"Time (h)\", ylabel=\"Normalized speed\")\n    prediction = []\n    grand_truth = []\n    for i in 1:3:length(features)\n        push!(grand_truth,targets[i][1,sensor,:])\n        push!(prediction, model(graph, features[i])[1,sensor,:]) \n    end\n    prediction = reduce(vcat,prediction)\n    grand_truth = reduce(vcat, grand_truth)\n    plot!(p, collect(1:length(features)), grand_truth, color = :blue, label = \"Grand Truth\", xticks =([i for i in 0:50:250], [\"$(i)\" for i in 0:4:24]))\n    plot!(p, collect(1:length(features)), prediction, color = :red, label= \"Prediction\")\n    return p\nend plot_predicted_data (generic function with 1 method) plot_predicted_data(graph,features[301:588],targets[301:588], 1) accuracy(ŷ, y) = 1 - Statistics.norm(y-ŷ)/Statistics.norm(y) accuracy (generic function with 1 method) mean([accuracy(model(graph,x), y) for (x, y) in test_loader]) 0.47803628f0 The accuracy is not very good but can be improved by training using more data. We used a small subset of the dataset for this tutorial because of the computational cost of training the model. From the plot of the predictions, we can see that the model is able to capture the general trend of the traffic speed, but it is not able to capture the peaks of the traffic."},{"id":283,"pagetitle":"Node autoregression","title":"Conclusion","ref":"/GraphNeuralNetworks.jl/docs/GraphNeuralNetworks.jl/stable/tutorials/traffic_prediction/#Conclusion","content":" Conclusion In this tutorial, we learned how to use a recurrent temporal graph convolutional network to predict traffic in a spatio-temporal setting. We used the TGCN model, which consists of a graph convolutional network (GCN) and a gated recurrent unit (GRU). We then trained the model for 100 epochs on a small subset of the METR-LA dataset. The accuracy of the model is not very good, but it can be improved by training on more data."},{"id":286,"pagetitle":"Home","title":"GNNLux.jl","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/#GNNLux.jl","content":" GNNLux.jl GNNLux.jl is a package that implements graph convolutional layers fully compatible with the  Lux.jl  deep learning framework. It is built on top of the GNNGraphs.jl, GNNlib.jl, and Lux.jl packages. See  GraphNeuralNetworks.jl  instead for a   Flux.jl -based implementation of graph neural networks."},{"id":287,"pagetitle":"Home","title":"Installation","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/#Installation","content":" Installation GNNLux.jl is a registered Julia package. You can easily install it through the package manager : pkg> add GNNLux"},{"id":288,"pagetitle":"Home","title":"Package overview","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/#Package-overview","content":" Package overview Let's give a brief overview of the package by solving a graph regression problem with synthetic data. "},{"id":289,"pagetitle":"Home","title":"Data preparation","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/#Data-preparation","content":" Data preparation We generate a dataset of multiple random graphs with associated data features, then split it into training and testing sets. using GNNLux, Lux, Statistics, MLUtils, Random\nusing Zygote, Optimisers\n\nrng = Random.default_rng()\n\nall_graphs = GNNGraph[]\n\nfor _ in 1:1000\n    g = rand_graph(rng, 10, 40,  \n            ndata=(; x = randn(rng, Float32, 16,10)),  # Input node features\n            gdata=(; y = randn(rng, Float32)))         # Regression target   \n    push!(all_graphs, g)\nend\n\ntrain_graphs, test_graphs = MLUtils.splitobs(all_graphs, at=0.8)"},{"id":290,"pagetitle":"Home","title":"Model building","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/#Model-building","content":" Model building We concisely define our model as a  GNNLux.GNNChain  containing two graph convolutional layers and initialize the model's parameters and state. model = GNNChain(GCNConv(16 => 64),\n                x -> relu.(x),    \n                Dropout(0.6), \n                GCNConv(64 => 64, relu),\n                x -> mean(x, dims=2),\n                Dense(64, 1)) \n\nps, st = LuxCore.setup(rng, model)"},{"id":291,"pagetitle":"Home","title":"Training","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/#Training","content":" Training Finally, we use a standard Lux training pipeline to fit our dataset. function custom_loss(model, ps, st, tuple)\n    g,x,y = tuple\n    y_pred,st = model(g, x, ps, st)  \n    return MSELoss()(y_pred, y), (layers = st,), 0\nend\n\nfunction train_model!(model, ps, st, train_graphs, test_graphs)\n    train_state = Lux.Training.TrainState(model, ps, st, Adam(0.0001f0))\n    train_loss=0\n    for iter in 1:100\n        for g in train_graphs\n            _, loss, _, train_state = Lux.Training.single_train_step!(AutoZygote(), custom_loss,(g, g.x, g.y), train_state)\n            train_loss += loss\n        end\n\n        train_loss = train_loss/length(train_graphs)\n\n        if iter % 10 == 0\n            st_ = Lux.testmode(train_state.states)\n            test_loss =0\n            for g in test_graphs\n                ŷ, st_ = model(g, g.x, train_state.parameters, st_)\n                st_ = (layers = st_,)\n                test_loss += MSELoss()(g.y,ŷ)\n            end\n            test_loss = test_loss/length(test_graphs)\n\n            @info (; iter, train_loss, test_loss)\n        end\n    end\n\n    return model, ps, st\nend\n\ntrain_model!(model, ps, st, train_graphs, test_graphs)"},{"id":294,"pagetitle":"GNNGraphs.jl","title":"GNNGraphs.jl","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/#GNNGraphs.jl","content":" GNNGraphs.jl GNNGraphs.jl is a package that provides graph data structures and helper functions specifically designed for working with graph neural networks. This package allows to store not only the graph structure, but also features associated with nodes, edges, and the graph itself. It is the core foundation for the GNNlib.jl, GraphNeuralNetworks.jl, and GNNLux.jl packages. It supports three types of graphs:  Static graph  is the basic graph type represented by  GNNGraph , where each node and edge can have associated features. This type of graph is used in typical graph neural network applications, where neural networks operate on both the structure of the graph and the features stored in it. It can be used to represent a graph where the structure does not change over time, but the features of the nodes and edges can change over time. Heterogeneous graph  is a graph that supports multiple types of nodes and edges, and is represented by  GNNHeteroGraph . Each type can have its own properties and relationships. This is useful in scenarios with different entities and interactions, such as in citation graphs or multi-relational data. Temporal graph  is a graph that changes over time, and is represented by  TemporalSnapshotsGNNGraph . Edges and features can change dynamically. This type of graph is useful for applications that involve tracking time-dependent relationships, such as social networks. This package depends on the package  Graphs.jl ."},{"id":295,"pagetitle":"GNNGraphs.jl","title":"Installation","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/#Installation","content":" Installation The package can be installed with the Julia package manager. From the Julia REPL, type  ]  to enter the Pkg REPL mode and run: pkg> add GNNGraphs"},{"id":298,"pagetitle":"Datasets","title":"Datasets","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/datasets/#Datasets","content":" Datasets"},{"id":299,"pagetitle":"Datasets","title":"GNNGraphs.mldataset2gnngraph","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/datasets/#GNNGraphs.mldataset2gnngraph","content":" GNNGraphs.mldataset2gnngraph  —  Function mldataset2gnngraph(dataset) Convert a graph dataset from the package MLDatasets.jl into one or many  GNNGraph s. Examples julia> using MLDatasets, GNNGraphs\n\njulia> mldataset2gnngraph(Cora())\nGNNGraph:\n  num_nodes: 2708\n  num_edges: 10556\n  ndata:\n        val_mask = 2708-element BitVector\n        targets = 2708-element Vector{Int64}\n        test_mask = 2708-element BitVector\n        features = 1433×2708 Matrix{Float32}\n        train_mask = 2708-element BitVector source"},{"id":302,"pagetitle":"GNNGraph","title":"GNNGraph","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/gnngraph/#GNNGraph","content":" GNNGraph Documentation page for the graph type  GNNGraph  provided by GNNGraphs.jl and related methods.  Besides the methods documented here, one can rely on the large set of functionalities given by  Graphs.jl  thanks to the fact that  GNNGraph  inherits from  Graphs.AbstractGraph ."},{"id":303,"pagetitle":"GNNGraph","title":"GNNGraph type","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/gnngraph/#GNNGraph-type","content":" GNNGraph type"},{"id":304,"pagetitle":"GNNGraph","title":"GNNGraphs.GNNGraph","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.GNNGraph","content":" GNNGraphs.GNNGraph  —  Type GNNGraph(data; [graph_type, ndata, edata, gdata, num_nodes, graph_indicator, dir])\nGNNGraph(g::GNNGraph; [ndata, edata, gdata]) A type representing a graph structure that also stores feature arrays associated to nodes, edges, and the graph itself. The feature arrays are stored in the fields  ndata ,  edata , and  gdata  as  DataStore  objects offering a convenient dictionary-like  and namedtuple-like interface. The features can be passed at construction time or added later. A  GNNGraph  can be constructed out of different  data  objects expressing the connections inside the graph. The internal representation type is determined by  graph_type . When constructed from another  GNNGraph , the internal graph representation is preserved and shared. The node/edge/graph features are retained as well, unless explicitely set by the keyword arguments  ndata ,  edata , and  gdata . A  GNNGraph  can also represent multiple graphs batched togheter (see  MLUtils.batch  or  SparseArrays.blockdiag ). The field  g.graph_indicator  contains the graph membership of each node. GNNGraph s are always directed graphs, therefore each edge is defined by a source node and a target node (see  edge_index ). Self loops (edges connecting a node to itself) and multiple edges (more than one edge between the same pair of nodes) are supported. A  GNNGraph  is a Graphs.jl's  AbstractGraph , therefore it supports most functionality from that library. Arguments data : Some data representing the graph topology. Possible type are An adjacency matrix An adjacency list. A tuple containing the source and target vectors (COO representation) A Graphs.jl' graph. graph_type : A keyword argument that specifies               the underlying representation used by the GNNGraph.               Currently supported values are :coo . Graph represented as a tuple  (source, target) , such that the  k -th edge         connects the node  source[k]  to node  target[k] .         Optionally, also edge weights can be given:  (source, target, weights) . :sparse . A sparse adjacency matrix representation. :dense . A dense adjacency matrix representation. Defaults to  :coo , currently the most supported type. dir : The assumed edge direction when given adjacency matrix or adjacency list input data  g .       Possible values are  :out  and  :in . Default  :out . num_nodes : The number of nodes. If not specified, inferred from  g . Default  nothing . graph_indicator : For batched graphs, a vector containing the graph assignment of each node. Default  nothing . ndata : Node features. An array or named tuple of arrays whose last dimension has size  num_nodes . edata : Edge features. An array or named tuple of arrays whose last dimension has size  num_edges . gdata : Graph features. An array or named tuple of arrays whose last dimension has size  num_graphs . Examples using GNNGraphs\n\n# Construct from adjacency list representation\ndata = [[2,3], [1,4,5], [1], [2,5], [2,4]]\ng = GNNGraph(data)\n\n# Number of nodes, edges, and batched graphs\ng.num_nodes  # 5\ng.num_edges  # 10\ng.num_graphs # 1\n\n# Same graph in COO representation\ns = [1,1,2,2,2,3,4,4,5,5]\nt = [2,3,1,4,5,3,2,5,2,4]\ng = GNNGraph(s, t)\n\n# From a Graphs' graph\ng = GNNGraph(erdos_renyi(100, 20))\n\n# Add 2 node feature arrays at creation time\ng = GNNGraph(g, ndata = (x=rand(100, g.num_nodes), y=rand(g.num_nodes)))\n\n# Add 1 edge feature array, after the graph creation\ng.edata.z = rand(16, g.num_edges)\n\n# Add node features and edge features with default names `x` and `e`\ng = GNNGraph(g, ndata = rand(100, g.num_nodes), edata = rand(16, g.num_edges))\n\ng.ndata.x # or just g.x\ng.edata.e # or just g.e\n\n# Collect edges' source and target nodes.\n# Both source and target are vectors of length num_edges\nsource, target = edge_index(g) A  GNNGraph  can be sent to the GPU, for example by using Flux.jl's  gpu  function or MLDataDevices.jl's utilities.  ``` source"},{"id":305,"pagetitle":"GNNGraph","title":"Base.copy","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/gnngraph/#Base.copy","content":" Base.copy  —  Function copy(g::GNNGraph; deep=false) Create a copy of  g . If  deep  is  true , then copy will be a deep copy (equivalent to  deepcopy(g) ), otherwise it will be a shallow copy with the same underlying graph data. source"},{"id":306,"pagetitle":"GNNGraph","title":"DataStore","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/gnngraph/#DataStore","content":" DataStore"},{"id":307,"pagetitle":"GNNGraph","title":"GNNGraphs.DataStore","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.DataStore","content":" GNNGraphs.DataStore  —  Type DataStore([n, data])\nDataStore([n,] k1 = x1, k2 = x2, ...) A container for feature arrays. The optional argument  n  enforces that  numobs(x) == n  for each array contained in the datastore. At construction time, the  data  can be provided as any iterables of pairs of symbols and arrays or as keyword arguments: julia> ds = DataStore(3, x = rand(Float32, 2, 3), y = rand(Float32, 3))\nDataStore(3) with 2 elements:\n  y = 3-element Vector{Float32}\n  x = 2×3 Matrix{Float32}\n\njulia> ds = DataStore(3, Dict(:x => rand(Float32, 2, 3), :y => rand(Float32, 3))); # equivalent to above The  DataStore  has an interface similar to both dictionaries and named tuples. Arrays can be accessed and added using either the indexing or the property syntax: julia> ds = DataStore(x = ones(Float32, 2, 3), y = zeros(Float32, 3))\nDataStore() with 2 elements:\n  y = 3-element Vector{Float32}\n  x = 2×3 Matrix{Float32}\n\njulia> ds.x   # same as `ds[:x]`\n2×3 Matrix{Float32}:\n 1.0  1.0  1.0\n 1.0  1.0  1.0\n\njulia> ds.z = zeros(Float32, 3)  # Add new feature array `z`. Same as `ds[:z] = rand(Float32, 3)`\n3-element Vector{Float32}:\n 0.0\n 0.0\n 0.0 The  DataStore  can be iterated over, and the keys and values can be accessed using  keys(ds)  and  values(ds) .  map(f, ds)  applies the function  f  to each feature array: julia> ds2 = map(x -> x .+ 1, ds)\nDataStore() with 3 elements:\n  y = 3-element Vector{Float32}\n  z = 3-element Vector{Float32}\n  x = 2×3 Matrix{Float32}\n\njulia> ds2.z\n3-element Vector{Float32}:\n 1.0\n 1.0\n 1.0 source"},{"id":308,"pagetitle":"GNNGraph","title":"Query","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/gnngraph/#Query","content":" Query"},{"id":309,"pagetitle":"GNNGraph","title":"GNNGraphs.adjacency_list","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.adjacency_list-Tuple{GNNGraph, Any}","content":" GNNGraphs.adjacency_list  —  Method adjacency_list(g; dir=:out)\nadjacency_list(g, nodes; dir=:out) Return the adjacency list representation (a vector of vectors) of the graph  g . Calling  a  the adjacency list, if  dir=:out  than  a[i]  will contain the neighbors of node  i  through outgoing edges. If  dir=:in , it will contain neighbors from incoming edges instead. If  nodes  is given, return the neighborhood of the nodes in  nodes  only. source"},{"id":310,"pagetitle":"GNNGraph","title":"GNNGraphs.edge_index","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.edge_index-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}}","content":" GNNGraphs.edge_index  —  Method edge_index(g::GNNGraph) Return a tuple containing two vectors, respectively storing  the source and target nodes for each edges in  g . s, t = edge_index(g) source"},{"id":311,"pagetitle":"GNNGraph","title":"GNNGraphs.get_graph_type","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.get_graph_type-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}}","content":" GNNGraphs.get_graph_type  —  Method get_graph_type(g::GNNGraph) Return the underlying representation for the graph  g  as a symbol. Possible values are: :coo : Coordinate list representation. The graph is stored as a tuple of vectors  (s, t, w) ,         where  s  and  t  are the source and target nodes of the edges, and  w  is the edge weights. :sparse : Sparse matrix representation. The graph is stored as a sparse matrix representing the weighted adjacency matrix. :dense : Dense matrix representation. The graph is stored as a dense matrix representing the weighted adjacency matrix. The default representation for graph constructors GNNGraphs.jl is  :coo . The underlying representation can be accessed through the  g.graph  field. See also  GNNGraph . Examples The default representation for graph constructors GNNGraphs.jl is  :coo . julia> g = rand_graph(5, 10)\nGNNGraph:\n  num_nodes: 5\n  num_edges: 10\n\njulia> get_graph_type(g)\n:coo The  GNNGraph  constructor can also be used to create graphs with different representations. julia> g = GNNGraph([2,3,5], [1,2,4], graph_type=:sparse)\nGNNGraph:\n  num_nodes: 5\n  num_edges: 3\n\njulia> g.graph\n5×5 SparseArrays.SparseMatrixCSC{Int64, Int64} with 3 stored entries:\n ⋅  ⋅  ⋅  ⋅  ⋅\n 1  ⋅  ⋅  ⋅  ⋅\n ⋅  1  ⋅  ⋅  ⋅\n ⋅  ⋅  ⋅  ⋅  ⋅\n ⋅  ⋅  ⋅  1  ⋅\n\njulia> get_graph_type(g)\n:sparse\n\njulia> gcoo = GNNGraph(g, graph_type=:coo);\n\njulia> gcoo.graph\n([2, 3, 5], [1, 2, 4], [1, 1, 1]) source"},{"id":312,"pagetitle":"GNNGraph","title":"GNNGraphs.graph_indicator","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.graph_indicator-Tuple{GNNGraph}","content":" GNNGraphs.graph_indicator  —  Method graph_indicator(g::GNNGraph; edges=false) Return a vector containing the graph membership (an integer from  1  to  g.num_graphs ) of each node in the graph. If  edges=true , return the graph membership of each edge instead. source"},{"id":313,"pagetitle":"GNNGraph","title":"GNNGraphs.has_isolated_nodes","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.has_isolated_nodes-Tuple{GNNGraph}","content":" GNNGraphs.has_isolated_nodes  —  Method has_isolated_nodes(g::GNNGraph; dir=:out) Return true if the graph  g  contains nodes with out-degree (if  dir=:out ) or in-degree (if  dir = :in ) equal to zero. source"},{"id":314,"pagetitle":"GNNGraph","title":"GNNGraphs.has_multi_edges","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.has_multi_edges-Tuple{GNNGraph}","content":" GNNGraphs.has_multi_edges  —  Method has_multi_edges(g::GNNGraph) Return  true  if  g  has any multiple edges. source"},{"id":315,"pagetitle":"GNNGraph","title":"GNNGraphs.is_bidirected","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.is_bidirected-Tuple{GNNGraph}","content":" GNNGraphs.is_bidirected  —  Method is_bidirected(g::GNNGraph) Check if the directed graph  g  essentially corresponds to an undirected graph, i.e. if for each edge it also contains the  reverse edge.  source"},{"id":316,"pagetitle":"GNNGraph","title":"GNNGraphs.khop_adj","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.khop_adj","content":" GNNGraphs.khop_adj  —  Function khop_adj(g::GNNGraph,k::Int,T::DataType=eltype(g); dir=:out, weighted=true) Return  $A^k$  where  $A$  is the adjacency matrix of the graph 'g'. source"},{"id":317,"pagetitle":"GNNGraph","title":"GNNGraphs.laplacian_lambda_max","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.laplacian_lambda_max","content":" GNNGraphs.laplacian_lambda_max  —  Function laplacian_lambda_max(g::GNNGraph, T=Float32; add_self_loops=false, dir=:out) Return the largest eigenvalue of the normalized symmetric Laplacian of the graph  g . If the graph is batched from multiple graphs, return the list of the largest eigenvalue for each graph. source"},{"id":318,"pagetitle":"GNNGraph","title":"GNNGraphs.normalized_laplacian","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.normalized_laplacian","content":" GNNGraphs.normalized_laplacian  —  Function normalized_laplacian(g, T=Float32; add_self_loops=false, dir=:out) Normalized Laplacian matrix of graph  g . Arguments g : A  GNNGraph . T : result element type. add_self_loops : add self-loops while calculating the matrix. dir : the edge directionality considered (:out, :in, :both). source"},{"id":319,"pagetitle":"GNNGraph","title":"GNNGraphs.scaled_laplacian","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.scaled_laplacian","content":" GNNGraphs.scaled_laplacian  —  Function scaled_laplacian(g, T=Float32; dir=:out) Scaled Laplacian matrix of graph  g , defined as  $\\hat{L} = \\frac{2}{\\lambda_{max}} L - I$  where  $L$  is the normalized Laplacian matrix. Arguments g : A  GNNGraph . T : result element type. dir : the edge directionality considered (:out, :in, :both). source"},{"id":320,"pagetitle":"GNNGraph","title":"Graphs.LinAlg.adjacency_matrix","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/gnngraph/#Graphs.LinAlg.adjacency_matrix","content":" Graphs.LinAlg.adjacency_matrix  —  Function adjacency_matrix(g::GNNGraph, T=eltype(g); dir=:out, weighted=true) Return the adjacency matrix  A  for the graph  g .  If  dir=:out ,  A[i,j] > 0  denotes the presence of an edge from node  i  to node  j . If  dir=:in  instead,  A[i,j] > 0  denotes the presence of an edge from node  j  to node  i . User may specify the eltype  T  of the returned matrix.  If  weighted=true , the  A  will contain the edge weights if any, otherwise the elements of  A  will be either 0 or 1. source"},{"id":321,"pagetitle":"GNNGraph","title":"Graphs.degree","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/gnngraph/#Graphs.degree-Union{Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}}, Tuple{TT}, Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}, TT}} where TT<:Union{Nothing, Type{<:Number}}","content":" Graphs.degree  —  Method degree(g::GNNGraph, T=nothing; dir=:out, edge_weight=true) Return a vector containing the degrees of the nodes in  g . The gradient is propagated through this function only if  edge_weight  is  true  or a vector. Arguments g : A graph. T : Element type of the returned vector. If  nothing , is      chosen based on the graph type and will be an integer      if  edge_weight = false . Default  nothing . dir : For  dir = :out  the degree of a node is counted based on the outgoing edges.        For  dir = :in , the ingoing edges are used. If  dir = :both  we have the sum of the two. edge_weight : If  true  and the graph contains weighted edges, the degree will                be weighted. Set to  false  instead to just count the number of               outgoing/ingoing edges.                Finally, you can also pass a vector of weights to be used               instead of the graph's own weights.               Default  true . source"},{"id":322,"pagetitle":"GNNGraph","title":"Graphs.has_self_loops","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/gnngraph/#Graphs.has_self_loops-Tuple{GNNGraph}","content":" Graphs.has_self_loops  —  Method has_self_loops(g::GNNGraph) Return  true  if  g  has any self loops. source"},{"id":323,"pagetitle":"GNNGraph","title":"Graphs.inneighbors","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/gnngraph/#Graphs.inneighbors-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}, Integer}","content":" Graphs.inneighbors  —  Method inneighbors(g::GNNGraph, i::Integer) Return the neighbors of node  i  in the graph  g  through incoming edges. See also  neighbors  and  outneighbors . source"},{"id":324,"pagetitle":"GNNGraph","title":"Graphs.outneighbors","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/gnngraph/#Graphs.outneighbors-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}, Integer}","content":" Graphs.outneighbors  —  Method outneighbors(g::GNNGraph, i::Integer) Return the neighbors of node  i  in the graph  g  through outgoing edges. See also  neighbors  and  inneighbors . source"},{"id":325,"pagetitle":"GNNGraph","title":"Graphs.neighbors","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/gnngraph/#Graphs.neighbors-Tuple{GNNGraph, Integer}","content":" Graphs.neighbors  —  Method neighbors(g::GNNGraph, i::Integer; dir=:out) Return the neighbors of node  i  in the graph  g . If  dir=:out , return the neighbors through outgoing edges. If  dir=:in , return the neighbors through incoming edges. See also  outneighbors ,  inneighbors . source"},{"id":326,"pagetitle":"GNNGraph","title":"Transform","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/gnngraph/#Transform","content":" Transform"},{"id":327,"pagetitle":"GNNGraph","title":"GNNGraphs.add_edges","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.add_edges-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}, AbstractVector, AbstractVector}","content":" GNNGraphs.add_edges  —  Method add_edges(g::GNNGraph, s::AbstractVector, t::AbstractVector; [edata])\nadd_edges(g::GNNGraph, (s, t); [edata])\nadd_edges(g::GNNGraph, (s, t, w); [edata]) Add to graph  g  the edges with source nodes  s  and target nodes  t . Optionally, pass the edge weight  w  and the features   edata  for the new edges. Returns a new graph sharing part of the underlying data with  g . If the  s  or  t  contain nodes that are not already present in the graph, they are added to the graph as well. Examples julia> s, t = [1, 2, 3, 3, 4], [2, 3, 4, 4, 4];\n\njulia> w = Float32[1.0, 2.0, 3.0, 4.0, 5.0];\n\njulia> g = GNNGraph((s, t, w))\nGNNGraph:\n  num_nodes: 4\n  num_edges: 5\n\njulia> add_edges(g, ([2, 3], [4, 1], [10.0, 20.0]))\nGNNGraph:\n  num_nodes: 4\n  num_edges: 7 julia> g = GNNGraph()\nGNNGraph:\n  num_nodes: 0\n  num_edges: 0\n\njulia> add_edges(g, [1,2], [2,3])\nGNNGraph:\n  num_nodes: 3\n  num_edges: 2 source"},{"id":328,"pagetitle":"GNNGraph","title":"GNNGraphs.add_nodes","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.add_nodes-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}, Integer}","content":" GNNGraphs.add_nodes  —  Method add_nodes(g::GNNGraph, n; [ndata]) Add  n  new nodes to graph  g . In the  new graph, these nodes will have indexes from  g.num_nodes + 1  to  g.num_nodes + n . source"},{"id":329,"pagetitle":"GNNGraph","title":"GNNGraphs.add_self_loops","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.add_self_loops-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}}","content":" GNNGraphs.add_self_loops  —  Method add_self_loops(g::GNNGraph) Return a graph with the same features as  g  but also adding edges connecting the nodes to themselves. Nodes with already existing self-loops will obtain a second self-loop. If the graphs has edge weights, the new edges will have weight 1. source"},{"id":330,"pagetitle":"GNNGraph","title":"GNNGraphs.getgraph","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.getgraph-Tuple{GNNGraph, Int64}","content":" GNNGraphs.getgraph  —  Method getgraph(g::GNNGraph, i; nmap=false) Return the subgraph of  g  induced by those nodes  j  for which  g.graph_indicator[j] == i  or, if  i  is a collection,  g.graph_indicator[j] ∈ i .  In other words, it extract the component graphs from a batched graph.  If  nmap=true , return also a vector  v  mapping the new nodes to the old ones.  The node  i  in the subgraph will correspond to the node  v[i]  in  g . source"},{"id":331,"pagetitle":"GNNGraph","title":"GNNGraphs.negative_sample","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.negative_sample-Tuple{GNNGraph}","content":" GNNGraphs.negative_sample  —  Method negative_sample(g::GNNGraph; \n                num_neg_edges = g.num_edges, \n                bidirected = is_bidirected(g)) Return a graph containing random negative edges (i.e. non-edges) from graph  g  as edges. If  bidirected=true , the output graph will be bidirected and there will be no leakage from the origin graph.  See also  is_bidirected . source"},{"id":332,"pagetitle":"GNNGraph","title":"GNNGraphs.perturb_edges","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.perturb_edges-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}, AbstractFloat}","content":" GNNGraphs.perturb_edges  —  Method perturb_edges([rng], g::GNNGraph, perturb_ratio) Return a new graph obtained from  g  by adding random edges, based on a specified  perturb_ratio .  The  perturb_ratio  determines the fraction of new edges to add relative to the current number of edges in the graph.  These new edges are added without creating self-loops.  The function returns a new  GNNGraph  instance that shares some of the underlying data with  g  but includes the additional edges.  The nodes for the new edges are selected randomly, and no edge data ( edata ) or weights ( w ) are assigned to these new edges. Arguments g::GNNGraph : The graph to be perturbed. perturb_ratio : The ratio of the number of new edges to add relative to the current number of edges in the graph. For example, a  perturb_ratio  of 0.1 means that 10% of the current number of edges will be added as new random edges. rng : An optionalrandom number generator to ensure reproducible results. Examples julia> g = GNNGraph((s, t, w))\nGNNGraph:\n  num_nodes: 4\n  num_edges: 5\n\njulia> perturbed_g = perturb_edges(g, 0.2)\nGNNGraph:\n  num_nodes: 4\n  num_edges: 6 source"},{"id":333,"pagetitle":"GNNGraph","title":"GNNGraphs.ppr_diffusion","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.ppr_diffusion-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}}","content":" GNNGraphs.ppr_diffusion  —  Method ppr_diffusion(g::GNNGraph{<:COO_T}, alpha =0.85f0) -> GNNGraph Calculates the Personalized PageRank (PPR) diffusion based on the edge weight matrix of a GNNGraph and updates the graph with new edge weights derived from the PPR matrix. References paper:  The pagerank citation ranking: Bringing order to the web The function performs the following steps: Constructs a modified adjacency matrix  A  using the graph's edge weights, where  A  is adjusted by  (α - 1) * A + I , with  α  being the damping factor ( alpha_f32 ) and  I  the identity matrix. Normalizes  A  to ensure each column sums to 1, representing transition probabilities. Applies the PPR formula  α * (I + (α - 1) * A)^-1  to compute the diffusion matrix. Updates the original edge weights of the graph based on the PPR diffusion matrix, assigning new weights for each edge from the PPR matrix. Arguments g::GNNGraph : The input graph for which PPR diffusion is to be calculated. It should have edge weights available. alpha_f32::Float32 : The damping factor used in PPR calculation, controlling the teleport probability in the random walk. Defaults to  0.85f0 . Returns A new  GNNGraph  instance with the same structure as  g  but with updated edge weights according to the PPR diffusion calculation. source"},{"id":334,"pagetitle":"GNNGraph","title":"GNNGraphs.rand_edge_split","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.rand_edge_split-Tuple{GNNGraph, Any}","content":" GNNGraphs.rand_edge_split  —  Method rand_edge_split(g::GNNGraph, frac; bidirected=is_bidirected(g)) -> g1, g2 Randomly partition the edges in  g  to form two graphs,  g1  and  g2 . Both will have the same number of nodes as  g .  g1  will contain a fraction  frac  of the original edges,  while  g2  wil contain the rest. If  bidirected = true  makes sure that an edge and its reverse go into the same split. This option is supported only for bidirected graphs with no self-loops and multi-edges. rand_edge_split  is tipically used to create train/test splits in link prediction tasks. source"},{"id":335,"pagetitle":"GNNGraph","title":"GNNGraphs.random_walk_pe","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.random_walk_pe-Tuple{GNNGraph, Int64}","content":" GNNGraphs.random_walk_pe  —  Method random_walk_pe(g, walk_length) Return the random walk positional encoding from the paper  Graph Neural Networks with Learnable Structural and Positional Representations  of the given graph  g  and the length of the walk  walk_length  as a matrix of size  (walk_length, g.num_nodes) .  source"},{"id":336,"pagetitle":"GNNGraph","title":"GNNGraphs.remove_edges","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.remove_edges-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}, AbstractVector{<:Integer}}","content":" GNNGraphs.remove_edges  —  Method remove_edges(g::GNNGraph, edges_to_remove::AbstractVector{<:Integer})\nremove_edges(g::GNNGraph, p=0.5) Remove specified edges from a GNNGraph, either by specifying edge indices or by randomly removing edges with a given probability. Arguments g : The input graph from which edges will be removed. edges_to_remove : Vector of edge indices to be removed. This argument is only required for the first method. p : Probability of removing each edge. This argument is only required for the second method and defaults to 0.5. Returns A new GNNGraph with the specified edges removed. Example julia> using GNNGraphs\n\n# Construct a GNNGraph\njulia> g = GNNGraph([1, 1, 2, 2, 3], [2, 3, 1, 3, 1])\nGNNGraph:\n  num_nodes: 3\n  num_edges: 5\n  \n# Remove the second edge\njulia> g_new = remove_edges(g, [2]);\n\njulia> g_new\nGNNGraph:\n  num_nodes: 3\n  num_edges: 4\n\n# Remove edges with a probability of 0.5\njulia> g_new = remove_edges(g, 0.5);\n\njulia> g_new\nGNNGraph:\n  num_nodes: 3\n  num_edges: 2 source"},{"id":337,"pagetitle":"GNNGraph","title":"GNNGraphs.remove_multi_edges","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.remove_multi_edges-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}}","content":" GNNGraphs.remove_multi_edges  —  Method remove_multi_edges(g::GNNGraph; aggr=+) Remove multiple edges (also called parallel edges or repeated edges) from graph  g . Possible edge features are aggregated according to  aggr , that can take value   + , min ,  max  or  mean . See also  remove_self_loops ,  has_multi_edges , and  to_bidirected . source"},{"id":338,"pagetitle":"GNNGraph","title":"GNNGraphs.remove_nodes","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.remove_nodes-Tuple{GNNGraph, AbstractFloat}","content":" GNNGraphs.remove_nodes  —  Method remove_nodes(g::GNNGraph, p) Returns a new graph obtained by dropping nodes from  g  with independent probabilities  p .  Examples julia> g = GNNGraph([1, 1, 2, 2, 3, 4], [1, 2, 3, 1, 3, 1])\nGNNGraph:\n  num_nodes: 4\n  num_edges: 6\n\njulia> g_new = remove_nodes(g, 0.5)\nGNNGraph:\n  num_nodes: 2\n  num_edges: 2 source"},{"id":339,"pagetitle":"GNNGraph","title":"GNNGraphs.remove_nodes","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.remove_nodes-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}, AbstractVector}","content":" GNNGraphs.remove_nodes  —  Method remove_nodes(g::GNNGraph, nodes_to_remove::AbstractVector) Remove specified nodes, and their associated edges, from a GNNGraph. This operation reindexes the remaining nodes to maintain a continuous sequence of node indices, starting from 1. Similarly, edges are reindexed to account for the removal of edges connected to the removed nodes. Arguments g : The input graph from which nodes (and their edges) will be removed. nodes_to_remove : Vector of node indices to be removed. Returns A new GNNGraph with the specified nodes and all edges associated with these nodes removed.  Example using GNNGraphs\n\ng = GNNGraph([1, 1, 2, 2, 3], [2, 3, 1, 3, 1])\n\n# Remove nodes with indices 2 and 3, for example\ng_new = remove_nodes(g, [2, 3])\n\n# g_new now does not contain nodes 2 and 3, and any edges that were connected to these nodes.\nprintln(g_new) source"},{"id":340,"pagetitle":"GNNGraph","title":"GNNGraphs.remove_self_loops","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.remove_self_loops-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}}","content":" GNNGraphs.remove_self_loops  —  Method remove_self_loops(g::GNNGraph) Return a graph constructed from  g  where self-loops (edges from a node to itself) are removed.  See also  add_self_loops  and  remove_multi_edges . source"},{"id":341,"pagetitle":"GNNGraph","title":"GNNGraphs.set_edge_weight","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.set_edge_weight-Tuple{GNNGraph, AbstractVector}","content":" GNNGraphs.set_edge_weight  —  Method set_edge_weight(g::GNNGraph, w::AbstractVector) Set  w  as edge weights in the returned graph.  source"},{"id":342,"pagetitle":"GNNGraph","title":"GNNGraphs.to_bidirected","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.to_bidirected-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}}","content":" GNNGraphs.to_bidirected  —  Method to_bidirected(g) Adds a reverse edge for each edge in the graph, then calls   remove_multi_edges  with  mean  aggregation to simplify the graph.  See also  is_bidirected .  Examples julia> s, t = [1, 2, 3, 3, 4], [2, 3, 4, 4, 4];\n\njulia> w = [1.0, 2.0, 3.0, 4.0, 5.0];\n\njulia> e = [10.0, 20.0, 30.0, 40.0, 50.0];\n\njulia> g = GNNGraph(s, t, w, edata = e)\nGNNGraph:\n  num_nodes: 4\n  num_edges: 5\n  edata:\n        e = 5-element Vector{Float64}\n\njulia> g2 = to_bidirected(g)\nGNNGraph:\n  num_nodes: 4\n  num_edges: 7\n  edata:\n        e = 7-element Vector{Float64}\n\njulia> edge_index(g2)\n([1, 2, 2, 3, 3, 4, 4], [2, 1, 3, 2, 4, 3, 4])\n\njulia> get_edge_weight(g2)\n7-element Vector{Float64}:\n 1.0\n 1.0\n 2.0\n 2.0\n 3.5\n 3.5\n 5.0\n\njulia> g2.edata.e\n7-element Vector{Float64}:\n 10.0\n 10.0\n 20.0\n 20.0\n 35.0\n 35.0\n 50.0 source"},{"id":343,"pagetitle":"GNNGraph","title":"GNNGraphs.to_unidirected","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.to_unidirected-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}}","content":" GNNGraphs.to_unidirected  —  Method to_unidirected(g::GNNGraph) Return a graph that for each multiple edge between two nodes in  g  keeps only an edge in one direction. source"},{"id":344,"pagetitle":"GNNGraph","title":"MLUtils.batch","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/gnngraph/#MLUtils.batch-Tuple{AbstractVector{<:GNNGraph}}","content":" MLUtils.batch  —  Method batch(gs::Vector{<:GNNGraph}) Batch together multiple  GNNGraph s into a single one  containing the total number of original nodes and edges. Equivalent to  SparseArrays.blockdiag . See also  MLUtils.unbatch . Examples julia> g1 = rand_graph(4, 4, ndata=ones(Float32, 3, 4))\nGNNGraph:\n  num_nodes: 4\n  num_edges: 4\n  ndata:\n        x = 3×4 Matrix{Float32}\n\njulia> g2 = rand_graph(5, 4, ndata=zeros(Float32, 3, 5))\nGNNGraph:\n  num_nodes: 5\n  num_edges: 4\n  ndata:\n        x = 3×5 Matrix{Float32}\n\njulia> g12 = MLUtils.batch([g1, g2])\nGNNGraph:\n  num_nodes: 9\n  num_edges: 8\n  num_graphs: 2\n  ndata:\n        x = 3×9 Matrix{Float32}\n\njulia> g12.ndata.x\n3×9 Matrix{Float32}:\n 1.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0\n 1.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0\n 1.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0 source"},{"id":345,"pagetitle":"GNNGraph","title":"MLUtils.unbatch","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/gnngraph/#MLUtils.unbatch-Union{Tuple{GNNGraph{T}}, Tuple{T}} where T<:(Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}})","content":" MLUtils.unbatch  —  Method unbatch(g::GNNGraph) Opposite of the  MLUtils.batch  operation, returns  an array of the individual graphs batched together in  g . See also  MLUtils.batch  and  getgraph . Examples julia> using MLUtils\n\njulia> gbatched = MLUtils.batch([rand_graph(5, 6), rand_graph(10, 8), rand_graph(4,2)])\nGNNGraph:\n  num_nodes: 19\n  num_edges: 16\n  num_graphs: 3\n\njulia> MLUtils.unbatch(gbatched)\n3-element Vector{GNNGraph{Tuple{Vector{Int64}, Vector{Int64}, Nothing}}}:\n GNNGraph(5, 6) with no data\n GNNGraph(10, 8) with no data\n GNNGraph(4, 2) with no data source"},{"id":346,"pagetitle":"GNNGraph","title":"SparseArrays.blockdiag","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/gnngraph/#SparseArrays.blockdiag-Tuple{GNNGraph, Vararg{GNNGraph}}","content":" SparseArrays.blockdiag  —  Method blockdiag(xs::GNNGraph...) Equivalent to  MLUtils.batch . source"},{"id":347,"pagetitle":"GNNGraph","title":"Utils","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/gnngraph/#Utils","content":" Utils"},{"id":348,"pagetitle":"GNNGraph","title":"GNNGraphs.sort_edge_index","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.sort_edge_index","content":" GNNGraphs.sort_edge_index  —  Function sort_edge_index(ei::Tuple) -> u', v'\nsort_edge_index(u, v) -> u', v' Return a sorted version of the tuple of vectors  ei = (u, v) , applying a common permutation to  u  and  v . The sorting is lexycographic, that is the pairs  (ui, vi)   are sorted first according to the  ui  and then according to  vi .  source"},{"id":349,"pagetitle":"GNNGraph","title":"GNNGraphs.color_refinement","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.color_refinement","content":" GNNGraphs.color_refinement  —  Function color_refinement(g::GNNGraph, [x0]) -> x, num_colors, niters The color refinement algorithm for graph coloring.  Given a graph  g  and an initial coloring  x0 , the algorithm  iteratively refines the coloring until a fixed point is reached. At each iteration the algorithm computes a hash of the coloring and the sorted list of colors of the neighbors of each node. This hash is used to determine if the coloring has changed. math x_i' = hashmap((x_i, sort([x_j for j \\in N(i)]))). ` This algorithm is related to the 1-Weisfeiler-Lehman algorithm for graph isomorphism testing. Arguments g::GNNGraph : The graph to color. x0::AbstractVector{<:Integer} : The initial coloring. If not provided, all nodes are colored with 1. Returns x::AbstractVector{<:Integer} : The final coloring. num_colors::Int : The number of colors used. niters::Int : The number of iterations until convergence. source"},{"id":350,"pagetitle":"GNNGraph","title":"Generate","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/gnngraph/#Generate","content":" Generate"},{"id":351,"pagetitle":"GNNGraph","title":"GNNGraphs.knn_graph","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.knn_graph-Tuple{AbstractMatrix, Int64}","content":" GNNGraphs.knn_graph  —  Method knn_graph(points::AbstractMatrix, \n          k::Int; \n          graph_indicator = nothing,\n          self_loops = false, \n          dir = :in, \n          kws...) Create a  k -nearest neighbor graph where each node is linked  to its  k  closest  points .   Arguments points : A num features × num nodes matrix storing the Euclidean positions of the nodes. k : The number of neighbors considered in the kNN algorithm. graph_indicator : Either nothing or a vector containing the graph assignment of each node,                     in which case the returned graph will be a batch of graphs.  self_loops : If  true , consider the node itself among its  k  nearest neighbors, in which               case the graph will contain self-loops.  dir : The direction of the edges. If  dir=:in  edges go from the  k          neighbors to the central node. If  dir=:out  we have the opposite        direction. kws : Further keyword arguments will be passed to the  GNNGraph  constructor. Examples julia> n, k = 10, 3;\n\njulia> x = rand(Float32, 3, n);\n\njulia> g = knn_graph(x, k)\nGNNGraph:\n    num_nodes = 10\n    num_edges = 30\n\njulia> graph_indicator = [1,1,1,1,1,2,2,2,2,2];\n\njulia> g = knn_graph(x, k; graph_indicator)\nGNNGraph:\n    num_nodes = 10\n    num_edges = 30\n    num_graphs = 2 source"},{"id":352,"pagetitle":"GNNGraph","title":"GNNGraphs.radius_graph","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.radius_graph-Tuple{AbstractMatrix, AbstractFloat}","content":" GNNGraphs.radius_graph  —  Method radius_graph(points::AbstractMatrix, \n             r::AbstractFloat; \n             graph_indicator = nothing,\n             self_loops = false, \n             dir = :in, \n             kws...) Create a graph where each node is linked  to its neighbors within a given distance  r .   Arguments points : A num features × num nodes matrix storing the Euclidean positions of the nodes. r : The radius. graph_indicator : Either nothing or a vector containing the graph assignment of each node,                     in which case the returned graph will be a batch of graphs.  self_loops : If  true , consider the node itself among its neighbors, in which               case the graph will contain self-loops.  dir : The direction of the edges. If  dir=:in  edges go from the        neighbors to the central node. If  dir=:out  we have the opposite        direction. kws : Further keyword arguments will be passed to the  GNNGraph  constructor. Examples julia> n, r = 10, 0.75;\n\njulia> x = rand(Float32, 3, n);\n\njulia> g = radius_graph(x, r)\nGNNGraph:\n    num_nodes = 10\n    num_edges = 46\n\njulia> graph_indicator = [1,1,1,1,1,2,2,2,2,2];\n\njulia> g = radius_graph(x, r; graph_indicator)\nGNNGraph:\n    num_nodes = 10\n    num_edges = 20\n    num_graphs = 2 References Section B paragraphs 1 and 2 of the paper  Dynamic Hidden-Variable Network Models source"},{"id":353,"pagetitle":"GNNGraph","title":"GNNGraphs.rand_graph","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.rand_graph-Tuple{Integer, Integer}","content":" GNNGraphs.rand_graph  —  Method rand_graph([rng,] n, m; bidirected=true, edge_weight = nothing, kws...) Generate a random (Erdós-Renyi)  GNNGraph  with  n  nodes and  m  edges. If  bidirected=true  the reverse edge of each edge will be present. If  bidirected=false  instead,  m  unrelated edges are generated. In any case, the output graph will contain no self-loops or multi-edges. A vector can be passed  as  edge_weight . Its length has to be equal to  m  in the directed case, and  m÷2  in the bidirected one. Pass a random number generator as the first argument to make the generation reproducible. Additional keyword arguments will be passed to the  GNNGraph  constructor. Examples julia> g = rand_graph(5, 4, bidirected=false)\nGNNGraph:\n  num_nodes: 5\n  num_edges: 4\n\njulia> edge_index(g)\n([4, 3, 2, 1], [5, 4, 3, 2])\n\n# In the bidirected case, edge data will be duplicated on the reverse edges if needed.\njulia> g = rand_graph(5, 4, edata=rand(Float32, 16, 2))\nGNNGraph:\n  num_nodes: 5\n  num_edges: 4\n  edata:\n        e = 16×4 Matrix{Float32}\n\n# Each edge has a reverse\njulia> edge_index(g)\n([1, 1, 5, 3], [5, 3, 1, 1]) source"},{"id":354,"pagetitle":"GNNGraph","title":"Operators","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/gnngraph/#Operators","content":" Operators"},{"id":355,"pagetitle":"GNNGraph","title":"Base.intersect","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/gnngraph/#Base.intersect","content":" Base.intersect  —  Function intersect(g1::GNNGraph, g2::GNNGraph) Intersect two graphs by keeping only the common edges. source"},{"id":356,"pagetitle":"GNNGraph","title":"Sampling","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/gnngraph/#Sampling","content":" Sampling"},{"id":357,"pagetitle":"GNNGraph","title":"GNNGraphs.sample_neighbors","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.sample_neighbors","content":" GNNGraphs.sample_neighbors  —  Function sample_neighbors(g, nodes, K=-1; dir=:in, replace=false, dropnodes=false) Sample neighboring edges of the given nodes and return the induced subgraph. For each node, a number of inbound (or outbound when  dir = :out ) edges will be randomly chosen.  If dropnodes=false`, the graph returned will then contain all the nodes in the original graph,  but only the sampled edges. The returned graph will contain an edge feature  EID  corresponding to the id of the edge in the original graph. If  dropnodes=true , it will also contain a node feature  NID  with the node ids in the original graph. Arguments g . The graph. nodes . A list of node IDs to sample neighbors from. K . The maximum number of edges to be sampled for each node.      If -1, all the neighboring edges will be selected. dir . Determines whether to sample inbound ( :in ) or outbound (` :out ) edges (Default  :in ). replace . If  true , sample with replacement. dropnodes . If  true , the resulting subgraph will contain only the nodes involved in the sampled edges. Examples julia> g = rand_graph(20, 100)\nGNNGraph:\n    num_nodes = 20\n    num_edges = 100\n\njulia> sample_neighbors(g, 2:3)\nGNNGraph:\n    num_nodes = 20\n    num_edges = 9\n    edata:\n        EID => (9,)\n\njulia> sg = sample_neighbors(g, 2:3, dropnodes=true)\nGNNGraph:\n    num_nodes = 10\n    num_edges = 9\n    ndata:\n        NID => (10,)\n    edata:\n        EID => (9,)\n\njulia> sg.ndata.NID\n10-element Vector{Int64}:\n  2\n  3\n 17\n 14\n 18\n 15\n 16\n 20\n  7\n 10\n\njulia> sample_neighbors(g, 2:3, 5, replace=true)\nGNNGraph:\n    num_nodes = 20\n    num_edges = 10\n    edata:\n        EID => (10,) source"},{"id":358,"pagetitle":"GNNGraph","title":"Graphs.induced_subgraph","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/gnngraph/#Graphs.induced_subgraph-Tuple{GNNGraph, Vector{Int64}}","content":" Graphs.induced_subgraph  —  Method induced_subgraph(graph, nodes) Generates a subgraph from the original graph using the provided  nodes .  The function includes the nodes' neighbors and creates edges between nodes that are connected in the original graph.  If a node has no neighbors, an isolated node will be added to the subgraph.  Returns A new  GNNGraph  containing the subgraph with the specified nodes and their features. Arguments graph . The original GNNGraph containing nodes, edges, and node features. nodes `. A vector of node indices to include in the subgraph. Examples julia> s = [1, 2]\n2-element Vector{Int64}:\n 1\n 2\n\njulia> t = [2, 3]\n2-element Vector{Int64}:\n 2\n 3\n\njulia> graph = GNNGraph((s, t), ndata = (; x=rand(Float32, 32, 3), y=rand(Float32, 3)), edata = rand(Float32, 2))\nGNNGraph:\n  num_nodes: 3\n  num_edges: 2\n  ndata:\n        y = 3-element Vector{Float32}\n        x = 32×3 Matrix{Float32}\n  edata:\n        e = 2-element Vector{Float32}\n\njulia> nodes = [1, 2]\n2-element Vector{Int64}:\n 1\n 2\n\njulia> subgraph = Graphs.induced_subgraph(graph, nodes)\nGNNGraph:\n  num_nodes: 2\n  num_edges: 1\n  ndata:\n        y = 2-element Vector{Float32}\n        x = 32×2 Matrix{Float32}\n  edata:\n        e = 1-element Vector{Float32} source"},{"id":361,"pagetitle":"GNNHeteroGraph","title":"Heterogeneous Graphs","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/heterograph/#Heterogeneous-Graphs","content":" Heterogeneous Graphs"},{"id":362,"pagetitle":"GNNHeteroGraph","title":"GNNHeteroGraph","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/heterograph/#GNNHeteroGraph","content":" GNNHeteroGraph Documentation page for the type  GNNHeteroGraph  representing heterogeneous graphs, where  nodes and edges can have different types."},{"id":363,"pagetitle":"GNNHeteroGraph","title":"GNNGraphs.GNNHeteroGraph","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/heterograph/#GNNGraphs.GNNHeteroGraph","content":" GNNGraphs.GNNHeteroGraph  —  Type GNNHeteroGraph(data; [ndata, edata, gdata, num_nodes])\nGNNHeteroGraph(pairs...; [ndata, edata, gdata, num_nodes]) A type representing a heterogeneous graph structure. It is similar to  GNNGraph  but nodes and edges are of different types. Constructor Arguments data : A dictionary or an iterable object that maps  (source_type, edge_type, target_type)          triples to  (source, target)  index vectors (or to  (source, target, weight)  if also edge weights are present). pairs : Passing multiple relations as pairs is equivalent to passing  data=Dict(pairs...) . ndata : Node features. A dictionary of arrays or named tuple of arrays.          The size of the last dimension of each array must be given by  g.num_nodes . edata : Edge features. A dictionary of arrays or named tuple of arrays. Default  nothing .          The size of the last dimension of each array must be given by  g.num_edges . Default  nothing . gdata : Graph features. An array or named tuple of arrays whose last dimension has size  num_graphs . Default  nothing . num_nodes : The number of nodes for each type. If not specified, inferred from  data . Default  nothing . Fields graph : A dictionary that maps (source type, edge type, target_type) triples to (source, target) index vectors. num_nodes : The number of nodes for each type. num_edges : The number of edges for each type. ndata : Node features. edata : Edge features. gdata : Graph features. ntypes : The node types. etypes : The edge types. Examples julia> using GNNGraphs\n\njulia> nA, nB = 10, 20;\n\njulia> num_nodes = Dict(:A => nA, :B => nB);\n\njulia> edges1 = (rand(1:nA, 20), rand(1:nB, 20))\n([4, 8, 6, 3, 4, 7, 2, 7, 3, 2, 3, 4, 9, 4, 2, 9, 10, 1, 3, 9], [6, 4, 20, 8, 16, 7, 12, 16, 5, 4, 6, 20, 11, 19, 17, 9, 12, 2, 18, 12])\n\njulia> edges2 = (rand(1:nB, 30), rand(1:nA, 30))\n([17, 5, 2, 4, 5, 3, 8, 7, 9, 7  …  19, 8, 20, 7, 16, 2, 9, 15, 8, 13], [1, 1, 3, 1, 1, 3, 2, 7, 4, 4  …  7, 10, 6, 3, 4, 9, 1, 5, 8, 5])\n\njulia> data = ((:A, :rel1, :B) => edges1, (:B, :rel2, :A) => edges2);\n\njulia> hg = GNNHeteroGraph(data; num_nodes)\nGNNHeteroGraph:\n  num_nodes: (:A => 10, :B => 20)\n  num_edges: ((:A, :rel1, :B) => 20, (:B, :rel2, :A) => 30)\n\njulia> hg.num_edges\nDict{Tuple{Symbol, Symbol, Symbol}, Int64} with 2 entries:\n(:A, :rel1, :B) => 20\n(:B, :rel2, :A) => 30\n\n# Let's add some node features\njulia> ndata = Dict(:A => (x = rand(2, nA), y = rand(3, num_nodes[:A])),\n                    :B => rand(10, nB));\n\njulia> hg = GNNHeteroGraph(data; num_nodes, ndata)\nGNNHeteroGraph:\n    num_nodes: (:A => 10, :B => 20)\n    num_edges: ((:A, :rel1, :B) => 20, (:B, :rel2, :A) => 30)\n    ndata:\n    :A  =>  (x = 2×10 Matrix{Float64}, y = 3×10 Matrix{Float64})\n    :B  =>  x = 10×20 Matrix{Float64}\n\n# Access features of nodes of type :A\njulia> hg.ndata[:A].x\n2×10 Matrix{Float64}:\n    0.825882  0.0797502  0.245813  0.142281  0.231253  0.685025  0.821457  0.888838  0.571347   0.53165\n    0.631286  0.316292   0.705325  0.239211  0.533007  0.249233  0.473736  0.595475  0.0623298  0.159307 See also  GNNGraph  for a homogeneous graph type and  rand_heterograph  for a function to generate random heterographs. source"},{"id":364,"pagetitle":"GNNHeteroGraph","title":"GNNGraphs.edge_type_subgraph","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/heterograph/#GNNGraphs.edge_type_subgraph-Tuple{GNNHeteroGraph, Tuple{Symbol, Symbol, Symbol}}","content":" GNNGraphs.edge_type_subgraph  —  Method edge_type_subgraph(g::GNNHeteroGraph, edge_ts) Return a subgraph of  g  that contains only the edges of type  edge_ts . Edge types can be specified as a single edge type (i.e. a tuple containing 3 symbols) or a vector of edge types. source"},{"id":365,"pagetitle":"GNNHeteroGraph","title":"GNNGraphs.num_edge_types","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/heterograph/#GNNGraphs.num_edge_types-Tuple{GNNGraph}","content":" GNNGraphs.num_edge_types  —  Method num_edge_types(g) Return the number of edge types in the graph. For  GNNGraph s, this is always 1. For  GNNHeteroGraph s, this is the number of unique edge types. source"},{"id":366,"pagetitle":"GNNHeteroGraph","title":"GNNGraphs.num_node_types","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/heterograph/#GNNGraphs.num_node_types-Tuple{GNNGraph}","content":" GNNGraphs.num_node_types  —  Method num_node_types(g) Return the number of node types in the graph. For  GNNGraph s, this is always 1. For  GNNHeteroGraph s, this is the number of unique node types. source"},{"id":367,"pagetitle":"GNNHeteroGraph","title":"Query","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/heterograph/#Query","content":" Query"},{"id":368,"pagetitle":"GNNHeteroGraph","title":"GNNGraphs.edge_index","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/heterograph/#GNNGraphs.edge_index-Tuple{GNNHeteroGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}, Tuple{Symbol, Symbol, Symbol}}","content":" GNNGraphs.edge_index  —  Method edge_index(g::GNNHeteroGraph, [edge_t]) Return a tuple containing two vectors, respectively storing the source and target nodes for each edges in  g  of type  edge_t = (src_t, rel_t, trg_t) . If  edge_t  is not provided, it will error if  g  has more than one edge type. source"},{"id":369,"pagetitle":"GNNHeteroGraph","title":"GNNGraphs.graph_indicator","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/heterograph/#GNNGraphs.graph_indicator-Tuple{GNNHeteroGraph}","content":" GNNGraphs.graph_indicator  —  Method graph_indicator(g::GNNHeteroGraph, [node_t]) Return a Dict of vectors containing the graph membership (an integer from  1  to  g.num_graphs ) of each node in the graph for each node type. If  node_t  is provided, return the graph membership of each node of type  node_t  instead. See also  batch . source"},{"id":370,"pagetitle":"GNNHeteroGraph","title":"Graphs.degree","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/heterograph/#Graphs.degree-Union{Tuple{TT}, Tuple{GNNHeteroGraph, Tuple{Symbol, Symbol, Symbol}}, Tuple{GNNHeteroGraph, Tuple{Symbol, Symbol, Symbol}, TT}} where TT<:Union{Nothing, Type{<:Number}}","content":" Graphs.degree  —  Method degree(g::GNNHeteroGraph, edge_type::EType; dir = :in) Return a vector containing the degrees of the nodes in  g  GNNHeteroGraph given  edge_type . Arguments g : A graph. edge_type : A tuple of symbols  (source_t, edge_t, target_t)  representing the edge type. T : Element type of the returned vector. If  nothing , is      chosen based on the graph type. Default  nothing . dir : For  dir = :out  the degree of a node is counted based on the outgoing edges.        For  dir = :in , the ingoing edges are used. If  dir = :both  we have the sum of the two.        Default  dir = :out . source"},{"id":371,"pagetitle":"GNNHeteroGraph","title":"Graphs.has_edge","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/heterograph/#Graphs.has_edge-Tuple{GNNHeteroGraph, Tuple{Symbol, Symbol, Symbol}, Integer, Integer}","content":" Graphs.has_edge  —  Method has_edge(g::GNNHeteroGraph, edge_t, i, j) Return  true  if there is an edge of type  edge_t  from node  i  to node  j  in  g . Examples julia> g = rand_bipartite_heterograph((2, 2), (4, 0), bidirected=false)\nGNNHeteroGraph:\n  num_nodes: Dict(:A => 2, :B => 2)\n  num_edges: Dict((:A, :to, :B) => 4, (:B, :to, :A) => 0)\n\njulia> has_edge(g, (:A,:to,:B), 1, 1)\ntrue\n\njulia> has_edge(g, (:B,:to,:A), 1, 1)\nfalse source"},{"id":372,"pagetitle":"GNNHeteroGraph","title":"Transform","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/heterograph/#Transform","content":" Transform"},{"id":373,"pagetitle":"GNNHeteroGraph","title":"GNNGraphs.add_edges","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/heterograph/#GNNGraphs.add_edges-Tuple{GNNHeteroGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}, Tuple{Symbol, Symbol, Symbol}, AbstractVector, AbstractVector}","content":" GNNGraphs.add_edges  —  Method add_edges(g::GNNHeteroGraph, edge_t, s, t; [edata, num_nodes])\nadd_edges(g::GNNHeteroGraph, edge_t => (s, t); [edata, num_nodes])\nadd_edges(g::GNNHeteroGraph, edge_t => (s, t, w); [edata, num_nodes]) Add to heterograph  g  edges of type  edge_t  with source node vector  s  and target node vector  t . Optionally, pass the  edge weights  w  or the features   edata  for the new edges.  edge_t  is a triplet of symbols  (src_t, rel_t, dst_t) .  If the edge type is not already present in the graph, it is added.  If it involves new node types, they are added to the graph as well. In this case, a dictionary or named tuple of  num_nodes  can be passed to specify the number of nodes of the new types, otherwise the number of nodes is inferred from the maximum node id in  s  and  t . source"},{"id":374,"pagetitle":"GNNHeteroGraph","title":"GNNGraphs.add_self_loops","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/heterograph/#GNNGraphs.add_self_loops-Tuple{GNNHeteroGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}, Tuple{Symbol, Symbol, Symbol}}","content":" GNNGraphs.add_self_loops  —  Method add_self_loops(g::GNNHeteroGraph, edge_t::EType)\nadd_self_loops(g::GNNHeteroGraph) If the source node type is the same as the destination node type in  edge_t , return a graph with the same features as  g  but also add self-loops  of the specified type,  edge_t . Otherwise, it returns  g  unchanged. Nodes with already existing self-loops of type  edge_t  will obtain  a second set of self-loops of the same type. If the graph has edge weights for edges of type  edge_t , the new edges will have weight 1. If no edges of type  edge_t  exist, or all existing edges have no weight,  then all new self loops will have no weight. If  edge_t  is not passed as argument, for the entire graph self-loop is added to each node for every edge type in the graph where the source and destination node types are the same.  This iterates over all edge types present in the graph, applying the self-loop addition logic to each applicable edge type. source"},{"id":375,"pagetitle":"GNNHeteroGraph","title":"Generate","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/heterograph/#Generate","content":" Generate"},{"id":376,"pagetitle":"GNNHeteroGraph","title":"GNNGraphs.rand_bipartite_heterograph","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/heterograph/#GNNGraphs.rand_bipartite_heterograph-Tuple{Any, Any}","content":" GNNGraphs.rand_bipartite_heterograph  —  Method rand_bipartite_heterograph([rng,] \n                           (n1, n2), (m12, m21); \n                           bidirected = true, \n                           node_t = (:A, :B), \n                           edge_t = :to, \n                           kws...) Construct an  GNNHeteroGraph  with random edges representing a bipartite graph. The graph will have two types of nodes, and edges will only connect nodes of different types. The first argument is a tuple  (n1, n2)  specifying the number of nodes of each type. The second argument is a tuple  (m12, m21)  specifying the number of edges connecting nodes of type  1  to nodes of type  2   and vice versa. The type of nodes and edges can be specified with the  node_t  and  edge_t  keyword arguments, which default to  (:A, :B)  and  :to  respectively. If  bidirected=true  (default), the reverse edge of each edge will be present. In this case  m12 == m21  is required. A random number generator can be passed as the first argument to make the generation reproducible. Additional keyword arguments will be passed to the  GNNHeteroGraph  constructor. See  rand_heterograph  for a more general version. Examples julia> g = rand_bipartite_heterograph((10, 15), 20)\nGNNHeteroGraph:\n  num_nodes: (:A => 10, :B => 15)\n  num_edges: ((:A, :to, :B) => 20, (:B, :to, :A) => 20)\n\njulia> g = rand_bipartite_heterograph((10, 15), (20, 0), node_t=(:user, :item), edge_t=:-, bidirected=false)\nGNNHeteroGraph:\n  num_nodes: Dict(:item => 15, :user => 10)\n  num_edges: Dict((:item, :-, :user) => 0, (:user, :-, :item) => 20) source"},{"id":377,"pagetitle":"GNNHeteroGraph","title":"GNNGraphs.rand_heterograph","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/heterograph/#GNNGraphs.rand_heterograph","content":" GNNGraphs.rand_heterograph  —  Function rand_heterograph([rng,] n, m; bidirected=false, kws...) Construct an  GNNHeteroGraph  with random edges and with number of nodes and edges  specified by  n  and  m  respectively.  n  and  m  can be any iterable of pairs specifing node/edge types and their numbers. Pass a random number generator as a first argument to make the generation reproducible. Setting  bidirected=true  will generate a bidirected graph, i.e. each edge will have a reverse edge. Therefore, for each edge type  (:A, :rel, :B)  a corresponding reverse edge type  (:B, :rel, :A)  will be generated. Additional keyword arguments will be passed to the  GNNHeteroGraph  constructor. Examples julia> g = rand_heterograph((:user => 10, :movie => 20),\n                            (:user, :rate, :movie) => 30)\nGNNHeteroGraph:\n  num_nodes: Dict(:movie => 20, :user => 10)\n  num_edges: Dict((:user, :rate, :movie) => 30) source"},{"id":380,"pagetitle":"Samplers","title":"Samplers","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/samplers/#Samplers","content":" Samplers"},{"id":381,"pagetitle":"Samplers","title":"GNNGraphs.NeighborLoader","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/samplers/#GNNGraphs.NeighborLoader","content":" GNNGraphs.NeighborLoader  —  Type NeighborLoader(graph; num_neighbors, input_nodes, num_layers, [batch_size]) A data structure for sampling neighbors from a graph for training Graph Neural Networks (GNNs).  It supports multi-layer sampling of neighbors for a batch of input nodes, useful for mini-batch training originally introduced in [\"Inductive Representation Learning on Large Graphs\"}(https://arxiv.org/abs/1706.02216) paper. Fields graph::GNNGraph : The input graph. num_neighbors::Vector{Int} : A vector specifying the number of neighbors to sample per node at each GNN layer. input_nodes::Vector{Int} : A vector containing the starting nodes for neighbor sampling. num_layers::Int : The number of layers for neighborhood expansion (how far to sample neighbors). batch_size::Union{Int, Nothing} : The size of the batch. If not specified, it defaults to the number of  input_nodes . Examples julia> loader = NeighborLoader(graph; num_neighbors=[10, 5], input_nodes=[1, 2, 3], num_layers=2)\n\njulia> batch_counter = 0\n\njulia> for mini_batch_gnn in loader\n            batch_counter += 1\n            println(\"Batch \", batch_counter, \": Nodes in mini-batch graph: \", nv(mini_batch_gnn))\n        end source"},{"id":384,"pagetitle":"TemporalSnapshotsGNNGraph","title":"Temporal Graphs","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/temporalgraph/#Temporal-Graphs","content":" Temporal Graphs"},{"id":385,"pagetitle":"TemporalSnapshotsGNNGraph","title":"TemporalSnapshotsGNNGraph","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/temporalgraph/#TemporalSnapshotsGNNGraph","content":" TemporalSnapshotsGNNGraph Documentation page for the graph type  TemporalSnapshotsGNNGraph  and related methods, representing time varying graphs with time varying features."},{"id":386,"pagetitle":"TemporalSnapshotsGNNGraph","title":"GNNGraphs.TemporalSnapshotsGNNGraph","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/temporalgraph/#GNNGraphs.TemporalSnapshotsGNNGraph","content":" GNNGraphs.TemporalSnapshotsGNNGraph  —  Type TemporalSnapshotsGNNGraph(snapshots::AbstractVector{<:GNNGraph}) A type representing a temporal graph as a sequence of snapshots. In this case a snapshot is a  GNNGraph . TemporalSnapshotsGNNGraph  can store the feature array associated to the graph itself as a  DataStore  object,  and it uses the  DataStore  objects of each snapshot for the node and edge features. The features can be passed at construction time or added later. Constructor Arguments snapshot : a vector of snapshots, where each snapshot must have the same number of nodes. Examples julia> using GNNGraphs\n\njulia> snapshots = [rand_graph(10,20) for i in 1:5];\n\njulia> tg = TemporalSnapshotsGNNGraph(snapshots)\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 10, 10, 10, 10]\n  num_edges: [20, 20, 20, 20, 20]\n  num_snapshots: 5\n\njulia> tg.tgdata.x = rand(4); # add temporal graph feature\n\njulia> tg # show temporal graph with new feature\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 10, 10, 10, 10]\n  num_edges: [20, 20, 20, 20, 20]\n  num_snapshots: 5\n  tgdata:\n        x = 4-element Vector{Float64} source"},{"id":387,"pagetitle":"TemporalSnapshotsGNNGraph","title":"GNNGraphs.add_snapshot","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/temporalgraph/#GNNGraphs.add_snapshot-Tuple{TemporalSnapshotsGNNGraph, Int64, GNNGraph}","content":" GNNGraphs.add_snapshot  —  Method add_snapshot(tg::TemporalSnapshotsGNNGraph, t::Int, g::GNNGraph) Return a  TemporalSnapshotsGNNGraph  created starting from  tg  by adding the snapshot  g  at time index  t . Examples julia> using GNNGraphs\n\njulia> snapshots = [rand_graph(10, 20) for i in 1:5];\n\njulia> tg = TemporalSnapshotsGNNGraph(snapshots)\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 10, 10, 10, 10]\n  num_edges: [20, 20, 20, 20, 20]\n  num_snapshots: 5\n\njulia> new_tg = add_snapshot(tg, 3, rand_graph(10, 16)) # add a new snapshot at time 3\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 10, 10, 10, 10, 10]\n  num_edges: [20, 20, 16, 20, 20, 20]\n  num_snapshots: 6 source"},{"id":388,"pagetitle":"TemporalSnapshotsGNNGraph","title":"GNNGraphs.remove_snapshot","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/temporalgraph/#GNNGraphs.remove_snapshot-Tuple{TemporalSnapshotsGNNGraph, Int64}","content":" GNNGraphs.remove_snapshot  —  Method remove_snapshot(tg::TemporalSnapshotsGNNGraph, t::Int) Return a  TemporalSnapshotsGNNGraph  created starting from  tg  by removing the snapshot at time index  t . Examples julia> using GNNGraphs\n\njulia> snapshots = [rand_graph(10,20), rand_graph(10,14), rand_graph(10,22)];\n\njulia> tg = TemporalSnapshotsGNNGraph(snapshots)\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 10, 10]\n  num_edges: [20, 14, 22]\n  num_snapshots: 3\n\njulia> new_tg = remove_snapshot(tg, 2) # remove snapshot at time 2\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 10]\n  num_edges: [20, 22]\n  num_snapshots: 2 source"},{"id":389,"pagetitle":"TemporalSnapshotsGNNGraph","title":"Random Generators","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/temporalgraph/#Random-Generators","content":" Random Generators"},{"id":390,"pagetitle":"TemporalSnapshotsGNNGraph","title":"GNNGraphs.rand_temporal_radius_graph","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/temporalgraph/#GNNGraphs.rand_temporal_radius_graph","content":" GNNGraphs.rand_temporal_radius_graph  —  Function rand_temporal_radius_graph(number_nodes::Int, \n                           number_snapshots::Int,\n                           speed::AbstractFloat,\n                           r::AbstractFloat;\n                           self_loops = false,\n                           dir = :in,\n                           kws...) Create a random temporal graph given  number_nodes  nodes and  number_snapshots  snapshots. First, the positions of the nodes are randomly generated in the unit square. Two nodes are connected if their distance is less than a given radius  r . Each following snapshot is obtained by applying the same construction to new positions obtained as follows. For each snapshot, the new positions of the points are determined by applying random independent displacement vectors to the previous positions. The direction of the displacement is chosen uniformly at random and its length is chosen uniformly in  [0, speed] . Then the connections are recomputed. If a point happens to move outside the boundary, its position is updated as if it had bounced off the boundary. Arguments number_nodes : The number of nodes of each snapshot. number_snapshots : The number of snapshots. speed : The speed to update the nodes. r : The radius of connection. self_loops : If  true , consider the node itself among its neighbors, in which               case the graph will contain self-loops.  dir : The direction of the edges. If  dir=:in  edges go from the        neighbors to the central node. If  dir=:out  we have the opposite        direction. kws : Further keyword arguments will be passed to the  GNNGraph  constructor of each snapshot. Example julia> n, snaps, s, r = 10, 5, 0.1, 1.5;\n\njulia> tg = rand_temporal_radius_graph(n,snaps,s,r) # complete graph at each snapshot\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 10, 10, 10, 10]\n  num_edges: [90, 90, 90, 90, 90]\n  num_snapshots: 5 source"},{"id":391,"pagetitle":"TemporalSnapshotsGNNGraph","title":"GNNGraphs.rand_temporal_hyperbolic_graph","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/api/temporalgraph/#GNNGraphs.rand_temporal_hyperbolic_graph","content":" GNNGraphs.rand_temporal_hyperbolic_graph  —  Function rand_temporal_hyperbolic_graph(number_nodes::Int, \n                               number_snapshots::Int;\n                               α::Real,\n                               R::Real,\n                               speed::Real,\n                               ζ::Real=1,\n                               self_loop = false,\n                               kws...) Create a random temporal graph given  number_nodes  nodes and  number_snapshots  snapshots. First, the positions of the nodes are generated with a quasi-uniform distribution (depending on the parameter  α ) in hyperbolic space within a disk of radius  R . Two nodes are connected if their hyperbolic distance is less than  R . Each following snapshot is created in order to keep the same initial distribution. Arguments number_nodes : The number of nodes of each snapshot. number_snapshots : The number of snapshots. α : The parameter that controls the position of the points. If  α=ζ , the points are uniformly distributed on the disk of radius  R . If  α>ζ , the points are more concentrated in the center of the disk. If  α<ζ , the points are more concentrated at the boundary of the disk. R : The radius of the disk and of connection. speed : The speed to update the nodes. ζ : The parameter that controls the curvature of the disk. self_loops : If  true , consider the node itself among its neighbors, in which               case the graph will contain self-loops. kws : Further keyword arguments will be passed to the  GNNGraph  constructor of each snapshot. Example julia> n, snaps, α, R, speed, ζ = 10, 5, 1.0, 4.0, 0.1, 1.0;\n\njulia> thg = rand_temporal_hyperbolic_graph(n, snaps; α, R, speed, ζ)\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 10, 10, 10, 10]\n  num_edges: [44, 46, 48, 42, 38]\n  num_snapshots: 5 References Section D of the paper  Dynamic Hidden-Variable Network Models  and the paper   Hyperbolic Geometry of Complex Networks source"},{"id":394,"pagetitle":"Datasets","title":"Datasets","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/guides/datasets/#Datasets","content":" Datasets GNNGraphs.jl doesn't come with its own datasets, but leverages those available in the Julia (and non-Julia) ecosystem. In particular, the  examples in the GraphNeuralNetworks.jl repository  make use of the  MLDatasets.jl  package. There you will find common graph datasets such as Cora, PubMed, Citeseer, TUDataset and  many others . For graphs with static structures and temporal features, datasets such as METRLA, PEMSBAY, ChickenPox, and WindMillEnergy are available. For graphs featuring both temporal structures and temporal features, the TemporalBrains dataset is suitable. GraphNeuralNetworks.jl provides the  mldataset2gnngraph  method for interfacing with MLDatasets.jl."},{"id":397,"pagetitle":"Graphs","title":"Static Graphs","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/guides/gnngraph/#Static-Graphs","content":" Static Graphs The fundamental graph type in GNNGraphs.jl is the  GNNGraph . A GNNGraph  g  is a directed graph with nodes labeled from 1 to  g.num_nodes . The underlying implementation allows for efficient application of graph neural network operators, gpu movement, and storage of node/edge/graph related feature arrays. GNNGraph  inherits from  Graphs.jl 's  AbstractGraph , therefore it supports most functionality from that library. "},{"id":398,"pagetitle":"Graphs","title":"Graph Creation","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/guides/gnngraph/#Graph-Creation","content":" Graph Creation A GNNGraph can be created from several different data sources encoding the graph topology: using GNNGraphs, Graphs, SparseArrays\n\n\n# Construct a GNNGraph from from a Graphs.jl's graph\nlg = erdos_renyi(10, 30)\ng = GNNGraph(lg)\n\n# Same as above using convenience method rand_graph\ng = rand_graph(10, 60)\n\n# From an adjacency matrix\nA = sprand(10, 10, 0.3)\ng = GNNGraph(A)\n\n# From an adjacency list\nadjlist = [[2,3], [1,3], [1,2,4], [3]]\ng = GNNGraph(adjlist)\n\n# From COO representation\nsource = [1,1,2,2,3,3,3,4]\ntarget = [2,3,1,3,1,2,4,3]\ng = GNNGraph(source, target) See also the related methods  Graphs.adjacency_matrix ,  edge_index , and  adjacency_list ."},{"id":399,"pagetitle":"Graphs","title":"Basic Queries","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/guides/gnngraph/#Basic-Queries","content":" Basic Queries julia> source = [1,1,2,2,3,3,3,4];\n\njulia> target = [2,3,1,3,1,2,4,3];\n\njulia> g = GNNGraph(source, target)\nGNNGraph:\n  num_nodes: 4\n  num_edges: 8\n\n\njulia> @assert g.num_nodes == 4   # number of nodes\n\njulia> @assert g.num_edges == 8   # number of edges\n\njulia> @assert g.num_graphs == 1  # number of subgraphs (a GNNGraph can batch many graphs together)\n\njulia> is_directed(g)      # a GNNGraph is always directed\ntrue\n\njulia> is_bidirected(g)      # for each edge, also the reverse edge is present\ntrue\n\njulia> has_self_loops(g)\nfalse\n\njulia> has_multi_edges(g)      \nfalse"},{"id":400,"pagetitle":"Graphs","title":"Data Features","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/guides/gnngraph/#Data-Features","content":" Data Features One or more arrays can be associated to nodes, edges, and (sub)graphs of a  GNNGraph . They will be stored in the fields  g.ndata ,  g.edata , and  g.gdata  respectively. The data fields are  DataStore  objects.  DataStore s conveniently offer an interface similar to both dictionaries and named tuples. Similarly to dictionaries, DataStores support addition of new features after creation time. The array contained in the datastores have last dimension equal to  num_nodes  (in  ndata ),  num_edges  (in  edata ), or  num_graphs  (in  gdata ) respectively. # Create a graph with a single feature array `x` associated to nodes\ng = rand_graph(10,  60, ndata = (; x = rand(Float32, 32, 10)))\n\ng.ndata.x  # access the features\n\n# Equivalent definition passing directly the array\ng = rand_graph(10,  60, ndata = rand(Float32, 32, 10))\n\ng.ndata.x  # `:x` is the default name for node features\n\ng.ndata.z = rand(Float32, 3, 10)  # add new feature array `z`\n\n# For convenience, we can access the features through the shortcut\ng.x \n\n# You can have multiple feature arrays\ng = rand_graph(10,  60, ndata = (; x=rand(Float32, 32, 10), y=rand(Float32, 10)))\n\ng.ndata.y, g.ndata.x   # or g.x, g.y\n\n# Attach an array with edge features.\n# Since `GNNGraph`s are directed, the number of edges\n# will be double that of the original Graphs' undirected graph.\ng = GNNGraph(erdos_renyi(10,  30), edata = rand(Float32, 60))\n@assert g.num_edges == 60\n\ng.edata.e  # or g.e\n\n# If we pass only half of the edge features, they will be copied\n# on the reversed edges.\ng = GNNGraph(erdos_renyi(10,  30), edata = rand(Float32, 30))\n\n\n# Create a new graph from previous one, inheriting edge data\n# but replacing node data\ng′ = GNNGraph(g, ndata =(; z = ones(Float32, 16, 10)))\n\ng′.z\ng′.e"},{"id":401,"pagetitle":"Graphs","title":"Edge weights","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/guides/gnngraph/#Edge-weights","content":" Edge weights It is common to denote scalar edge features as edge weights. The  GNNGraph  has specific support for edge weights: they can be stored as part of internal representations of the graph (COO or adjacency matrix). Some graph convolutional layers, most notably the  GCNConv , can use the edge weights to perform weighted sums over the nodes' neighborhoods. julia> source = [1, 1, 2, 2, 3, 3];\n\njulia> target = [2, 3, 1, 3, 1, 2];\n\njulia> weight = [1.0, 0.5, 2.1, 2.3, 4, 4.1];\n\njulia> g = GNNGraph(source, target, weight)\nGNNGraph:\n  num_nodes: 3\n  num_edges: 6\n\njulia> get_edge_weight(g)\n6-element Vector{Float64}:\n 1.0\n 0.5\n 2.1\n 2.3\n 4.0\n 4.1"},{"id":402,"pagetitle":"Graphs","title":"Batches and Subgraphs","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/guides/gnngraph/#Batches-and-Subgraphs","content":" Batches and Subgraphs Multiple  GNNGraph s can be batched together into a single graph that contains the total number of the original nodes  and where the original graphs are disjoint subgraphs. using MLUtils\n\ndata = [rand_graph(10, 30, ndata=rand(Float32, 3, 10)) for _ in 1:160]\ngall = MLUtils.batch(data)\n\n# gall is a GNNGraph containing many graphs\n@assert gall.num_graphs == 160 \n@assert gall.num_nodes == 1600   # 10 nodes x 160 graphs\n@assert gall.num_edges == 4800  # 30 undirected edges x 160 graphs\n\n# Let's create a mini-batch from gall\ng23 = getgraph(gall, 2:3)\n@assert g23.num_graphs == 2\n@assert g23.num_nodes == 20   # 10 nodes x 2 graphs\n@assert g23.num_edges == 60  # 30 undirected edges X 2 graphs\n\n# We can pass a GNNGraph to MLUtils' DataLoader\ntrain_loader = DataLoader(gall, batchsize=16, shuffle=true)\n\nfor g in train_loader\n    @assert g.num_graphs == 16\n    @assert g.num_nodes == 160\n    @assert size(g.ndata.x) = (3, 160)    \n    # .....\nend\n\n# Access the nodes' graph memberships \ngraph_indicator(gall)"},{"id":403,"pagetitle":"Graphs","title":"DataLoader and mini-batch iteration","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/guides/gnngraph/#DataLoader-and-mini-batch-iteration","content":" DataLoader and mini-batch iteration While constructing a batched graph and passing it to the  DataLoader  is always  an option for mini-batch iteration, the recommended way for better performance is to pass an array of graphs directly and set the  collate  option to  true : using MLUtils: DataLoader\n\ndata = [rand_graph(10, 30, ndata=rand(Float32, 3, 10)) for _ in 1:320]\n\ntrain_loader = DataLoader(data, batchsize=16, shuffle=true, collate=true)\n\nfor g in train_loader\n    @assert g.num_graphs == 16\n    @assert g.num_nodes == 160\n    @assert size(g.ndata.x) = (3, 160)    \n    # .....\nend"},{"id":404,"pagetitle":"Graphs","title":"Graph Manipulation","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/guides/gnngraph/#Graph-Manipulation","content":" Graph Manipulation g′ = add_self_loops(g)\ng′ = remove_self_loops(g)\ng′ = add_edges(g, [1, 2], [2, 3]) # add edges 1->2 and 2->3"},{"id":405,"pagetitle":"Graphs","title":"GPU movement","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/guides/gnngraph/#GPU-movement","content":" GPU movement Move a  GNNGraph  to a CUDA device using  Flux.gpu  method.  using Flux, CUDA # or using Metal or using AMDGPU \n\ng_gpu = g |> Flux.gpu"},{"id":406,"pagetitle":"Graphs","title":"Integration with Graphs.jl","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/guides/gnngraph/#Integration-with-Graphs.jl","content":" Integration with Graphs.jl Since  GNNGraph <: Graphs.AbstractGraph , we can use any functionality from  Graphs.jl  for querying and analyzing the graph structure.  Moreover, a  GNNGraph  can be easily constructed from a  Graphs.Graph  or a  Graphs.DiGraph : julia> import Graphs\n\njulia> using GNNGraphs\n\n# A Graphs.jl undirected graph\njulia> gu = Graphs.erdos_renyi(10, 20)    \n{10, 20} undirected simple Int64 graph\n\n# Since GNNGraphs are undirected, the edges are doubled when converting \n# to GNNGraph\njulia> GNNGraph(gu)\nGNNGraph:\n  num_nodes: 10\n  num_edges: 40\n\n# A Graphs.jl directed graph\njulia> gd = Graphs.erdos_renyi(10, 20, is_directed=true)\n{10, 20} directed simple Int64 graph\n\njulia> GNNGraph(gd)\nGNNGraph:\n  num_nodes: 10\n  num_edges: 20"},{"id":409,"pagetitle":"Heterogeneous Graphs","title":"Heterogeneous Graphs","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/guides/heterograph/#Heterogeneous-Graphs","content":" Heterogeneous Graphs Heterogeneous graphs (also called heterographs), are graphs where each node has a type, that we denote with symbols such as  :user  and  :movie . Relations such as  :rate  or  :like  can connect nodes of different types. We call a triplet  (source_node_type, relation_type, target_node_type)  the type of a edge, e.g.  (:user, :rate, :movie) . Different node/edge types can store different groups of features and this makes heterographs a very flexible modeling tools  and data containers. In GNNGraphs.jl heterographs are implemented in  the type  GNNHeteroGraph ."},{"id":410,"pagetitle":"Heterogeneous Graphs","title":"Creating a Heterograph","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/guides/heterograph/#Creating-a-Heterograph","content":" Creating a Heterograph A heterograph can be created empty or by passing pairs  edge_type => data  to the constructor. julia> using GNNGraphs\n\njulia> g = GNNHeteroGraph()\nGNNHeteroGraph:\n  num_nodes: Dict()\n  num_edges: Dict()\n  \njulia> g = GNNHeteroGraph((:user, :like, :actor) => ([1,2,2,3], [1,3,2,9]),\n                          (:user, :rate, :movie) => ([1,1,2,3], [7,13,5,7]))\nGNNHeteroGraph:\n  num_nodes: Dict(:actor => 9, :movie => 13, :user => 3)\n  num_edges: Dict((:user, :like, :actor) => 4, (:user, :rate, :movie) => 4)\n\njulia> g = GNNHeteroGraph((:user, :rate, :movie) => ([1,1,2,3], [7,13,5,7]))\nGNNHeteroGraph:\n  num_nodes: Dict(:movie => 13, :user => 3)\n  num_edges: Dict((:user, :rate, :movie) => 4) New relations, possibly with new node types, can be added with the function  add_edges . julia> g = add_edges(g, (:user, :like, :actor) => ([1,2,3,3,3], [3,5,1,9,4]))\nGNNHeteroGraph:\n  num_nodes: Dict(:actor => 9, :movie => 13, :user => 3)\n  num_edges: Dict((:user, :like, :actor) => 5, (:user, :rate, :movie) => 4) See  rand_heterograph ,  rand_bipartite_heterograph  for generating random heterographs.  julia> g = rand_bipartite_heterograph((10, 15), 20)\nGNNHeteroGraph:\n  num_nodes: Dict(:A => 10, :B => 15)\n  num_edges: Dict((:A, :to, :B) => 20, (:B, :to, :A) => 20)"},{"id":411,"pagetitle":"Heterogeneous Graphs","title":"Basic Queries","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/guides/heterograph/#Basic-Queries","content":" Basic Queries Basic queries are similar to those for homogeneous graphs: julia> g = GNNHeteroGraph((:user, :rate, :movie) => ([1,1,2,3], [7,13,5,7]))\nGNNHeteroGraph:\n  num_nodes: Dict(:movie => 13, :user => 3)\n  num_edges: Dict((:user, :rate, :movie) => 4)\n\njulia> g.num_nodes\nDict{Symbol, Int64} with 2 entries:\n  :user  => 3\n  :movie => 13\n\njulia> g.num_edges\nDict{Tuple{Symbol, Symbol, Symbol}, Int64} with 1 entry:\n  (:user, :rate, :movie) => 4\n\njulia> edge_index(g, (:user, :rate, :movie)) # source and target node for a given relation\n([1, 1, 2, 3], [7, 13, 5, 7])\n\njulia> g.ntypes  # node types\n2-element Vector{Symbol}:\n :user\n :movie\n\njulia> g.etypes  # edge types\n1-element Vector{Tuple{Symbol, Symbol, Symbol}}:\n (:user, :rate, :movie)"},{"id":412,"pagetitle":"Heterogeneous Graphs","title":"Data Features","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/guides/heterograph/#Data-Features","content":" Data Features Node, edge, and graph features can be added at construction time or later using: # equivalent to g.ndata[:user][:x] = ...\njulia> g[:user].x = rand(Float32, 64, 3);\n\njulia> g[:movie].z = rand(Float32, 64, 13);\n\n# equivalent to g.edata[(:user, :rate, :movie)][:e] = ...\njulia> g[:user, :rate, :movie].e = rand(Float32, 64, 4);\n\njulia> g\nGNNHeteroGraph:\n  num_nodes: Dict(:movie => 13, :user => 3)\n  num_edges: Dict((:user, :rate, :movie) => 4)\n  ndata:\n        :movie  =>  DataStore(z = [64×13 Matrix{Float32}])\n        :user  =>  DataStore(x = [64×3 Matrix{Float32}])\n  edata:\n        (:user, :rate, :movie)  =>  DataStore(e = [64×4 Matrix{Float32}])"},{"id":413,"pagetitle":"Heterogeneous Graphs","title":"Batching","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/guides/heterograph/#Batching","content":" Batching Similarly to graphs, also heterographs can be batched together. julia> gs = [rand_bipartite_heterograph((5, 10), 20) for _ in 1:32];\n\njulia> MLUtils.batch(gs)\nGNNHeteroGraph:\n  num_nodes: Dict(:A => 160, :B => 320)\n  num_edges: Dict((:A, :to, :B) => 640, (:B, :to, :A) => 640)\n  num_graphs: 32 Batching is automatically performed by the  DataLoader  iterator when the  collate  option is set to  true . using MLUtils: DataLoader\n\ndata = [rand_bipartite_heterograph((5, 10), 20, \n            ndata=Dict(:A=>rand(Float32, 3, 5))) \n        for _ in 1:320];\n\ntrain_loader = DataLoader(data, batchsize=16, shuffle=true, collate=true)\n\nfor g in train_loader\n    @assert g.num_graphs == 16\n    @assert g.num_nodes[:A] == 80\n    @assert size(g.ndata[:A].x) == (3, 80)    \n    # ...\nend"},{"id":414,"pagetitle":"Heterogeneous Graphs","title":"Graph convolutions on heterographs","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/guides/heterograph/#Graph-convolutions-on-heterographs","content":" Graph convolutions on heterographs See  HeteroGraphConv  for how to perform convolutions on heterogeneous graphs."},{"id":417,"pagetitle":"Temporal Graphs","title":"Temporal Graphs","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/guides/temporalgraph/#Temporal-Graphs","content":" Temporal Graphs Temporal Graphs are graphs with time varying topologies and  features. In GNNGraphs.jl, temporal graphs with fixed number of nodes over time are supported by the  TemporalSnapshotsGNNGraph  type."},{"id":418,"pagetitle":"Temporal Graphs","title":"Creating a TemporalSnapshotsGNNGraph","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/guides/temporalgraph/#Creating-a-TemporalSnapshotsGNNGraph","content":" Creating a TemporalSnapshotsGNNGraph A temporal graph can be created by passing a list of snapshots to the constructor. Each snapshot is a  GNNGraph .  julia> using GNNGraphs\n\njulia> snapshots = [rand_graph(10,20) for i in 1:5];\n\njulia> tg = TemporalSnapshotsGNNGraph(snapshots)\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 10, 10, 10, 10]\n  num_edges: [20, 20, 20, 20, 20]\n  num_snapshots: 5 A new temporal graph can be created by adding or removing snapshots to an existing temporal graph.  julia> new_tg = add_snapshot(tg, 3, rand_graph(10, 16)) # add a new snapshot at time 3\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 10, 10, 10, 10, 10]\n  num_edges: [20, 20, 16, 20, 20, 20]\n  num_snapshots: 6 julia> snapshots = [rand_graph(10,20), rand_graph(10,14), rand_graph(10,22)];\n\njulia> tg = TemporalSnapshotsGNNGraph(snapshots)\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 10, 10]\n  num_edges: [20, 14, 22]\n  num_snapshots: 3\n\njulia> new_tg = remove_snapshot(tg, 2) # remove snapshot at time 2\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 10]\n  num_edges: [20, 22]\n  num_snapshots: 2 See  rand_temporal_radius_graph  and  rand_temporal_hyperbolic_graph  for generating random temporal graphs.  julia> tg = rand_temporal_radius_graph(10, 3, 0.1, 0.5)\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 10, 10]\n  num_edges: [32, 30, 34]\n  num_snapshots: 3"},{"id":419,"pagetitle":"Temporal Graphs","title":"Basic Queries","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/guides/temporalgraph/#Basic-Queries","content":" Basic Queries Basic queries are similar to those for  GNNGraph s: julia> snapshots = [rand_graph(10,20), rand_graph(10,14), rand_graph(10,22)];\n\njulia> tg = TemporalSnapshotsGNNGraph(snapshots)\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 10, 10]\n  num_edges: [20, 14, 22]\n  num_snapshots: 3\n\njulia> tg.num_nodes         # number of nodes in each snapshot\n3-element Vector{Int64}:\n 10\n 10\n 10\n\njulia> tg.num_edges         # number of edges in each snapshot\n3-element Vector{Int64}:\n 20\n 14\n 22\n\njulia> tg.num_snapshots     # number of snapshots\n3\n\njulia> tg.snapshots         # list of snapshots\n3-element Vector{GNNGraph{Tuple{Vector{Int64}, Vector{Int64}, Nothing}}}:\n GNNGraph(10, 20) with no data\n GNNGraph(10, 14) with no data\n GNNGraph(10, 22) with no data\n\njulia> tg.snapshots[1]      # first snapshot, same as tg[1]\nGNNGraph:\n  num_nodes: 10\n  num_edges: 20"},{"id":420,"pagetitle":"Temporal Graphs","title":"Data Features","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNGraphs/guides/temporalgraph/#Data-Features","content":" Data Features A temporal graph can store global feature for the entire time series in the  tgdata  filed. Also, each snapshot can store node, edge, and graph features in the  ndata ,  edata , and  gdata  fields, respectively.  julia> snapshots = [rand_graph(10, 20; ndata = rand(Float32, 3, 10)), \n                    rand_graph(10, 14; ndata = rand(Float32, 4, 10)), \n                    rand_graph(10, 22; ndata = rand(Float32, 5, 10))]; # node features at construction time\n\njulia> tg = TemporalSnapshotsGNNGraph(snapshots);\n\njulia> tg.tgdata.y = rand(Float32, 3, 1); # add global features after construction\n\njulia> tg\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 10, 10]\n  num_edges: [20, 14, 22]\n  num_snapshots: 3\n  tgdata:\n        y = 3×1 Matrix{Float32}\n\njulia> tg.ndata # vector of DataStore containing node features for each snapshot\n3-element Vector{DataStore}:\n DataStore(10) with 1 element:\n  x = 3×10 Matrix{Float32}\n DataStore(10) with 1 element:\n  x = 4×10 Matrix{Float32}\n DataStore(10) with 1 element:\n  x = 5×10 Matrix{Float32}\n\njulia> [ds.x for ds in tg.ndata]; # vector containing the x feature of each snapshot\n\njulia> [g.x for g in tg.snapshots]; # same vector as above, now accessing \n                                   # the x feature directly from the snapshots"},{"id":423,"pagetitle":"GNNlib.jl","title":"GNNlib.jl","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNlib/#GNNlib.jl","content":" GNNlib.jl GNNlib.jl is a package that provides the implementation of the basic message passing functions and  functional implementation of graph convolutional layers, which are used to build graph neural networks in both the  Flux.jl  and  Lux.jl  machine learning frameworks, created in the GraphNeuralNetworks.jl and GNNLux.jl packages, respectively. This package depends on GNNGraphs.jl and NNlib.jl, and is primarily intended for developers looking to create new GNN architectures. For most users, the higher-level GraphNeuralNetworks.jl and GNNLux.jl packages are recommended."},{"id":424,"pagetitle":"GNNlib.jl","title":"Installation","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNlib/#Installation","content":" Installation The package can be installed with the Julia package manager. From the Julia REPL, type  ]  to enter the Pkg REPL mode and run: pkg> add GNNlib"},{"id":427,"pagetitle":"Message Passing","title":"Message Passing","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNlib/api/messagepassing/#Message-Passing","content":" Message Passing"},{"id":428,"pagetitle":"Message Passing","title":"Interface","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNlib/api/messagepassing/#Interface","content":" Interface"},{"id":429,"pagetitle":"Message Passing","title":"GNNlib.apply_edges","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNlib/api/messagepassing/#GNNlib.apply_edges","content":" GNNlib.apply_edges  —  Function apply_edges(fmsg, g; [xi, xj, e])\napply_edges(fmsg, g, xi, xj, e=nothing) Returns the message from node  j  to node  i  applying the message function  fmsg  on the edges in graph  g . In the message-passing scheme, the incoming messages  from the neighborhood of  i  will later be aggregated in order to update the features of node  i  (see  aggregate_neighbors ). The function  fmsg  operates on batches of edges, therefore  xi ,  xj , and  e  are tensors whose last dimension is the batch size, or can be named tuples of  such tensors. Arguments g : An  AbstractGNNGraph . xi : An array or a named tuple containing arrays whose last dimension's size        is  g.num_nodes . It will be appropriately materialized on the       target node of each edge (see also  edge_index ). xj : As  xi , but now to be materialized on each edge's source node.  e : An array or a named tuple containing arrays whose last dimension's size is  g.num_edges . fmsg : A function that takes as inputs the edge-materialized  xi ,  xj , and  e .      These are arrays (or named tuples of arrays) whose last dimension' size is the size of      a batch of edges. The output of  f  has to be an array (or a named tuple of arrays)      with the same batch size. If also  layer  is passed to propagate,     the signature of  fmsg  has to be  fmsg(layer, xi, xj, e)       instead of  fmsg(xi, xj, e) . See also  propagate  and  aggregate_neighbors . source"},{"id":430,"pagetitle":"Message Passing","title":"GNNlib.aggregate_neighbors","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNlib/api/messagepassing/#GNNlib.aggregate_neighbors","content":" GNNlib.aggregate_neighbors  —  Function aggregate_neighbors(g, aggr, m) Given a graph  g , edge features  m , and an aggregation operator  aggr  (e.g  +, min, max, mean ), returns the new node features  \\[\\mathbf{x}_i = \\square_{j \\in \\mathcal{N}(i)} \\mathbf{m}_{j\\to i}\\] Neighborhood aggregation is the second step of  propagate ,  where it comes after  apply_edges . source"},{"id":431,"pagetitle":"Message Passing","title":"GNNlib.propagate","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNlib/api/messagepassing/#GNNlib.propagate","content":" GNNlib.propagate  —  Function propagate(fmsg, g, aggr; [xi, xj, e])\npropagate(fmsg, g, aggr xi, xj, e=nothing) Performs message passing on graph  g . Takes care of materializing the node features on each edge,  applying the message function  fmsg , and returning an aggregated message  $\\bar{\\mathbf{m}}$   (depending on the return value of  fmsg , an array or a named tuple of  arrays with last dimension's size  g.num_nodes ). It can be decomposed in two steps: m = apply_edges(fmsg, g, xi, xj, e)\nm̄ = aggregate_neighbors(g, aggr, m) GNN layers typically call  propagate  in their forward pass, providing as input  f  a closure.   Arguments g : A  GNNGraph . xi : An array or a named tuple containing arrays whose last dimension's size        is  g.num_nodes . It will be appropriately materialized on the       target node of each edge (see also  edge_index ). xj : As  xj , but to be materialized on edges' sources.  e : An array or a named tuple containing arrays whose last dimension's size is  g.num_edges . fmsg : A generic function that will be passed over to  apply_edges .      Has to take as inputs the edge-materialized  xi ,  xj , and  e       (arrays or named tuples of arrays whose last dimension' size is the size of      a batch of edges). Its output has to be an array or a named tuple of arrays     with the same batch size. If also  layer  is passed to propagate,     the signature of  fmsg  has to be  fmsg(layer, xi, xj, e)       instead of  fmsg(xi, xj, e) . aggr : Neighborhood aggregation operator. Use  + ,  mean ,  max , or  min .  Examples using GraphNeuralNetworks, Flux\n\nstruct GNNConv <: GNNLayer\n    W\n    b\n    σ\nend\n\nFlux.@layer GNNConv\n\nfunction GNNConv(ch::Pair{Int,Int}, σ=identity)\n    in, out = ch\n    W = Flux.glorot_uniform(out, in)\n    b = zeros(Float32, out)\n    GNNConv(W, b, σ)\nend\n\nfunction (l::GNNConv)(g::GNNGraph, x::AbstractMatrix)\n    message(xi, xj, e) = l.W * xj\n    m̄ = propagate(message, g, +, xj=x)\n    return l.σ.(m̄ .+ l.bias)\nend\n\nl = GNNConv(10 => 20)\nl(g, x) See also  apply_edges  and  aggregate_neighbors . source"},{"id":432,"pagetitle":"Message Passing","title":"Built-in message functions","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNlib/api/messagepassing/#Built-in-message-functions","content":" Built-in message functions"},{"id":433,"pagetitle":"Message Passing","title":"GNNlib.copy_xi","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNlib/api/messagepassing/#GNNlib.copy_xi","content":" GNNlib.copy_xi  —  Function copy_xi(xi, xj, e) = xi source"},{"id":434,"pagetitle":"Message Passing","title":"GNNlib.copy_xj","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNlib/api/messagepassing/#GNNlib.copy_xj","content":" GNNlib.copy_xj  —  Function copy_xj(xi, xj, e) = xj source"},{"id":435,"pagetitle":"Message Passing","title":"GNNlib.xi_dot_xj","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNlib/api/messagepassing/#GNNlib.xi_dot_xj","content":" GNNlib.xi_dot_xj  —  Function xi_dot_xj(xi, xj, e) = sum(xi .* xj, dims=1) source"},{"id":436,"pagetitle":"Message Passing","title":"GNNlib.xi_sub_xj","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNlib/api/messagepassing/#GNNlib.xi_sub_xj","content":" GNNlib.xi_sub_xj  —  Function xi_sub_xj(xi, xj, e) = xi .- xj source"},{"id":437,"pagetitle":"Message Passing","title":"GNNlib.xj_sub_xi","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNlib/api/messagepassing/#GNNlib.xj_sub_xi","content":" GNNlib.xj_sub_xi  —  Function xj_sub_xi(xi, xj, e) = xj .- xi source"},{"id":438,"pagetitle":"Message Passing","title":"GNNlib.e_mul_xj","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNlib/api/messagepassing/#GNNlib.e_mul_xj","content":" GNNlib.e_mul_xj  —  Function e_mul_xj(xi, xj, e) = reshape(e, (...)) .* xj Reshape  e  into a broadcast compatible shape with  xj  (by prepending singleton dimensions) then perform broadcasted multiplication. source"},{"id":439,"pagetitle":"Message Passing","title":"GNNlib.w_mul_xj","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNlib/api/messagepassing/#GNNlib.w_mul_xj","content":" GNNlib.w_mul_xj  —  Function w_mul_xj(xi, xj, w) = reshape(w, (...)) .* xj Similar to  e_mul_xj  but specialized on scalar edge features (weights). source"},{"id":442,"pagetitle":"Other Operators","title":"Utility Functions","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNlib/api/utils/#Utility-Functions","content":" Utility Functions"},{"id":443,"pagetitle":"Other Operators","title":"Graph-wise operations","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNlib/api/utils/#Graph-wise-operations","content":" Graph-wise operations"},{"id":444,"pagetitle":"Other Operators","title":"GNNlib.reduce_nodes","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNlib/api/utils/#GNNlib.reduce_nodes","content":" GNNlib.reduce_nodes  —  Function reduce_nodes(aggr, g, x) For a batched graph  g , return the graph-wise aggregation of the node features  x . The aggregation operator  aggr  can be  + ,  mean ,  max , or  min . The returned array will have last dimension  g.num_graphs . See also:  reduce_edges . source reduce_nodes(aggr, indicator::AbstractVector, x) Return the graph-wise aggregation of the node features  x  given the graph indicator  indicator . The aggregation operator  aggr  can be  + ,  mean ,  max , or  min . See also  graph_indicator . source"},{"id":445,"pagetitle":"Other Operators","title":"GNNlib.reduce_edges","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNlib/api/utils/#GNNlib.reduce_edges","content":" GNNlib.reduce_edges  —  Function reduce_edges(aggr, g, e) For a batched graph  g , return the graph-wise aggregation of the edge features  e . The aggregation operator  aggr  can be  + ,  mean ,  max , or  min . The returned array will have last dimension  g.num_graphs . source"},{"id":446,"pagetitle":"Other Operators","title":"GNNlib.softmax_nodes","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNlib/api/utils/#GNNlib.softmax_nodes","content":" GNNlib.softmax_nodes  —  Function softmax_nodes(g, x) Graph-wise softmax of the node features  x . source"},{"id":447,"pagetitle":"Other Operators","title":"GNNlib.softmax_edges","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNlib/api/utils/#GNNlib.softmax_edges","content":" GNNlib.softmax_edges  —  Function softmax_edges(g, e) Graph-wise softmax of the edge features  e . source"},{"id":448,"pagetitle":"Other Operators","title":"GNNlib.broadcast_nodes","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNlib/api/utils/#GNNlib.broadcast_nodes","content":" GNNlib.broadcast_nodes  —  Function broadcast_nodes(g, x) Graph-wise broadcast array  x  of size  (*, g.num_graphs)   to size  (*, g.num_nodes) . source"},{"id":449,"pagetitle":"Other Operators","title":"GNNlib.broadcast_edges","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNlib/api/utils/#GNNlib.broadcast_edges","content":" GNNlib.broadcast_edges  —  Function broadcast_edges(g, x) Graph-wise broadcast array  x  of size  (*, g.num_graphs)   to size  (*, g.num_edges) . source"},{"id":450,"pagetitle":"Other Operators","title":"Neighborhood operations","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNlib/api/utils/#Neighborhood-operations","content":" Neighborhood operations"},{"id":451,"pagetitle":"Other Operators","title":"GNNlib.softmax_edge_neighbors","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNlib/api/utils/#GNNlib.softmax_edge_neighbors","content":" GNNlib.softmax_edge_neighbors  —  Function softmax_edge_neighbors(g, e) Softmax over each node's neighborhood of the edge features  e . \\[\\mathbf{e}'_{j\\to i} = \\frac{e^{\\mathbf{e}_{j\\to i}}}\n                    {\\sum_{j'\\in N(i)} e^{\\mathbf{e}_{j'\\to i}}}.\\] source"},{"id":452,"pagetitle":"Other Operators","title":"NNlib's gather and scatter functions","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNlib/api/utils/#NNlib's-gather-and-scatter-functions","content":" NNlib's gather and scatter functions Primitive functions for message passing implemented in  NNlib.jl : gather! gather scatter! scatter"},{"id":455,"pagetitle":"Message Passing","title":"Message Passing","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNlib/guides/messagepassing/#Message-Passing","content":" Message Passing A generic message passing on graph takes the form \\[\\begin{aligned}\n\\mathbf{m}_{j\\to i} &= \\phi(\\mathbf{x}_i, \\mathbf{x}_j, \\mathbf{e}_{j\\to i}) \\\\\n\\bar{\\mathbf{m}}_{i} &= \\square_{j\\in N(i)}  \\mathbf{m}_{j\\to i} \\\\\n\\mathbf{x}_{i}' &= \\gamma_x(\\mathbf{x}_{i}, \\bar{\\mathbf{m}}_{i})\\\\\n\\mathbf{e}_{j\\to i}^\\prime &=  \\gamma_e(\\mathbf{e}_{j \\to i},\\mathbf{m}_{j \\to i})\n\\end{aligned}\\] where we refer to  $\\phi$  as to the message function,  and to  $\\gamma_x$  and  $\\gamma_e$  as to the node update and edge update function respectively. The aggregation  $\\square$  is over the neighborhood  $N(i)$  of node  $i$ ,  and it is usually equal either to  $\\sum$ , to  max  or to a  mean  operation.  In GNNlib.jl, the message passing mechanism is exposed by the  propagate  function.  propagate  takes care of materializing the node features on each edge, applying the message function, performing the aggregation, and returning  $\\bar{\\mathbf{m}}$ .  It is then left to the user to perform further node and edge updates, manipulating arrays of size  $D_{node} \\times num\\_nodes$  and     $D_{edge} \\times num\\_edges$ . propagate  is composed of two steps, also available as two independent methods: apply_edges  materializes node features on edges and applies the message function.  aggregate_neighbors  applies a reduction operator on the messages coming from the neighborhood of each node. The whole propagation mechanism internally relies on the  NNlib.gather   and  NNlib.scatter  methods."},{"id":456,"pagetitle":"Message Passing","title":"Examples","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNlib/guides/messagepassing/#Examples","content":" Examples"},{"id":457,"pagetitle":"Message Passing","title":"Basic use of apply_edges and propagate","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNlib/guides/messagepassing/#Basic-use-of-apply_edges-and-propagate","content":" Basic use of apply_edges and propagate The function  apply_edges  can be used to broadcast node data on each edge and produce new edge data. julia> using GNNlib, Graphs, Statistics\n\njulia> g = rand_graph(10, 20)\nGNNGraph:\n    num_nodes = 10\n    num_edges = 20\n\njulia> x = ones(2,10);\n\njulia> z = 2ones(2,10);\n\n# Return an edge features arrays (D × num_edges)\njulia> apply_edges((xi, xj, e) -> xi .+ xj, g, xi=x, xj=z)\n2×20 Matrix{Float64}:\n 3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0\n 3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0\n\n# now returning a named tuple\njulia> apply_edges((xi, xj, e) -> (a=xi .+ xj, b=xi .- xj), g, xi=x, xj=z)\n(a = [3.0 3.0 … 3.0 3.0; 3.0 3.0 … 3.0 3.0], b = [-1.0 -1.0 … -1.0 -1.0; -1.0 -1.0 … -1.0 -1.0])\n\n# Here we provide a named tuple input\njulia> apply_edges((xi, xj, e) -> xi.a + xi.b .* xj, g, xi=(a=x,b=z), xj=z)\n2×20 Matrix{Float64}:\n 5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0\n 5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0 The function  propagate  instead performs the  apply_edges  operation but then also applies a reduction over each node's neighborhood (see  aggregate_neighbors ). julia> propagate((xi, xj, e) -> xi .+ xj, g, +, xi=x, xj=z)\n2×10 Matrix{Float64}:\n 3.0  6.0  9.0  9.0  0.0  6.0  6.0  3.0  15.0  3.0\n 3.0  6.0  9.0  9.0  0.0  6.0  6.0  3.0  15.0  3.0\n\n# Previous output can be understood by looking at the degree\njulia> degree(g)\n10-element Vector{Int64}:\n 1\n 2\n 3\n 3\n 0\n 2\n 2\n 1\n 5\n 1"},{"id":458,"pagetitle":"Message Passing","title":"Implementing a custom Graph Convolutional Layer using Flux.jl","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNlib/guides/messagepassing/#Implementing-a-custom-Graph-Convolutional-Layer-using-Flux.jl","content":" Implementing a custom Graph Convolutional Layer using Flux.jl Let's implement a simple graph convolutional layer using the message passing framework using the machine learning framework Flux.jl. The convolution reads  \\[\\mathbf{x}'_i = W \\cdot \\sum_{j \\in N(i)}  \\mathbf{x}_j\\] We will also add a bias and an activation function. using Flux, Graphs, GraphNeuralNetworks\n\nstruct GCN{A<:AbstractMatrix, B, F} <: GNNLayer\n    weight::A\n    bias::B\n    σ::F\nend\n\nFlux.@layer GCN # allow gpu movement, select trainable params etc...\n\nfunction GCN(ch::Pair{Int,Int}, σ=identity)\n    in, out = ch\n    W = Flux.glorot_uniform(out, in)\n    b = zeros(Float32, out)\n    GCN(W, b, σ)\nend\n\nfunction (l::GCN)(g::GNNGraph, x::AbstractMatrix{T}) where T\n    @assert size(x, 2) == g.num_nodes\n\n    # Computes messages from source/neighbour nodes (j) to target/root nodes (i).\n    # The message function will have to handle matrices of size (*, num_edges).\n    # In this simple case we just let the neighbor features go through.\n    message(xi, xj, e) = xj \n\n    # The + operator gives the sum aggregation.\n    # `mean`, `max`, `min`, and `*` are other possibilities.\n    x = propagate(message, g, +, xj=x) \n\n    return l.σ.(l.weight * x .+ l.bias)\nend See the  GATConv  implementation  here  for a more complex example."},{"id":459,"pagetitle":"Message Passing","title":"Built-in message functions","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/GNNlib/guides/messagepassing/#Built-in-message-functions","content":" Built-in message functions In order to exploit optimized specializations of the  propagate , it is recommended  to use built-in message functions such as  copy_xj  whenever possible. "},{"id":462,"pagetitle":"Basic layers","title":"Basic Layers","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/api/basic/#Basic-Layers","content":" Basic Layers"},{"id":463,"pagetitle":"Basic layers","title":"GNNLux.GNNLayer","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/api/basic/#GNNLux.GNNLayer","content":" GNNLux.GNNLayer  —  Type abstract type GNNLayer <: AbstractLuxLayer end An abstract type from which graph neural network layers are derived. It is derived from Lux's  AbstractLuxLayer  type. See also  GNNLux.GNNChain . source"},{"id":464,"pagetitle":"Basic layers","title":"GNNLux.GNNChain","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/api/basic/#GNNLux.GNNChain","content":" GNNLux.GNNChain  —  Type GNNChain(layers...)\nGNNChain(name = layer, ...) Collects multiple layers / functions to be called in sequence on given input graph and input node features.  It allows to compose layers in a sequential fashion as  Lux.Chain  does, propagating the output of each layer to the next one. In addition,  GNNChain  handles the input graph as well, providing it  as a first argument only to layers subtyping the  GNNLayer  abstract type.  GNNChain  supports indexing and slicing,  m[2]  or  m[1:end-1] , and if names are given,  m[:name] == m[1]  etc. Examples julia> using Lux, GNNLux, Random\n\njulia> rng = Random.default_rng();\n\njulia> m = GNNChain(GCNConv(2=>5), \n                    x -> relu.(x), \n                    Dense(5=>4))\n\njulia> x = randn(rng, Float32, 2, 3);\n\njulia> g = rand_graph(rng, 3, 6)\nGNNGraph:\n  num_nodes: 3\n  num_edges: 6\n\njulia> ps, st = LuxCore.setup(rng, m);\n\njulia> m(g, x, ps, st)     # First entry is the output, second entry is the state of the model\n(Float32[-0.15594329 -0.15594329 -0.15594329; 0.93431795 0.93431795 0.93431795; 0.27568763 0.27568763 0.27568763; 0.12568939 0.12568939 0.12568939], (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = NamedTuple())) source"},{"id":467,"pagetitle":"Convolutional layers","title":"Convolutional Layers","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/api/conv/#Convolutional-Layers","content":" Convolutional Layers Many different types of graphs convolutional layers have been proposed in the literature. Choosing the right layer for your application could involve a lot of exploration.  Multiple graph convolutional layers are typically stacked together to create a graph neural network model (see  GNNChain ). The table below lists all graph convolutional layers implemented in the  GNNLux.jl . It also highlights the presence of some additional capabilities with respect to basic message passing: Sparse Ops : implements message passing as multiplication by sparse adjacency matrix instead of the gather/scatter mechanism. This can lead to better CPU performances but it is not supported on GPU yet.  Edge Weight : supports scalar weights (or equivalently scalar features) on edges.  Edge Features : supports feature vectors on edges. Heterograph : supports heterogeneous graphs (see  GNNHeteroGraph ). TemporalSnapshotsGNNGraphs : supports temporal graphs (see  TemporalSnapshotsGNNGraph ) by applying the convolution layers to each snapshot independently. Layer Sparse Ops Edge Weight Edge Features Heterograph TemporalSnapshotsGNNGraphs AGNNConv ✓ CGConv ✓ ✓ ✓ ChebConv ✓ EGNNConv ✓ EdgeConv ✓ GATConv ✓ ✓ ✓ GATv2Conv ✓ ✓ ✓ GatedGraphConv ✓ ✓ GCNConv ✓ ✓ ✓ GINConv ✓ ✓ ✓ GMMConv ✓ GraphConv ✓ ✓ ✓ MEGNetConv ✓ NNConv ✓ ResGatedGraphConv ✓ ✓ SAGEConv ✓ ✓ ✓ SGConv ✓ ✓"},{"id":468,"pagetitle":"Convolutional layers","title":"GNNLux.AGNNConv","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/api/conv/#GNNLux.AGNNConv","content":" GNNLux.AGNNConv  —  Type AGNNConv(; init_beta=1.0f0, trainable=true, add_self_loops=true) Attention-based Graph Neural Network layer from paper  Attention-based Graph Neural Network for Semi-Supervised Learning . The forward pass is given by \\[\\mathbf{x}_i' = \\sum_{j \\in N(i)} \\alpha_{ij} \\mathbf{x}_j\\] where the attention coefficients  $\\alpha_{ij}$  are given by \\[\\alpha_{ij} =\\frac{e^{\\beta \\cos(\\mathbf{x}_i, \\mathbf{x}_j)}}\n                  {\\sum_{j'}e^{\\beta \\cos(\\mathbf{x}_i, \\mathbf{x}_{j'})}}\\] with the cosine distance defined by \\[\\cos(\\mathbf{x}_i, \\mathbf{x}_j) = \n  \\frac{\\mathbf{x}_i \\cdot \\mathbf{x}_j}{\\lVert\\mathbf{x}_i\\rVert \\lVert\\mathbf{x}_j\\rVert}\\] and  $\\beta$  a trainable parameter if  trainable=true . Arguments init_beta : The initial value of  $\\beta$ . Default 1.0f0. trainable : If true,  $\\beta$  is trainable. Default  true . add_self_loops : Add self loops to the graph before performing the convolution. Default  true . Examples: using GNNLux, Lux, Random\n\n# initialize random number generator\nrng = Random.default_rng()\n\n# create data\ns = [1,1,2,3]\nt = [2,3,1,1]\ng = GNNGraph(s, t)\n\n# create layer\nl = AGNNConv(init_beta=2.0f0)\n\n# setup layer\nps, st = LuxCore.setup(rng, l)\n\n# forward pass\ny, st = l(g, x, ps, st)    source"},{"id":469,"pagetitle":"Convolutional layers","title":"GNNLux.CGConv","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/api/conv/#GNNLux.CGConv","content":" GNNLux.CGConv  —  Type CGConv((in, ein) => out, act = identity; residual = false,\n            use_bias = true, init_weight = glorot_uniform, init_bias = zeros32)\nCGConv(in => out, ...) The crystal graph convolutional layer from the paper  Crystal Graph Convolutional Neural Networks for an Accurate and Interpretable Prediction of Material Properties . Performs the operation \\[\\mathbf{x}_i' = \\mathbf{x}_i + \\sum_{j\\in N(i)}\\sigma(W_f \\mathbf{z}_{ij} + \\mathbf{b}_f)\\, act(W_s \\mathbf{z}_{ij} + \\mathbf{b}_s)\\] where  $\\mathbf{z}_{ij}$   is the node and edge features concatenation   $[\\mathbf{x}_i; \\mathbf{x}_j; \\mathbf{e}_{j\\to i}]$   and  $\\sigma$  is the sigmoid function. The residual  $\\mathbf{x}_i$  is added only if  residual=true  and the output size is the same  as the input size. Arguments in : The dimension of input node features. ein : The dimension of input edge features.  If  ein  is not given, assumes that no edge features are passed as input in the forward pass. out : The dimension of output node features. act : Activation function. residual : Add a residual connection. init_weight : Weights' initializer. Default  glorot_uniform . init_bias : Bias initializer. Default  zeros32 . use_bias : Add learnable bias. Default  true . Examples using GNNLux, Lux, Random\n\n# initialize random number generator\nrng = Random.default_rng()\n\n# create random graph\ng = rand_graph(rng, 5, 6)\nx = rand(rng, Float32, 2, g.num_nodes)\ne = rand(rng, Float32, 3, g.num_edges)\n\nl = CGConv((2, 3) => 4, tanh)\n\n# setup layer\nps, st = LuxCore.setup(rng, l)\n\n# forward pass\ny, st = l(g, x, e, ps, st)    # size: (4, num_nodes)\n\n# No edge features\nl = CGConv(2 => 4, tanh)\nps, st = LuxCore.setup(rng, l)\ny, st = l(g, x, ps, st)    # size: (4, num_nodes) source"},{"id":470,"pagetitle":"Convolutional layers","title":"GNNLux.ChebConv","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/api/conv/#GNNLux.ChebConv","content":" GNNLux.ChebConv  —  Type ChebConv(in => out, k; init_weight = glorot_uniform, init_bias = zeros32, use_bias = true) Chebyshev spectral graph convolutional layer from paper  Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering . Implements \\[X' = \\sum^{K-1}_{k=0}  W^{(k)} Z^{(k)}\\] where  $Z^{(k)}$  is the  $k$ -th term of Chebyshev polynomials, and can be calculated by the following recursive form: \\[\\begin{aligned}\nZ^{(0)} &= X \\\\\nZ^{(1)} &= \\hat{L} X \\\\\nZ^{(k)} &= 2 \\hat{L} Z^{(k-1)} - Z^{(k-2)}\n\\end{aligned}\\] with  $\\hat{L}$  the  scaled_laplacian . Arguments in : The dimension of input features. out : The dimension of output features. k : The order of Chebyshev polynomial. init_weight : Weights' initializer. Default  glorot_uniform . init_bias : Bias initializer. Default  zeros32 . use_bias : Add learnable bias. Default  true . Examples using GNNLux, Lux, Random\n\n# initialize random number generator\nrng = Random.default_rng()\n\n# create data\ns = [1,1,2,3]\nt = [2,3,1,1]\ng = GNNGraph(s, t)\nx = randn(rng, Float32, 3, g.num_nodes)\n\n# create layer\nl = ChebConv(3 => 5, 5)\n\n# setup layer\nps, st = LuxCore.setup(rng, l)\n\n# forward pass\ny, st = l(g, x, ps, st)       # size of the output y:  5 × num_nodes source"},{"id":471,"pagetitle":"Convolutional layers","title":"GNNLux.DConv","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/api/conv/#GNNLux.DConv","content":" GNNLux.DConv  —  Type DConv(in => out, k; init_weight = glorot_uniform, init_bias = zeros32, use_bias = true) Diffusion convolution layer from the paper  Diffusion Convolutional Recurrent Neural Networks: Data-Driven Traffic Forecasting . Arguments in : The dimension of input features. out : The dimension of output features. k : Number of diffusion steps. init_weight : Weights' initializer. Default  glorot_uniform . init_bias : Bias initializer. Default  zeros32 . use_bias : Add learnable bias. Default  true . Examples using GNNLux, Lux, Random\n\n# initialize random number generator\nrng = Random.default_rng()\n\n# create random graph\ng = GNNGraph(rand(rng, 10, 10), ndata = rand(rng, Float32, 2, 10))\n\ndconv = DConv(2 => 4, 4)\n\n# setup layer\nps, st = LuxCore.setup(rng, dconv)\n\n# forward pass\ny, st = dconv(g, g.ndata.x, ps, st)   # size: (4, num_nodes) source"},{"id":472,"pagetitle":"Convolutional layers","title":"GNNLux.EGNNConv","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/api/conv/#GNNLux.EGNNConv","content":" GNNLux.EGNNConv  —  Type EGNNConv((in, ein) => out; hidden_size=2in, residual=false)\nEGNNConv(in => out; hidden_size=2in, residual=false) Equivariant Graph Convolutional Layer from  E(n) Equivariant Graph Neural Networks . The layer performs the following operation: \\[\\begin{aligned}\n\\mathbf{m}_{j\\to i} &=\\phi_e(\\mathbf{h}_i, \\mathbf{h}_j, \\lVert\\mathbf{x}_i-\\mathbf{x}_j\\rVert^2, \\mathbf{e}_{j\\to i}),\\\\\n\\mathbf{x}_i' &= \\mathbf{x}_i + C_i\\sum_{j\\in\\mathcal{N}(i)}(\\mathbf{x}_i-\\mathbf{x}_j)\\phi_x(\\mathbf{m}_{j\\to i}),\\\\\n\\mathbf{m}_i &= C_i\\sum_{j\\in\\mathcal{N}(i)} \\mathbf{m}_{j\\to i},\\\\\n\\mathbf{h}_i' &= \\mathbf{h}_i + \\phi_h(\\mathbf{h}_i, \\mathbf{m}_i)\n\\end{aligned}\\] where  $\\mathbf{h}_i$ ,  $\\mathbf{x}_i$ ,  $\\mathbf{e}_{j\\to i}$  are invariant node features, equivariant node features, and edge features respectively.  $\\phi_e$ ,  $\\phi_h$ , and  $\\phi_x$  are two-layer MLPs.  C  is a constant for normalization, computed as  $1/|\\mathcal{N}(i)|$ . Constructor Arguments in : Number of input features for  h . out : Number of output features for  h . ein : Number of input edge features. hidden_size : Hidden representation size. residual : If  true , add a residual connection. Only possible if  in == out . Default  false . Forward Pass l(g, x, h, e=nothing, ps, st) Forward Pass Arguments: g  : The graph. x  : Matrix of equivariant node coordinates. h  : Matrix of invariant node features. e  : Matrix of invariant edge features. Default  nothing . ps  : Parameters. st  : State. Returns updated  h  and  x . Examples using GNNLux, Lux, Random\n\n# initialize random number generator\nrng = Random.default_rng()\n\n# create random graph\ng = rand_graph(rng, 10, 10)\nh = randn(rng, Float32, 5, g.num_nodes)\nx = randn(rng, Float32, 3, g.num_nodes)\n\negnn = EGNNConv(5 => 6, 10)\n\n# setup layer\nps, st = LuxCore.setup(rng, egnn)\n\n# forward pass\n(hnew, xnew), st = egnn(g, h, x, ps, st) source"},{"id":473,"pagetitle":"Convolutional layers","title":"GNNLux.EdgeConv","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/api/conv/#GNNLux.EdgeConv","content":" GNNLux.EdgeConv  —  Type EdgeConv(nn; aggr=max) Edge convolutional layer from paper  Dynamic Graph CNN for Learning on Point Clouds . Performs the operation \\[\\mathbf{x}_i' = \\square_{j \\in N(i)}\\, nn([\\mathbf{x}_i; \\mathbf{x}_j - \\mathbf{x}_i])\\] where  nn  generally denotes a learnable function, e.g. a linear layer or a multi-layer perceptron. Arguments nn : A (possibly learnable) function.  aggr : Aggregation operator for the incoming messages (e.g.  + ,  * ,  max ,  min , and  mean ). Examples: using GNNLux, Lux, Random\n\n# initialize random number generator\nrng = Random.default_rng()\n\n# create data\ns = [1,1,2,3]\nt = [2,3,1,1]\nin_channel = 3\nout_channel = 5\ng = GNNGraph(s, t)\nx = rand(rng, Float32, in_channel, g.num_nodes)\n\n# create layer\nl = EdgeConv(Dense(2 * in_channel, out_channel), aggr = +)\n\n# setup layer\nps, st = LuxCore.setup(rng, l)\n\n# forward pass\ny, st = l(g, x, ps, st) source"},{"id":474,"pagetitle":"Convolutional layers","title":"GNNLux.GATConv","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/api/conv/#GNNLux.GATConv","content":" GNNLux.GATConv  —  Type GATConv(in => out, σ = identity; heads = 1, concat = true, negative_slope = 0.2, init_weight = glorot_uniform, init_bias = zeros32, use_bias = true, add_self_loops = true, dropout=0.0)\nGATConv((in, ein) => out, ...) Graph attentional layer from the paper  Graph Attention Networks . Implements the operation \\[\\mathbf{x}_i' = \\sum_{j \\in N(i) \\cup \\{i\\}} \\alpha_{ij} W \\mathbf{x}_j\\] where the attention coefficients  $\\alpha_{ij}$  are given by \\[\\alpha_{ij} = \\frac{1}{z_i} \\exp(LeakyReLU(\\mathbf{a}^T [W \\mathbf{x}_i; W \\mathbf{x}_j]))\\] with  $z_i$  a normalization factor.  In case  ein > 0  is given, edge features of dimension  ein  will be expected in the forward pass  and the attention coefficients will be calculated as   \\[\\alpha_{ij} = \\frac{1}{z_i} \\exp(LeakyReLU(\\mathbf{a}^T [W_e \\mathbf{e}_{j\\to i}; W \\mathbf{x}_i; W \\mathbf{x}_j]))\\] Arguments in : The dimension of input node features. ein : The dimension of input edge features. Default 0 (i.e. no edge features passed in the forward). out : The dimension of output node features. σ : Activation function. Default  identity . heads : Number attention heads. Default  1 . concat : Concatenate layer output or not. If not, layer output is averaged over the heads. Default  true . negative_slope : The parameter of LeakyReLU.Default  0.2 . init_weight : Weights' initializer. Default  glorot_uniform . init_bias : Bias initializer. Default  zeros32 . use_bias : Add learnable bias. Default  true . add_self_loops : Add self loops to the graph before performing the convolution. Default  true . dropout : Dropout probability on the normalized attention coefficient. Default  0.0 . Examples using GNNLux, Lux, Random\n\n# initialize random number generator\nrng = Random.default_rng()\n\n# create data\ns = [1,1,2,3]\nt = [2,3,1,1]\nin_channel = 3\nout_channel = 5\ng = GNNGraph(s, t)\nx = randn(rng, Float32, 3, g.num_nodes)\n\n# create layer\nl = GATConv(in_channel => out_channel; add_self_loops = false, use_bias = false, heads=2, concat=true)\n\n# setup layer\nps, st = LuxCore.setup(rng, l)\n\n# forward pass\ny, st = l(g, x, ps, st)        source"},{"id":475,"pagetitle":"Convolutional layers","title":"GNNLux.GATv2Conv","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/api/conv/#GNNLux.GATv2Conv","content":" GNNLux.GATv2Conv  —  Type GATv2Conv(in => out, σ = identity; heads = 1, concat = true, negative_slope = 0.2, init_weight = glorot_uniform, init_bias = zeros32, use_bias = true, add_self_loops = true, dropout=0.0)\nGATv2Conv((in, ein) => out, ...) GATv2 attentional layer from the paper  How Attentive are Graph Attention Networks? . Implements the operation \\[\\mathbf{x}_i' = \\sum_{j \\in N(i) \\cup \\{i\\}} \\alpha_{ij} W_1 \\mathbf{x}_j\\] where the attention coefficients  $\\alpha_{ij}$  are given by \\[\\alpha_{ij} = \\frac{1}{z_i} \\exp(\\mathbf{a}^T LeakyReLU(W_2 \\mathbf{x}_i + W_1 \\mathbf{x}_j))\\] with  $z_i$  a normalization factor. In case  ein > 0  is given, edge features of dimension  ein  will be expected in the forward pass  and the attention coefficients will be calculated as   \\[\\alpha_{ij} = \\frac{1}{z_i} \\exp(\\mathbf{a}^T LeakyReLU(W_3 \\mathbf{e}_{j\\to i} + W_2 \\mathbf{x}_i + W_1 \\mathbf{x}_j)).\\] Arguments in : The dimension of input node features. ein : The dimension of input edge features. Default 0 (i.e. no edge features passed in the forward). out : The dimension of output node features. σ : Activation function. Default  identity . heads : Number attention heads. Default  1 . concat : Concatenate layer output or not. If not, layer output is averaged over the heads. Default  true . negative_slope : The parameter of LeakyReLU.Default  0.2 . add_self_loops : Add self loops to the graph before performing the convolution. Default  true . dropout : Dropout probability on the normalized attention coefficient. Default  0.0 . init_weight : Weights' initializer. Default  glorot_uniform . init_bias : Bias initializer. Default  zeros32 . use_bias : Add learnable bias. Default  true . Examples using GNNLux, Lux, Random\n\n# initialize random number generator\nrng = Random.default_rng()\n\n# create data\ns = [1,1,2,3]\nt = [2,3,1,1]\nin_channel = 3\nout_channel = 5\nein = 3\ng = GNNGraph(s, t)\nx = randn(rng, Float32, 3, g.num_nodes)\n\n# create layer\nl = GATv2Conv((in_channel, ein) => out_channel, add_self_loops = false)\n\n# setup layer\nps, st = LuxCore.setup(rng, l)\n\n# edge features\ne = randn(rng, Float32, ein, length(s))\n\n# forward pass\ny, st = l(g, x, e, ps, st)     source"},{"id":476,"pagetitle":"Convolutional layers","title":"GNNLux.GCNConv","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/api/conv/#GNNLux.GCNConv","content":" GNNLux.GCNConv  —  Type GCNConv(in => out, σ=identity; [init_weight, init_bias, use_bias, add_self_loops, use_edge_weight]) Graph convolutional layer from paper  Semi-supervised Classification with Graph Convolutional Networks . Performs the operation \\[\\mathbf{x}'_i = \\sum_{j\\in N(i)} a_{ij} W \\mathbf{x}_j\\] where  $a_{ij} = 1 / \\sqrt{|N(i)||N(j)|}$  is a normalization factor computed from the node degrees.  If the input graph has weighted edges and  use_edge_weight=true , than  $a_{ij}$  will be computed as \\[a_{ij} = \\frac{e_{j\\to i}}{\\sqrt{\\sum_{j \\in N(i)}  e_{j\\to i}} \\sqrt{\\sum_{i \\in N(j)}  e_{i\\to j}}}\\] Arguments in : Number of input features. out : Number of output features. σ : Activation function. Default  identity . init_weight : Weights' initializer. Default  glorot_uniform . init_bias : Bias initializer. Default  zeros32 . use_bias : Add learnable bias. Default  true . add_self_loops : Add self loops to the graph before performing the convolution. Default  false . use_edge_weight : If  true , consider the edge weights in the input graph (if available).                    If  add_self_loops=true  the new weights will be set to 1.                     This option is ignored if the  edge_weight  is explicitly provided in the forward pass.                    Default  false . Forward (::GCNConv)(g, x, [edge_weight], ps, st; norm_fn = d -> 1 ./ sqrt.(d), conv_weight=nothing) Takes as input a graph  g , a node feature matrix  x  of size  [in, num_nodes] , optionally an edge weight vector and the parameter and state of the layer. Returns a node feature matrix of size   [out, num_nodes] . The  norm_fn  parameter allows for custom normalization of the graph convolution operation by passing a function as argument.  By default, it computes  $\\frac{1}{\\sqrt{d}}$  i.e the inverse square root of the degree ( d ) of each node in the graph.  If  conv_weight  is an  AbstractMatrix  of size  [out, in] , then the convolution is performed using that weight matrix. Examples using GNNLux, Lux, Random\n\n# initialize random number generator\nrng = Random.default_rng()\n\n# create data\ns = [1,1,2,3]\nt = [2,3,1,1]\ng = GNNGraph(s, t)\nx = randn(rng, Float32, 3, g.num_nodes)\n\n# create layer\nl = GCNConv(3 => 5) \n\n# setup layer\nps, st = LuxCore.setup(rng, l)\n\n# forward pass\ny = l(g, x, ps, st)       # size of the output first entry:  5 × num_nodes\n\n# convolution with edge weights and custom normalization function\nw = [1.1, 0.1, 2.3, 0.5]\ncustom_norm_fn(d) = 1 ./ sqrt.(d + 1)  # Custom normalization function\ny = l(g, x, w, ps, st; norm_fn = custom_norm_fn)\n\n# Edge weights can also be embedded in the graph.\ng = GNNGraph(s, t, w)\nl = GCNConv(3 => 5, use_edge_weight=true)\nps, st = Lux.setup(rng, l)\ny = l(g, x, ps, st) # same as l(g, x, w)  source"},{"id":477,"pagetitle":"Convolutional layers","title":"GNNLux.GINConv","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/api/conv/#GNNLux.GINConv","content":" GNNLux.GINConv  —  Type GINConv(f, ϵ; aggr=+) Graph Isomorphism convolutional layer from paper  How Powerful are Graph Neural Networks? . Implements the graph convolution \\[\\mathbf{x}_i' = f_\\Theta\\left((1 + \\epsilon) \\mathbf{x}_i + \\sum_{j \\in N(i)} \\mathbf{x}_j \\right)\\] where  $f_\\Theta$  typically denotes a learnable function, e.g. a linear layer or a multi-layer perceptron. Arguments f : A (possibly learnable) function acting on node features.  ϵ : Weighting factor. Examples: using GNNLux, Lux, Random\n\n# initialize random number generator\nrng = Random.default_rng()\n\n# create data\ns = [1,1,2,3]\nt = [2,3,1,1]\nin_channel = 3\nout_channel = 5\ng = GNNGraph(s, t)\nx = randn(rng, Float32, in_channel, g.num_nodes)\n\n# create dense layer\nnn = Dense(in_channel, out_channel)\n\n# create layer\nl = GINConv(nn, 0.01f0, aggr = mean)\n\n# setup layer\nps, st = LuxCore.setup(rng, l)\n\n# forward pass\ny, st = l(g, x, ps, st)       # size:  out_channel × num_nodes source"},{"id":478,"pagetitle":"Convolutional layers","title":"GNNLux.GMMConv","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/api/conv/#GNNLux.GMMConv","content":" GNNLux.GMMConv  —  Type GMMConv((in, ein) => out, σ=identity; K = 1, residual = false init_weight = glorot_uniform, init_bias = zeros32, use_bias = true) Graph mixture model convolution layer from the paper  Geometric deep learning on graphs and manifolds using mixture model CNNs  Performs the operation \\[\\mathbf{x}_i' = \\mathbf{x}_i + \\frac{1}{|N(i)|} \\sum_{j\\in N(i)}\\frac{1}{K}\\sum_{k=1}^K \\mathbf{w}_k(\\mathbf{e}_{j\\to i}) \\odot \\Theta_k \\mathbf{x}_j\\] where  $w^a_{k}(e^a)$  for feature  a  and kernel  k  is given by \\[w^a_{k}(e^a) = \\exp(-\\frac{1}{2}(e^a - \\mu^a_k)^T (\\Sigma^{-1})^a_k(e^a - \\mu^a_k))\\] $\\Theta_k, \\mu^a_k, (\\Sigma^{-1})^a_k$  are learnable parameters. The input to the layer is a node feature array  x  of size  (num_features, num_nodes)  and edge pseudo-coordinate array  e  of size  (num_features, num_edges)  The residual  $\\mathbf{x}_i$  is added only if  residual=true  and the output size is the same  as the input size. Arguments in : Number of input node features. ein : Number of input edge features. out : Number of output features. σ : Activation function. Default  identity . K : Number of kernels. Default  1 . residual : Residual conncetion. Default  false . init_weight : Weights' initializer. Default  glorot_uniform . init_bias : Bias initializer. Default  zeros32 . use_bias : Add learnable bias. Default  true . Examples using GNNLux, Lux, Random\n\n# initialize random number generator\nrng = Random.default_rng()\n\n# create data\ns = [1,1,2,3]\nt = [2,3,1,1]\ng = GNNGraph(s,t)\nnin, ein, out, K = 4, 10, 7, 8 \nx = randn(rng, Float32, nin, g.num_nodes)\ne = randn(rng, Float32, ein, g.num_edges)\n\n# create layer\nl = GMMConv((nin, ein) => out, K=K)\n\n# setup layer\nps, st = LuxCore.setup(rng, l)\n\n# forward pass\ny, st = l(g, x, e, ps, st)       # size:  out × num_nodes source"},{"id":479,"pagetitle":"Convolutional layers","title":"GNNLux.GatedGraphConv","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/api/conv/#GNNLux.GatedGraphConv","content":" GNNLux.GatedGraphConv  —  Type GatedGraphConv(out, num_layers; \n        aggr = +, init_weight = glorot_uniform) Gated graph convolution layer from  Gated Graph Sequence Neural Networks . Implements the recursion \\[\\begin{aligned}\n\\mathbf{h}^{(0)}_i &= [\\mathbf{x}_i; \\mathbf{0}] \\\\\n\\mathbf{h}^{(l)}_i &= GRU(\\mathbf{h}^{(l-1)}_i, \\square_{j \\in N(i)} W \\mathbf{h}^{(l-1)}_j)\n\\end{aligned}\\] where  $\\mathbf{h}^{(l)}_i$  denotes the  $l$ -th hidden variables passing through GRU. The dimension of input  $\\mathbf{x}_i$  needs to be less or equal to  out . Arguments out : The dimension of output features. num_layers : The number of recursion steps. aggr : Aggregation operator for the incoming messages (e.g.  + ,  * ,  max ,  min , and  mean ). init_weight : Weights' initializer. Default  glorot_uniform . Examples: using GNNLux, Lux, Random\n\n# initialize random number generator\nrng = Random.default_rng()\n\n# create data\ns = [1,1,2,3]\nt = [2,3,1,1]\nout_channel = 5\nnum_layers = 3\ng = GNNGraph(s, t)\n\n# create layer\nl = GatedGraphConv(out_channel, num_layers)\n\n# setup layer\nps, st = LuxCore.setup(rng, l)\n\n# forward pass\ny, st = l(g, x, ps, st)       # size:  out_channel × num_nodes   source"},{"id":480,"pagetitle":"Convolutional layers","title":"GNNLux.GraphConv","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/api/conv/#GNNLux.GraphConv","content":" GNNLux.GraphConv  —  Type GraphConv(in => out, σ = identity; aggr = +, init_weight = glorot_uniform,init_bias = zeros32, use_bias = true) Graph convolution layer from Reference:  Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks . Performs: \\[\\mathbf{x}_i' = W_1 \\mathbf{x}_i + \\square_{j \\in \\mathcal{N}(i)} W_2 \\mathbf{x}_j\\] where the aggregation type is selected by  aggr . Arguments in : The dimension of input features. out : The dimension of output features. σ : Activation function. aggr : Aggregation operator for the incoming messages (e.g.  + ,  * ,  max ,  min , and  mean ). init_weight : Weights' initializer. Default  glorot_uniform . init_bias : Bias initializer. Default  zeros32 . use_bias : Add learnable bias. Default  true . Examples using GNNLux, Lux, Random\n\n# initialize random number generator\nrng = Random.default_rng()\n\n# create data\ns = [1,1,2,3]\nt = [2,3,1,1]\nin_channel = 3\nout_channel = 5\ng = GNNGraph(s, t)\nx = randn(rng, Float32, 3, g.num_nodes)\n\n# create layer\nl = GraphConv(in_channel => out_channel, relu, use_bias = false, aggr = mean)\n\n# setup layer\nps, st = LuxCore.setup(rng, l)\n\n# forward pass\ny, st = l(g, x, ps, st)       # size of the output y:  5 × num_nodes source"},{"id":481,"pagetitle":"Convolutional layers","title":"GNNLux.MEGNetConv","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/api/conv/#GNNLux.MEGNetConv","content":" GNNLux.MEGNetConv  —  Type MEGNetConv(ϕe, ϕv; aggr=mean)\nMEGNetConv(in => out; aggr=mean) Convolution from  Graph Networks as a Universal Machine Learning Framework for Molecules and Crystals  paper. In the forward pass, takes as inputs node features  x  and edge features  e  and returns updated features  x'  and  e'  according to  \\[\\begin{aligned}\n\\mathbf{e}_{i\\to j}'  = \\phi_e([\\mathbf{x}_i;\\,  \\mathbf{x}_j;\\,  \\mathbf{e}_{i\\to j}]),\\\\\n\\mathbf{x}_{i}'  = \\phi_v([\\mathbf{x}_i;\\, \\square_{j\\in \\mathcal{N}(i)}\\,\\mathbf{e}_{j\\to i}']).\n\\end{aligned}\\] aggr  defines the aggregation to be performed. If the neural networks  ϕe  and   ϕv  are not provided, they will be constructed from the  in  and  out  arguments instead as multi-layer perceptron with one hidden layer and  relu   activations. Examples using GNNLux, Lux, Random\n\n# initialize random number generator\nrng = Random.default_rng()\n\n# create a random graph\ng = rand_graph(rng, 10, 30)\nx = randn(rng, Float32, 3, 10)\ne = randn(rng, Float32, 3, 30)\n\n# create a MEGNetConv layer\nm = MEGNetConv(3 => 3)\n\n# setup layer\nps, st = LuxCore.setup(rng, m)\n\n# forward pass\n(x′, e′), st = m(g, x, e, ps, st) source"},{"id":482,"pagetitle":"Convolutional layers","title":"GNNLux.NNConv","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/api/conv/#GNNLux.NNConv","content":" GNNLux.NNConv  —  Type NNConv(in => out, f, σ=identity; aggr=+, init_bias = zeros32, use_bias = true, init_weight = glorot_uniform) The continuous kernel-based convolutional operator from the   Neural Message Passing for Quantum Chemistry  paper.  This convolution is also known as the edge-conditioned convolution from the   Dynamic Edge-Conditioned Filters in Convolutional Neural Networks on Graphs  paper. Performs the operation \\[\\mathbf{x}_i' = W \\mathbf{x}_i + \\square_{j \\in N(i)} f_\\Theta(\\mathbf{e}_{j\\to i})\\,\\mathbf{x}_j\\] where  $f_\\Theta$   denotes a learnable function (e.g. a linear layer or a multi-layer perceptron). Given an input of batched edge features  e  of size  (num_edge_features, num_edges) ,  the function  f  will return an batched matrices array whose size is  (out, in, num_edges) . For convenience, also functions returning a single  (out*in, num_edges)  matrix are allowed. Arguments in : The dimension of input node features. out : The dimension of output node features. f : A (possibly learnable) function acting on edge features. aggr : Aggregation operator for the incoming messages (e.g.  + ,  * ,  max ,  min , and  mean ). σ : Activation function. init_weight : Weights' initializer. Default  glorot_uniform . init_bias : Bias initializer. Default  zeros32 . use_bias : Add learnable bias. Default  true . Examples: using GNNLux, Lux, Random\n\n# initialize random number generator\nrng = Random.default_rng()\n\n# create data\nn_in = 3\nn_in_edge = 10\nn_out = 5\n\ns = [1,1,2,3]\nt = [2,3,1,1]\ng = GNNGraph(s, t)\nx = randn(rng, Float32, n_in, g.num_nodes)\ne = randn(rng, Float32, n_in_edge, g.num_edges)\n\n# create dense layer\nnn = Dense(n_in_edge => n_out * n_in)\n\n# create layer\nl = NNConv(n_in => n_out, nn, tanh, use_bias = true, aggr = +)\n\n# setup layer\nps, st = LuxCore.setup(rng, l)\n\n# forward pass\ny, st = l(g, x, e, ps, st)       # size:  n_out × num_nodes  source"},{"id":483,"pagetitle":"Convolutional layers","title":"GNNLux.ResGatedGraphConv","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/api/conv/#GNNLux.ResGatedGraphConv","content":" GNNLux.ResGatedGraphConv  —  Type ResGatedGraphConv(in => out, act=identity; init_weight = glorot_uniform, init_bias = zeros32, use_bias = true) The residual gated graph convolutional operator from the  Residual Gated Graph ConvNets  paper. The layer's forward pass is given by \\[\\mathbf{x}_i' = act\\big(U\\mathbf{x}_i + \\sum_{j \\in N(i)} \\eta_{ij} V \\mathbf{x}_j\\big),\\] where the edge gates  $\\eta_{ij}$  are given by \\[\\eta_{ij} = sigmoid(A\\mathbf{x}_i + B\\mathbf{x}_j).\\] Arguments in : The dimension of input features. out : The dimension of output features. act : Activation function. init_weight : Weights' initializer. Default  glorot_uniform . init_bias : Bias initializer. Default  zeros32 . use_bias : Add learnable bias. Default  true . Examples: using GNNLux, Lux, Random\n\n# initialize random number generator\nrng = Random.default_rng()\n\n# create data\ns = [1,1,2,3]\nt = [2,3,1,1]\nin_channel = 3\nout_channel = 5\ng = GNNGraph(s, t)\nx = randn(rng, Float32, in_channel, g.num_nodes)\n\n# create layer\nl = ResGatedGraphConv(in_channel => out_channel, tanh, use_bias = true)\n\n# setup layer\nps, st = LuxCore.setup(rng, l)\n\n# forward pass\ny, st = l(g, x, ps, st)       # size:  out_channel × num_nodes   source"},{"id":484,"pagetitle":"Convolutional layers","title":"GNNLux.SAGEConv","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/api/conv/#GNNLux.SAGEConv","content":" GNNLux.SAGEConv  —  Type SAGEConv(in => out, σ=identity; aggr=mean, init_weight = glorot_uniform, init_bias = zeros32, use_bias=true) GraphSAGE convolution layer from paper  Inductive Representation Learning on Large Graphs . Performs: \\[\\mathbf{x}_i' = W \\cdot [\\mathbf{x}_i; \\square_{j \\in \\mathcal{N}(i)} \\mathbf{x}_j]\\] where the aggregation type is selected by  aggr . Arguments in : The dimension of input features. out : The dimension of output features. σ : Activation function. aggr : Aggregation operator for the incoming messages (e.g.  + ,  * ,  max ,  min , and  mean ). init_bias : Bias initializer. Default  zeros32 . use_bias : Add learnable bias. Default  true . Examples: using GNNLux, Lux, Random\n\n# initialize random number generator\nrng = Random.default_rng()\n\n# create data\ns = [1,1,2,3]\nt = [2,3,1,1]\nin_channel = 3\nout_channel = 5\ng = GNNGraph(s, t)\nx = rand(rng, Float32, in_channel, g.num_nodes)\n\n# create layer\nl = SAGEConv(in_channel => out_channel, tanh, use_bias = false, aggr = +)\n\n# setup layer\nps, st = LuxCore.setup(rng, l)\n\n# forward pass\ny, st = l(g, x, ps, st)       # size:  out_channel × num_nodes    source"},{"id":485,"pagetitle":"Convolutional layers","title":"GNNLux.SGConv","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/api/conv/#GNNLux.SGConv","content":" GNNLux.SGConv  —  Type SGConv(int => out, k = 1; init_weight = glorot_uniform, init_bias = zeros32, use_bias = true, add_self_loops = true,use_edge_weight = false) SGC layer from  Simplifying Graph Convolutional Networks  Performs operation \\[H^{K} = (\\tilde{D}^{-1/2} \\tilde{A} \\tilde{D}^{-1/2})^K X \\Theta\\] where  $\\tilde{A}$  is  $A + I$ . Arguments in : Number of input features. out : Number of output features. k  : Number of hops k. Default  1 . add_self_loops : Add self loops to the graph before performing the convolution. Default  false . use_edge_weight : If  true , consider the edge weights in the input graph (if available).                    If  add_self_loops=true  the new weights will be set to 1. Default  false . init_weight : Weights' initializer. Default  glorot_uniform . init_bias : Bias initializer. Default  zeros32 . use_bias : Add learnable bias. Default  true . Examples using GNNLux, Lux, Random\n\n# initialize random number generator\nrng = Random.default_rng()\n\n# create data\ns = [1,1,2,3]\nt = [2,3,1,1]\ng = GNNGraph(s, t)\nx = randn(rng, Float32, 3, g.num_nodes)\n\n# create layer\nl = SGConv(3 => 5; add_self_loops = true) \n\n# setup layer\nps, st = LuxCore.setup(rng, l)\n\n# forward pass\ny, st = l(g, x, ps, st)       # size:  5 × num_nodes\n\n# convolution with edge weights\nw = [1.1, 0.1, 2.3, 0.5]\ny = l(g, x, w, ps, st)\n\n# Edge weights can also be embedded in the graph.\ng = GNNGraph(s, t, w)\nl = SGConv(3 => 5, add_self_loops = true, use_edge_weight=true) \nps, st = LuxCore.setup(rng, l)\ny, st = l(g, x, ps, st) # same as l(g, x, w)  source"},{"id":488,"pagetitle":"Temporal Convolutional layers","title":"Temporal Graph-Convolutional Layers","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/api/temporalconv/#Temporal-Graph-Convolutional-Layers","content":" Temporal Graph-Convolutional Layers Convolutions for time-varying graphs (temporal graphs) such as the  TemporalSnapshotsGNNGraph ."},{"id":489,"pagetitle":"Temporal Convolutional layers","title":"GNNLux.A3TGCN","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/api/temporalconv/#GNNLux.A3TGCN","content":" GNNLux.A3TGCN  —  Type A3TGCN(in => out; use_bias = true, init_weight = glorot_uniform, init_state = zeros32, init_bias = zeros32, add_self_loops = false, use_edge_weight = true) Attention Temporal Graph Convolutional Network (A3T-GCN) model from the paper  A3T-GCN: Attention Temporal Graph Convolutional Network for Traffic Forecasting . Performs a TGCN layer, followed by a soft attention layer. Arguments in : Number of input features. out : Number of output features. use_bias : Add learnable bias. Default  true . init_weight : Weights' initializer. Default  glorot_uniform . init_state : Initial state of the hidden stat of the GRU layer. Default  zeros32 . init_bias : Bias initializer. Default  zeros32 . add_self_loops : Add self loops to the graph before performing the convolution. Default  false . use_edge_weight : If  true , consider the edge weights in the input graph (if available).                    If  add_self_loops=true  the new weights will be set to 1.                     This option is ignored if the  edge_weight  is explicitly provided in the forward pass.                    Default  false . Examples using GNNLux, Lux, Random\n\n# initialize random number generator\nrng = Random.default_rng()\n\n# create data\ng = rand_graph(rng, 5, 10)\nx = rand(rng, Float32, 2, 5)\n\n# create A3TGCN layer\nl = A3TGCN(2 => 6)\n\n# setup layer\nps, st = LuxCore.setup(rng, l)\n\n# forward pass\ny, st = l(g, x, ps, st)      # result size (6, 5) source"},{"id":490,"pagetitle":"Temporal Convolutional layers","title":"GNNLux.EvolveGCNO","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/api/temporalconv/#GNNLux.EvolveGCNO","content":" GNNLux.EvolveGCNO  —  Type EvolveGCNO(ch; use_bias = true, init_weight = glorot_uniform, init_state = zeros32, init_bias = zeros32) Evolving Graph Convolutional Network (EvolveGCNO) layer from the paper  EvolveGCN: Evolving Graph Convolutional Networks for Dynamic Graphs . Perfoms a Graph Convolutional layer with parameters derived from a Long Short-Term Memory (LSTM) layer across the snapshots of the temporal graph. Arguments in : Number of input features. out : Number of output features. use_bias : Add learnable bias. Default  true . init_weight : Weights' initializer. Default  glorot_uniform . init_state : Initial state of the hidden stat of the GRU layer. Default  zeros32 . init_bias : Bias initializer. Default  zeros32 . Examples using GNNLux, Lux, Random\n\n# initialize random number generator\nrng = Random.default_rng()\n\n# create data\ntg = TemporalSnapshotsGNNGraph([rand_graph(rng, 10, 20; ndata = rand(rng, 4, 10)), rand_graph(rng, 10, 14; ndata = rand(rng, 4, 10)), rand_graph(rng, 10, 22; ndata = rand(rng, 4, 10))])\n\n# create layer\nl = EvolveGCNO(4 => 5)\n\n# setup layer\nps, st = LuxCore.setup(rng, l)\n\n# forward pass\ny, st = l(tg, tg.ndata.x , ps, st)      # result size 3, size y[1] (5, 10) source"},{"id":491,"pagetitle":"Temporal Convolutional layers","title":"GNNLux.DCGRU","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/api/temporalconv/#GNNLux.DCGRU-Tuple{Pair{Int64, Int64}, Int64}","content":" GNNLux.DCGRU  —  Method DCGRU(in => out, k; use_bias = true, init_weight = glorot_uniform, init_state = zeros32, init_bias = zeros32) Diffusion Convolutional Recurrent Neural Network (DCGRU) layer from the paper  Diffusion Convolutional Recurrent Neural Network: Data-driven Traffic Forecasting . Performs a Diffusion Convolutional layer to model spatial dependencies, followed by a Gated Recurrent Unit (GRU) cell to model temporal dependencies. Arguments in : Number of input features. out : Number of output features. k : Diffusion step. use_bias : Add learnable bias. Default  true . init_weight : Weights' initializer. Default  glorot_uniform . init_state : Initial state of the hidden stat of the GRU layer. Default  zeros32 . init_bias : Bias initializer. Default  zeros32 . Examples using GNNLux, Lux, Random\n\n# initialize random number generator\nrng = Random.default_rng()\n\n# create data\ng = rand_graph(rng, 5, 10)\nx = rand(rng, Float32, 2, 5)\n\n# create layer\nl = DCGRU(2 => 5, 2)\n\n# setup layer\nps, st = LuxCore.setup(rng, l)\n\n# forward pass\ny, st = l(g, x, ps, st)      # result size (5, 5) source"},{"id":492,"pagetitle":"Temporal Convolutional layers","title":"GNNLux.GConvGRU","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/api/temporalconv/#GNNLux.GConvGRU-Tuple{Pair{Int64, Int64}, Int64}","content":" GNNLux.GConvGRU  —  Method GConvGRU(in => out, k; use_bias = true, init_weight = glorot_uniform, init_state = zeros32, init_bias = zeros32) Graph Convolutional Gated Recurrent Unit (GConvGRU) recurrent layer from the paper  Structured Sequence Modeling with Graph Convolutional Recurrent Networks . Performs a layer of ChebConv to model spatial dependencies, followed by a Gated Recurrent Unit (GRU) cell to model temporal dependencies. Arguments in : Number of input features. out : Number of output features. k : Chebyshev polynomial order. use_bias : Add learnable bias. Default  true . init_weight : Weights' initializer. Default  glorot_uniform . init_state : Initial state of the hidden stat of the GRU layer. Default  zeros32 . init_bias : Bias initializer. Default  zeros32 . Examples using GNNLux, Lux, Random\n\n# initialize random number generator\nrng = Random.default_rng()\n\n# create data\ng = rand_graph(rng, 5, 10)\nx = rand(rng, Float32, 2, 5)\n\n# create layer\nl = GConvGRU(2 => 5, 2)\n\n# setup layer\nps, st = LuxCore.setup(rng, l)\n\n# forward pass\ny, st = l(g, x, ps, st)      # result size (5, 5) source"},{"id":493,"pagetitle":"Temporal Convolutional layers","title":"GNNLux.GConvLSTM","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/api/temporalconv/#GNNLux.GConvLSTM-Tuple{Pair{Int64, Int64}, Int64}","content":" GNNLux.GConvLSTM  —  Method GConvLSTM(in => out, k; use_bias = true, init_weight = glorot_uniform, init_state = zeros32, init_bias = zeros32) Graph Convolutional Long Short-Term Memory (GConvLSTM) recurrent layer from the paper  Structured Sequence Modeling with Graph Convolutional Recurrent Networks .  Performs a layer of ChebConv to model spatial dependencies, followed by a Long Short-Term Memory (LSTM) cell to model temporal dependencies. Arguments in : Number of input features. out : Number of output features. k : Chebyshev polynomial order. use_bias : Add learnable bias. Default  true . init_weight : Weights' initializer. Default  glorot_uniform . init_state : Initial state of the hidden stat of the GRU layer. Default  zeros32 . init_bias : Bias initializer. Default  zeros32 . Examples using GNNLux, Lux, Random\n\n# initialize random number generator\nrng = Random.default_rng()\n\n# create data\ng = rand_graph(rng, 5, 10)\nx = rand(rng, Float32, 2, 5)\n\n# create GConvLSTM layer\nl = GConvLSTM(2 => 5, 2)\n\n# setup layer\nps, st = LuxCore.setup(rng, l)\n\n# forward pass\ny, st = l(g, x, ps, st)      # result size (5, 5) source"},{"id":494,"pagetitle":"Temporal Convolutional layers","title":"GNNLux.TGCN","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/api/temporalconv/#GNNLux.TGCN-Tuple{Pair{Int64, Int64}}","content":" GNNLux.TGCN  —  Method TGCN(in => out; use_bias = true, init_weight = glorot_uniform, init_state = zeros32, init_bias = zeros32, add_self_loops = false, use_edge_weight = true) Temporal Graph Convolutional Network (T-GCN) recurrent layer from the paper  T-GCN: A Temporal Graph Convolutional Network for Traffic Prediction . Performs a layer of GCNConv to model spatial dependencies, followed by a Gated Recurrent Unit (GRU) cell to model temporal dependencies. Arguments in : Number of input features. out : Number of output features. use_bias : Add learnable bias. Default  true . init_weight : Weights' initializer. Default  glorot_uniform . init_state : Initial state of the hidden stat of the GRU layer. Default  zeros32 . init_bias : Bias initializer. Default  zeros32 . add_self_loops : Add self loops to the graph before performing the convolution. Default  false . use_edge_weight : If  true , consider the edge weights in the input graph (if available).                    If  add_self_loops=true  the new weights will be set to 1.                     This option is ignored if the  edge_weight  is explicitly provided in the forward pass.                    Default  false . Examples using GNNLux, Lux, Random\n\n# initialize random number generator\nrng = Random.default_rng()\n\n# create data\ng = rand_graph(rng, 5, 10)\nx = rand(rng, Float32, 2, 5)\n\n# create TGCN layer\ntgcn = TGCN(2 => 6)\n\n# setup layer\nps, st = LuxCore.setup(rng, tgcn)\n\n# forward pass\ny, st = tgcn(g, x, ps, st)      # result size (6, 5) source"},{"id":497,"pagetitle":"Models","title":"Models","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/guides/models/#Models","content":" Models GNNLux.jl provides common graph convolutional layers by which you can assemble arbitrarily deep or complex models. GNN layers are compatible with  Lux.jl ones, therefore expert Lux users are promptly able to define and train  their models.  In what follows, we discuss two different styles for model creation: the  explicit modeling  style, more verbose but more flexible,  and the  implicit modeling  style based on  GNNLux.GNNChain , more concise but less flexible."},{"id":498,"pagetitle":"Models","title":"Explicit modeling","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/guides/models/#Explicit-modeling","content":" Explicit modeling In the explicit modeling style, the model is created according to the following steps: Define a new type for your model ( GNN  in the example below). Refer to the Lux Manual  for the  definition of the type. Define a convenience constructor for your model. Define the forward pass by implementing the call method for your type. Instantiate the model.  Here is an example of this construction: using Lux, GNNLux\nusing Zygote\nusing Random, Statistics\n\nstruct GNN <: AbstractLuxContainerLayer{(:conv1, :bn, :conv2, :dropout, :dense)} # step 1\n    conv1\n    bn\n    conv2\n    dropout\n    dense\nend\n\nfunction GNN(din::Int, d::Int, dout::Int) # step 2\n    GNN(GraphConv(din => d),\n        BatchNorm(d),\n        GraphConv(d => d, relu),\n        Dropout(0.5),\n        Dense(d, dout))\nend\n\nfunction (model::GNN)(g::GNNGraph, x, ps, st) # step 3\n    x, st_conv1 = model.conv1(g, x, ps.conv1, st.conv1)\n    x, st_bn = model.bn(x, ps.bn, st.bn)\n    x = relu.(x)\n    x, st_conv2 = model.conv2(g, x, ps.conv2, st.conv2)\n    x, st_drop = model.dropout(x, ps.dropout, st.dropout)\n    x, st_dense = model.dense(x, ps.dense, st.dense)\n    return x, (conv1=st_conv1, bn=st_bn, conv2=st_conv2, dropout=st_drop, dense=st_dense)\nend\n\ndin, d, dout = 3, 4, 2 \nmodel = GNN(din, d, dout)                 # step 4\nrng = Random.default_rng()\nps, st = Lux.setup(rng, model)\ng = rand_graph(rng, 10, 30)\nX = randn(Float32, din, 10) \n\nst = Lux.testmode(st)\ny, st = model(g, X, ps, st) \nst = Lux.trainmode(st)\ngrad = Zygote.gradient(ps -> mean(model(g, X, ps, st)[1]), ps)[1]"},{"id":499,"pagetitle":"Models","title":"Implicit modeling with GNNChains","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/guides/models/#Implicit-modeling-with-GNNChains","content":" Implicit modeling with GNNChains While very flexible, the way in which we defined  GNN  model definition in last section is a bit verbose. In order to simplify things, we provide the  GNNLux.GNNChain  type. It is very similar  to Lux's well known  Chain . It allows to compose layers in a sequential fashion as Chain does, propagating the output of each layer to the next one. In addition,  GNNChain    propagates the input graph as well, providing it as a first argument to layers subtyping the  GNNLux.GNNLayer  abstract type.  Using  GNNChain , the model definition becomes more concise: model = GNNChain(GraphConv(din => d),\n                 BatchNorm(d),\n                 x -> relu.(x),\n                 GraphConv(d => d, relu),\n                 Dropout(0.5),\n                 Dense(d, dout)) The  GNNChain  only propagates the graph and the node features. More complex scenarios, e.g. when also edge features are updated, have to be handled using the explicit definition of the forward pass. "},{"id":502,"pagetitle":"Hands on","title":"Hands-on introduction to Graph Neural Networks","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/tutorials/gnn_intro/#Hands-on-introduction-to-Graph-Neural-Networks","content":" Hands-on introduction to Graph Neural Networks This Pluto notebook is a Julia adaptation of the Pytorch Geometric tutorials that can be found  here . Recently, deep learning on graphs has emerged to be one of the hottest research fields in the deep learning community. Here,  Graph Neural Networks (GNNs)  aim to generalize classical deep learning concepts to irregular structured data (in contrast to images or texts) and to enable neural networks to reason about objects and their relations. This is done by following a simple  neural message passing scheme , where node features  $\\mathbf{x}_i^{(\\ell)}$  of all nodes  $i \\in \\mathcal{V}$  in a graph  $\\mathcal{G} = (\\mathcal{V}, \\mathcal{E})$  are iteratively updated by aggregating localized information from their neighbors  $\\mathcal{N}(i)$ : \\[\\mathbf{x}_i^{(\\ell + 1)} = f^{(\\ell + 1)}_{\\theta} \\left( \\mathbf{x}_i^{(\\ell)}, \\left\\{ \\mathbf{x}_j^{(\\ell)} : j \\in \\mathcal{N}(i) \\right\\} \\right)\\] This tutorial will introduce you to some fundamental concepts regarding deep learning on graphs via Graph Neural Networks based on the  GNNLux.jl library . GNNLux.jl is an extension library to the deep learning framework  Lux.jl , and consists of various methods and utilities to ease the implementation of Graph Neural Networks. Let's first import the packages we need: using Lux, GNNLux\nusing MLDatasets\nusing LinearAlgebra, Random, Statistics\nimport GraphMakie\nimport CairoMakie as Makie\nusing Zygote, Optimisers, OneHotArrays\n\nENV[\"DATADEPS_ALWAYS_ACCEPT\"] = \"true\"  # don't ask for dataset download confirmation\nrng = Random.seed!(10) # for reproducibility TaskLocalRNG() Following  Kipf et al. (2017) , let's dive into the world of GNNs by looking at a simple graph-structured example, the well-known  Zachary's karate club network . This graph describes a social network of 34 members of a karate club and documents links between members who interacted outside the club. Here, we are interested in detecting communities that arise from the member's interaction. GNNLux.jl provides utilities to convert  MLDatasets.jl 's datasets to its own type: dataset = MLDatasets.KarateClub() dataset KarateClub:\n  metadata  =>    Dict{String, Any} with 0 entries\n  graphs    =>    1-element Vector{MLDatasets.Graph} After initializing the  KarateClub  dataset, we first can inspect some of its properties. For example, we can see that this dataset holds exactly  one graph . Furthermore, the graph holds exactly  4 classes , which represent the community each node belongs to. karate = dataset[1]\n\nkarate.node_data.labels_comm 34-element Vector{Int64}:\n 1\n 1\n 1\n 1\n 3\n 3\n 3\n 1\n 0\n 1\n 3\n 1\n 1\n 1\n 0\n 0\n 3\n 1\n 0\n 1\n 0\n 1\n 0\n 0\n 2\n 2\n 0\n 0\n 2\n 0\n 0\n 2\n 0\n 0 Now we convert the single-graph dataset to a  GNNGraph . Moreover, we add a an array of node features, a  34-dimensional feature vector   for each node which uniquely describes the members of the karate club. We also add a training mask selecting the nodes to be used for training in our semi-supervised node classification task. g = mldataset2gnngraph(dataset)\n\nx = zeros(Float32, g.num_nodes, g.num_nodes)\nx[diagind(x)] .= 1\n\ntrain_mask = [true, false, false, false, true, false, false, false, true,\n    false, false, false, false, false, false, false, false, false, false, false,\n    false, false, false, false, true, false, false, false, false, false,\n    false, false, false, false]\n\nlabels = g.ndata.labels_comm\ny = onehotbatch(labels, 0:3)\n\ng = GNNGraph(g, ndata = (; x, y, train_mask)) GNNGraph:\n  num_nodes: 34\n  num_edges: 156\n  ndata:\n\ty = 4×34 OneHotMatrix(::Vector{UInt32}) with eltype Bool\n\ttrain_mask = 34-element Vector{Bool}\n\tx = 34×34 Matrix{Float32} Let's now look at the underlying graph in more detail: println(\"Number of nodes: $(g.num_nodes)\")\nprintln(\"Number of edges: $(g.num_edges)\")\nprintln(\"Average node degree: $(g.num_edges / g.num_nodes)\")\nprintln(\"Number of training nodes: $(sum(g.ndata.train_mask))\")\nprintln(\"Training node label rate: $(mean(g.ndata.train_mask))\")\nprintln(\"Has self-loops: $(has_self_loops(g))\")\nprintln(\"Is undirected: $(is_bidirected(g))\") Number of nodes: 34\nNumber of edges: 156\nAverage node degree: 4.588235294117647\nNumber of training nodes: 4\nTraining node label rate: 0.11764705882352941\nHas self-loops: false\nIs undirected: true\n Each graph in GNNLux.jl is represented by a   GNNGraph  object, which holds all the information to describe its graph representation. We can print the data object anytime via  print(g)  to receive a short summary about its attributes and their shapes. The   g  object holds 3 attributes: g.ndata : contains node-related information. g.edata : holds edge-related information. g.gdata : this stores the global data, therefore neither node nor edge-specific features. These attributes are  NamedTuples  that can store multiple feature arrays: we can access a specific set of features e.g.  x , with  g.ndata.x . In our task,  g.ndata.train_mask  describes for which nodes we already know their community assignments. In total, we are only aware of the ground-truth labels of 4 nodes (one for each community), and the task is to infer the community assignment for the remaining nodes. The  g  object also provides some  utility functions  to infer some basic properties of the underlying graph. For example, we can easily infer whether there exist isolated nodes in the graph ( i.e.  there exists no edge to any node), whether the graph contains self-loops ( i.e. ,  $(v, v) \\in \\mathcal{E}$ ), or whether the graph is bidirected ( i.e. , for each edge  $(v, w) \\in \\mathcal{E}$  there also exists the edge  $(w, v) \\in \\mathcal{E}$ ). Let us now inspect the  edge_index  method: edge_index(g) ([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 6, 6, 6, 6, 7, 7, 7, 7, 8, 8, 8, 8, 9, 9, 9, 9, 9, 10, 10, 11, 11, 11, 12, 13, 13, 14, 14, 14, 14, 14, 15, 15, 16, 16, 17, 17, 18, 18, 19, 19, 20, 20, 20, 21, 21, 22, 22, 23, 23, 24, 24, 24, 24, 24, 25, 25, 25, 26, 26, 26, 27, 27, 28, 28, 28, 28, 29, 29, 29, 30, 30, 30, 30, 31, 31, 31, 31, 32, 32, 32, 32, 32, 32, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34], [2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 18, 20, 22, 32, 1, 3, 4, 8, 14, 18, 20, 22, 31, 1, 2, 4, 8, 9, 10, 14, 28, 29, 33, 1, 2, 3, 8, 13, 14, 1, 7, 11, 1, 7, 11, 17, 1, 5, 6, 17, 1, 2, 3, 4, 1, 3, 31, 33, 34, 3, 34, 1, 5, 6, 1, 1, 4, 1, 2, 3, 4, 34, 33, 34, 33, 34, 6, 7, 1, 2, 33, 34, 1, 2, 34, 33, 34, 1, 2, 33, 34, 26, 28, 30, 33, 34, 26, 28, 32, 24, 25, 32, 30, 34, 3, 24, 25, 34, 3, 32, 34, 24, 27, 33, 34, 2, 9, 33, 34, 1, 25, 26, 29, 33, 34, 3, 9, 15, 16, 19, 21, 23, 24, 30, 31, 32, 34, 9, 10, 14, 15, 16, 19, 20, 21, 23, 24, 27, 28, 29, 30, 31, 32, 33]) By printing  edge_index(g) , we can understand how GNNGraphs.jl represents graph connectivity internally. We can see that for each edge,  edge_index  holds a tuple of two node indices, where the first value describes the node index of the source node and the second value describes the node index of the destination node of an edge. This representation is known as the  COO format (coordinate format)  commonly used for representing sparse matrices. Instead of holding the adjacency information in a dense representation  $\\mathbf{A} \\in \\{ 0, 1 \\}^{|\\mathcal{V}| \\times |\\mathcal{V}|}$ , GNNGraphs.jl represents graphs sparsely, which refers to only holding the coordinates/values for which entries in  $\\mathbf{A}$  are non-zero. Importantly, GNNGraphs.jl does not distinguish between directed and undirected graphs, and treats undirected graphs as a special case of directed graphs in which reverse edges exist for every entry in the  edge_index . Since a  GNNGraph  is an  AbstractGraph  from the  Graphs.jl  library, it supports graph algorithms and visualization tools from the wider julia graph ecosystem: GraphMakie.graphplot(g |> to_unidirected, node_size = 20, node_color = labels, arrow_show = false)"},{"id":503,"pagetitle":"Hands on","title":"Implementing Graph Neural Networks","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/tutorials/gnn_intro/#Implementing-Graph-Neural-Networks","content":" Implementing Graph Neural Networks After learning about GNNGraphs.jl's data handling, it's time to implement our first Graph Neural Network! For this, we will use on of the most simple GNN operators, the  GCN layer  ( Kipf et al. (2017) ), which is defined as \\[\\mathbf{x}_v^{(\\ell + 1)} = \\mathbf{W}^{(\\ell + 1)} \\sum_{w \\in \\mathcal{N}(v) \\, \\cup \\, \\{ v \\}} \\frac{1}{c_{w,v}} \\cdot \\mathbf{x}_w^{(\\ell)}\\] where  $\\mathbf{W}^{(\\ell + 1)}$  denotes a trainable weight matrix of shape  [num_output_features, num_input_features]  and  $c_{w,v}$  refers to a fixed normalization coefficient for each edge. GNNLux.jl implements this layer via  GCNConv , which can be executed by passing in the node feature representation  x  and the COO graph connectivity representation  edge_index . With this, we are ready to create our first Graph Neural Network by defining our network architecture: Lux.@concrete struct GCN <: GNNContainerLayer{(:conv1, :conv2, :conv3, :dense)}\n    nf::Int\n    nc::Int\n    hd1::Int\n    hd2::Int\n    conv1\n    conv2\n    conv3\n    dense\n    use_bias::Bool\n    init_weight\n    init_bias\nend\n\nfunction GCN(num_features, num_classes, hidden_dim1, hidden_dim2; use_bias = true, init_weight = glorot_uniform, init_bias = zeros32) # constructor\n    conv1 = GCNConv(num_features => hidden_dim1)\n    conv2 = GCNConv(hidden_dim1 => hidden_dim1)\n    conv3 = GCNConv(hidden_dim1 => hidden_dim2)\n    dense = Dense(hidden_dim2, num_classes)\n    return GCN(num_features, num_classes, hidden_dim1, hidden_dim2, conv1, conv2, conv3, dense, use_bias, init_weight, init_bias)\nend\n\nfunction (gcn::GCN)(g::GNNGraph, x, ps, st) # forward pass\n    dense = StatefulLuxLayer{true}(gcn.dense, ps.dense, GNNLux._getstate(st, :dense))\n    x, stconv1 = gcn.conv1(g, x, ps.conv1, st.conv1)\n    x = tanh.(x)\n    x, stconv2 = gcn.conv2(g, x, ps.conv2, st.conv2)\n    x = tanh.(x)\n    x, stconv3 = gcn.conv3(g, x, ps.conv3, st.conv3)\n    x = tanh.(x)\n    out = dense(x)\n    return (out, x), (conv1 = stconv1, conv2 = stconv2, conv3 = stconv3)\nend\n\n\nfunction LuxCore.initialparameters(rng::TaskLocalRNG, l::GCN) # initialize model parameters\n    weight_c1 = l.init_weight(rng, l.hd1, l.nf)\n    weight_c2 = l.init_weight(rng, l.hd1, l.hd1)\n    weight_c3 = l.init_weight(rng, l.hd2, l.hd1)\n    weight_d = l.init_weight(rng, l.nc, l.hd2)\n    if l.use_bias\n        bias_c1 = l.init_bias(rng, l.hd1)\n        bias_c2 = l.init_bias(rng, l.hd1)\n        bias_c3 = l.init_bias(rng, l.hd2)\n        bias_d = l.init_bias(rng, l.nc)\n        return (; conv1 = ( weight = weight_c1, bias = bias_c1), conv2 = ( weight = weight_c2, bias = bias_c2), conv3 = ( weight = weight_c3, bias = bias_c3), dense = ( weight = weight_d,bias =  bias_d))\n    end\n    return (; conv1 = ( weight = weight_c1), conv2 = ( weight = weight_c2), conv3 = ( weight = weight_c3), dense = ( weight_d))\nend Here, we first initialize all of our building blocks in the constructor and define the computation flow of our network in the call method. We first define and stack  three graph convolution layers , which corresponds to aggregating 3-hop neighborhood information around each node (all nodes up to 3 \"hops\" away). In addition, the  GCNConv  layers reduce the node feature dimensionality to  $2$ ,  i.e. ,  $34 \\rightarrow 4 \\rightarrow 4 \\rightarrow 2$ . Each  GCNConv  layer is enhanced by a  tanh  non-linearity. After that, we apply a single linear transformation ( Lux.Dense  that acts as a classifier to map our nodes to 1 out of the 4 classes/communities. We return both the output of the final classifier as well as the final node embeddings produced by our GNN. num_features = 34\nnum_classes = 4\nhidden_dim1 = 4\nhidden_dim2 = 2\n\ngcn = GCN(num_features, num_classes, hidden_dim1, hidden_dim2)\nps, st = LuxCore.setup(rng, gcn)\n\n(ŷ, emb_init), st = gcn(g, g.x, ps, st)\n\nfunction visualize_embeddings(h; colors = nothing)\n    xs = h[1, :] |> vec\n    ys = h[2, :] |> vec\n    Makie.scatter(xs, ys, color = labels, markersize = 20)\nend\n\nvisualize_embeddings(emb_init, colors = labels) Remarkably, even before training the weights of our model, the model produces an embedding of nodes that closely resembles the community-structure of the graph. Nodes of the same color (community) are already closely clustered together in the embedding space, although the weights of our model are initialized  completely at random  and we have not yet performed any training so far! This leads to the conclusion that GNNs introduce a strong inductive bias, leading to similar embeddings for nodes that are close to each other in the input graph."},{"id":504,"pagetitle":"Hands on","title":"Training on the Karate Club Network","ref":"/GraphNeuralNetworks.jl/docs/GNNLux.jl/stable/tutorials/gnn_intro/#Training-on-the-Karate-Club-Network","content":" Training on the Karate Club Network But can we do better? Let's look at an example on how to train our network parameters based on the knowledge of the community assignments of 4 nodes in the graph (one for each community). Since everything in our model is differentiable and parameterized, we can add some labels, train the model and observe how the embeddings react. Here, we make use of a semi-supervised or transductive learning procedure: we simply train against one node per class, but are allowed to make use of the complete input graph data. Training our model is very similar to any other Lux model. In addition to defining our network architecture, we define a loss criterion (here,  logitcrossentropy ), and initialize a stochastic gradient optimizer (here,  Adam ). After that, we perform multiple rounds of optimization, where each round consists of a forward and backward pass to compute the gradients of our model parameters w.r.t. to the loss derived from the forward pass. If you are not new to Lux, this scheme should appear familiar to you. Note that our semi-supervised learning scenario is achieved by the following line: logitcrossentropy(ŷ[:,train_mask], y[:,train_mask]) While we compute node embeddings for all of our nodes, we  only make use of the training nodes for computing the loss . Here, this is implemented by filtering the output of the classifier  out  and ground-truth labels  data.y  to only contain the nodes in the  train_mask . Let us now start training and see how our node embeddings evolve over time (best experienced by explicitly running the code): function custom_loss(gcn, ps, st, tuple)\n    g, x, y = tuple\n    logitcrossentropy = CrossEntropyLoss(; logits=Val(true))\n    (ŷ, _) ,st = gcn(g, x, ps, st)\n    return  logitcrossentropy(ŷ[:, train_mask], y[:, train_mask]), (st), 0\nend\n\nfunction train_model!(gcn, ps, st, g)\n    train_state = Lux.Training.TrainState(gcn, ps, st, Adam(1e-2))\n    for iter in 1:2000\n            _, loss, _, train_state = Lux.Training.single_train_step!(AutoZygote(), custom_loss,(g, g.x, g.y), train_state)\n\n        if iter % 100 == 0\n            println(\"Epoch: $(iter) Loss: $(loss)\")\n        end\n    end\n\n    return gcn, ps, st\nend\n\ngcn, ps, st = train_model!(gcn, ps, st, g); Epoch: 100 Loss: 0.21202108\nEpoch: 200 Loss: 0.06440282\nEpoch: 300 Loss: 0.03517379\nEpoch: 400 Loss: 0.022871211\nEpoch: 500 Loss: 0.016305141\nEpoch: 600 Loss: 0.012315823\nEpoch: 700 Loss: 0.009680824\nEpoch: 800 Loss: 0.007835168\nEpoch: 900 Loss: 0.006484421\nEpoch: 1000 Loss: 0.0054619424\nEpoch: 1100 Loss: 0.004666665\nEpoch: 1200 Loss: 0.004034364\nEpoch: 1300 Loss: 0.0035222156\nEpoch: 1400 Loss: 0.003100942\nEpoch: 1500 Loss: 0.0027497704\nEpoch: 1600 Loss: 0.002453527\nEpoch: 1700 Loss: 0.0022013204\nEpoch: 1800 Loss: 0.0019845003\nEpoch: 1900 Loss: 0.0017966953\nEpoch: 2000 Loss: 0.0016328939\n Train accuracy: (ŷ, emb_final), st = gcn(g, g.x, ps, st)\nmean(onecold(ŷ[:, train_mask]) .== onecold(g.y[:, train_mask])) 1.0 Test accuracy: mean(onecold(ŷ[:, .!train_mask]) .== onecold(y[:, .!train_mask])) 0.7333333333333333 Final embedding: visualize_embeddings(emb_final, colors = labels) As one can see, our 3-layer GCN model manages to linearly separating the communities and classifying most of the nodes correctly. Furthermore, we did this all with a few lines of code, thanks to the GNNLux.jl which helped us out with data handling and GNN implementations. This page was generated using  Literate.jl ."},{"id":507,"pagetitle":"Home","title":"GNNGraphs.jl","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/#GNNGraphs.jl","content":" GNNGraphs.jl GNNGraphs.jl is a package that provides graph data structures and helper functions specifically designed for working with graph neural networks. This package allows to store not only the graph structure, but also features associated with nodes, edges, and the graph itself. It is the core foundation for the GNNlib.jl, GraphNeuralNetworks.jl, and GNNLux.jl packages. It supports three types of graphs:  Static graph  is the basic graph type represented by  GNNGraph , where each node and edge can have associated features. This type of graph is used in typical graph neural network applications, where neural networks operate on both the structure of the graph and the features stored in it. It can be used to represent a graph where the structure does not change over time, but the features of the nodes and edges can change over time. Heterogeneous graph  is a graph that supports multiple types of nodes and edges, and is represented by  GNNHeteroGraph . Each type can have its own properties and relationships. This is useful in scenarios with different entities and interactions, such as in citation graphs or multi-relational data. Temporal graph  is a graph that changes over time, and is represented by  TemporalSnapshotsGNNGraph . Edges and features can change dynamically. This type of graph is useful for applications that involve tracking time-dependent relationships, such as social networks. This package depends on the package  Graphs.jl ."},{"id":508,"pagetitle":"Home","title":"Installation","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/#Installation","content":" Installation The package can be installed with the Julia package manager. From the Julia REPL, type  ]  to enter the Pkg REPL mode and run: pkg> add GNNGraphs"},{"id":511,"pagetitle":"Datasets","title":"Datasets","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/datasets/#Datasets","content":" Datasets"},{"id":512,"pagetitle":"Datasets","title":"GNNGraphs.mldataset2gnngraph","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/datasets/#GNNGraphs.mldataset2gnngraph","content":" GNNGraphs.mldataset2gnngraph  —  Function mldataset2gnngraph(dataset) Convert a graph dataset from the package MLDatasets.jl into one or many  GNNGraph s. Examples julia> using MLDatasets, GNNGraphs\n\njulia> mldataset2gnngraph(Cora())\nGNNGraph:\n  num_nodes: 2708\n  num_edges: 10556\n  ndata:\n        val_mask = 2708-element BitVector\n        targets = 2708-element Vector{Int64}\n        test_mask = 2708-element BitVector\n        features = 1433×2708 Matrix{Float32}\n        train_mask = 2708-element BitVector source"},{"id":515,"pagetitle":"GNNGraph","title":"GNNGraph","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/gnngraph/#GNNGraph","content":" GNNGraph Documentation page for the graph type  GNNGraph  provided by GNNGraphs.jl and related methods.  Besides the methods documented here, one can rely on the large set of functionalities given by  Graphs.jl  thanks to the fact that  GNNGraph  inherits from  Graphs.AbstractGraph ."},{"id":516,"pagetitle":"GNNGraph","title":"GNNGraph type","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/gnngraph/#GNNGraph-type","content":" GNNGraph type"},{"id":517,"pagetitle":"GNNGraph","title":"GNNGraphs.GNNGraph","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/gnngraph/#GNNGraphs.GNNGraph","content":" GNNGraphs.GNNGraph  —  Type GNNGraph(data; [graph_type, ndata, edata, gdata, num_nodes, graph_indicator, dir])\nGNNGraph(g::GNNGraph; [ndata, edata, gdata]) A type representing a graph structure that also stores feature arrays associated to nodes, edges, and the graph itself. The feature arrays are stored in the fields  ndata ,  edata , and  gdata  as  DataStore  objects offering a convenient dictionary-like  and namedtuple-like interface. The features can be passed at construction time or added later. A  GNNGraph  can be constructed out of different  data  objects expressing the connections inside the graph. The internal representation type is determined by  graph_type . When constructed from another  GNNGraph , the internal graph representation is preserved and shared. The node/edge/graph features are retained as well, unless explicitely set by the keyword arguments  ndata ,  edata , and  gdata . A  GNNGraph  can also represent multiple graphs batched togheter (see  MLUtils.batch  or  SparseArrays.blockdiag ). The field  g.graph_indicator  contains the graph membership of each node. GNNGraph s are always directed graphs, therefore each edge is defined by a source node and a target node (see  edge_index ). Self loops (edges connecting a node to itself) and multiple edges (more than one edge between the same pair of nodes) are supported. A  GNNGraph  is a Graphs.jl's  AbstractGraph , therefore it supports most functionality from that library. Arguments data : Some data representing the graph topology. Possible type are An adjacency matrix An adjacency list. A tuple containing the source and target vectors (COO representation) A Graphs.jl' graph. graph_type : A keyword argument that specifies               the underlying representation used by the GNNGraph.               Currently supported values are :coo . Graph represented as a tuple  (source, target) , such that the  k -th edge         connects the node  source[k]  to node  target[k] .         Optionally, also edge weights can be given:  (source, target, weights) . :sparse . A sparse adjacency matrix representation. :dense . A dense adjacency matrix representation. Defaults to  :coo , currently the most supported type. dir : The assumed edge direction when given adjacency matrix or adjacency list input data  g .       Possible values are  :out  and  :in . Default  :out . num_nodes : The number of nodes. If not specified, inferred from  g . Default  nothing . graph_indicator : For batched graphs, a vector containing the graph assignment of each node. Default  nothing . ndata : Node features. An array or named tuple of arrays whose last dimension has size  num_nodes . edata : Edge features. An array or named tuple of arrays whose last dimension has size  num_edges . gdata : Graph features. An array or named tuple of arrays whose last dimension has size  num_graphs . Examples using GNNGraphs\n\n# Construct from adjacency list representation\ndata = [[2,3], [1,4,5], [1], [2,5], [2,4]]\ng = GNNGraph(data)\n\n# Number of nodes, edges, and batched graphs\ng.num_nodes  # 5\ng.num_edges  # 10\ng.num_graphs # 1\n\n# Same graph in COO representation\ns = [1,1,2,2,2,3,4,4,5,5]\nt = [2,3,1,4,5,3,2,5,2,4]\ng = GNNGraph(s, t)\n\n# From a Graphs' graph\ng = GNNGraph(erdos_renyi(100, 20))\n\n# Add 2 node feature arrays at creation time\ng = GNNGraph(g, ndata = (x=rand(100, g.num_nodes), y=rand(g.num_nodes)))\n\n# Add 1 edge feature array, after the graph creation\ng.edata.z = rand(16, g.num_edges)\n\n# Add node features and edge features with default names `x` and `e`\ng = GNNGraph(g, ndata = rand(100, g.num_nodes), edata = rand(16, g.num_edges))\n\ng.ndata.x # or just g.x\ng.edata.e # or just g.e\n\n# Collect edges' source and target nodes.\n# Both source and target are vectors of length num_edges\nsource, target = edge_index(g) A  GNNGraph  can be sent to the GPU, for example by using Flux.jl's  gpu  function or MLDataDevices.jl's utilities.  ``` source"},{"id":518,"pagetitle":"GNNGraph","title":"Base.copy","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/gnngraph/#Base.copy","content":" Base.copy  —  Function copy(g::GNNGraph; deep=false) Create a copy of  g . If  deep  is  true , then copy will be a deep copy (equivalent to  deepcopy(g) ), otherwise it will be a shallow copy with the same underlying graph data. source"},{"id":519,"pagetitle":"GNNGraph","title":"DataStore","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/gnngraph/#DataStore","content":" DataStore"},{"id":520,"pagetitle":"GNNGraph","title":"GNNGraphs.DataStore","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/gnngraph/#GNNGraphs.DataStore","content":" GNNGraphs.DataStore  —  Type DataStore([n, data])\nDataStore([n,] k1 = x1, k2 = x2, ...) A container for feature arrays. The optional argument  n  enforces that  numobs(x) == n  for each array contained in the datastore. At construction time, the  data  can be provided as any iterables of pairs of symbols and arrays or as keyword arguments: julia> ds = DataStore(3, x = rand(Float32, 2, 3), y = rand(Float32, 3))\nDataStore(3) with 2 elements:\n  y = 3-element Vector{Float32}\n  x = 2×3 Matrix{Float32}\n\njulia> ds = DataStore(3, Dict(:x => rand(Float32, 2, 3), :y => rand(Float32, 3))); # equivalent to above The  DataStore  has an interface similar to both dictionaries and named tuples. Arrays can be accessed and added using either the indexing or the property syntax: julia> ds = DataStore(x = ones(Float32, 2, 3), y = zeros(Float32, 3))\nDataStore() with 2 elements:\n  y = 3-element Vector{Float32}\n  x = 2×3 Matrix{Float32}\n\njulia> ds.x   # same as `ds[:x]`\n2×3 Matrix{Float32}:\n 1.0  1.0  1.0\n 1.0  1.0  1.0\n\njulia> ds.z = zeros(Float32, 3)  # Add new feature array `z`. Same as `ds[:z] = rand(Float32, 3)`\n3-element Vector{Float32}:\n 0.0\n 0.0\n 0.0 The  DataStore  can be iterated over, and the keys and values can be accessed using  keys(ds)  and  values(ds) .  map(f, ds)  applies the function  f  to each feature array: julia> ds2 = map(x -> x .+ 1, ds)\nDataStore() with 3 elements:\n  y = 3-element Vector{Float32}\n  z = 3-element Vector{Float32}\n  x = 2×3 Matrix{Float32}\n\njulia> ds2.z\n3-element Vector{Float32}:\n 1.0\n 1.0\n 1.0 source"},{"id":521,"pagetitle":"GNNGraph","title":"Query","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/gnngraph/#Query","content":" Query"},{"id":522,"pagetitle":"GNNGraph","title":"GNNGraphs.adjacency_list","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/gnngraph/#GNNGraphs.adjacency_list-Tuple{GNNGraph, Any}","content":" GNNGraphs.adjacency_list  —  Method adjacency_list(g; dir=:out)\nadjacency_list(g, nodes; dir=:out) Return the adjacency list representation (a vector of vectors) of the graph  g . Calling  a  the adjacency list, if  dir=:out  than  a[i]  will contain the neighbors of node  i  through outgoing edges. If  dir=:in , it will contain neighbors from incoming edges instead. If  nodes  is given, return the neighborhood of the nodes in  nodes  only. source"},{"id":523,"pagetitle":"GNNGraph","title":"GNNGraphs.edge_index","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/gnngraph/#GNNGraphs.edge_index-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}}","content":" GNNGraphs.edge_index  —  Method edge_index(g::GNNGraph) Return a tuple containing two vectors, respectively storing  the source and target nodes for each edges in  g . s, t = edge_index(g) source"},{"id":524,"pagetitle":"GNNGraph","title":"GNNGraphs.get_graph_type","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/gnngraph/#GNNGraphs.get_graph_type-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}}","content":" GNNGraphs.get_graph_type  —  Method get_graph_type(g::GNNGraph) Return the underlying representation for the graph  g  as a symbol. Possible values are: :coo : Coordinate list representation. The graph is stored as a tuple of vectors  (s, t, w) ,         where  s  and  t  are the source and target nodes of the edges, and  w  is the edge weights. :sparse : Sparse matrix representation. The graph is stored as a sparse matrix representing the weighted adjacency matrix. :dense : Dense matrix representation. The graph is stored as a dense matrix representing the weighted adjacency matrix. The default representation for graph constructors GNNGraphs.jl is  :coo . The underlying representation can be accessed through the  g.graph  field. See also  GNNGraph . Examples The default representation for graph constructors GNNGraphs.jl is  :coo . julia> g = rand_graph(5, 10)\nGNNGraph:\n  num_nodes: 5\n  num_edges: 10\n\njulia> get_graph_type(g)\n:coo The  GNNGraph  constructor can also be used to create graphs with different representations. julia> g = GNNGraph([2,3,5], [1,2,4], graph_type=:sparse)\nGNNGraph:\n  num_nodes: 5\n  num_edges: 3\n\njulia> g.graph\n5×5 SparseArrays.SparseMatrixCSC{Int64, Int64} with 3 stored entries:\n ⋅  ⋅  ⋅  ⋅  ⋅\n 1  ⋅  ⋅  ⋅  ⋅\n ⋅  1  ⋅  ⋅  ⋅\n ⋅  ⋅  ⋅  ⋅  ⋅\n ⋅  ⋅  ⋅  1  ⋅\n\njulia> get_graph_type(g)\n:sparse\n\njulia> gcoo = GNNGraph(g, graph_type=:coo);\n\njulia> gcoo.graph\n([2, 3, 5], [1, 2, 4], [1, 1, 1]) source"},{"id":525,"pagetitle":"GNNGraph","title":"GNNGraphs.graph_indicator","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/gnngraph/#GNNGraphs.graph_indicator-Tuple{GNNGraph}","content":" GNNGraphs.graph_indicator  —  Method graph_indicator(g::GNNGraph; edges=false) Return a vector containing the graph membership (an integer from  1  to  g.num_graphs ) of each node in the graph. If  edges=true , return the graph membership of each edge instead. source"},{"id":526,"pagetitle":"GNNGraph","title":"GNNGraphs.has_isolated_nodes","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/gnngraph/#GNNGraphs.has_isolated_nodes-Tuple{GNNGraph}","content":" GNNGraphs.has_isolated_nodes  —  Method has_isolated_nodes(g::GNNGraph; dir=:out) Return true if the graph  g  contains nodes with out-degree (if  dir=:out ) or in-degree (if  dir = :in ) equal to zero. source"},{"id":527,"pagetitle":"GNNGraph","title":"GNNGraphs.has_multi_edges","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/gnngraph/#GNNGraphs.has_multi_edges-Tuple{GNNGraph}","content":" GNNGraphs.has_multi_edges  —  Method has_multi_edges(g::GNNGraph) Return  true  if  g  has any multiple edges. source"},{"id":528,"pagetitle":"GNNGraph","title":"GNNGraphs.is_bidirected","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/gnngraph/#GNNGraphs.is_bidirected-Tuple{GNNGraph}","content":" GNNGraphs.is_bidirected  —  Method is_bidirected(g::GNNGraph) Check if the directed graph  g  essentially corresponds to an undirected graph, i.e. if for each edge it also contains the  reverse edge.  source"},{"id":529,"pagetitle":"GNNGraph","title":"GNNGraphs.khop_adj","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/gnngraph/#GNNGraphs.khop_adj","content":" GNNGraphs.khop_adj  —  Function khop_adj(g::GNNGraph,k::Int,T::DataType=eltype(g); dir=:out, weighted=true) Return  $A^k$  where  $A$  is the adjacency matrix of the graph 'g'. source"},{"id":530,"pagetitle":"GNNGraph","title":"GNNGraphs.laplacian_lambda_max","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/gnngraph/#GNNGraphs.laplacian_lambda_max","content":" GNNGraphs.laplacian_lambda_max  —  Function laplacian_lambda_max(g::GNNGraph, T=Float32; add_self_loops=false, dir=:out) Return the largest eigenvalue of the normalized symmetric Laplacian of the graph  g . If the graph is batched from multiple graphs, return the list of the largest eigenvalue for each graph. source"},{"id":531,"pagetitle":"GNNGraph","title":"GNNGraphs.normalized_laplacian","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/gnngraph/#GNNGraphs.normalized_laplacian","content":" GNNGraphs.normalized_laplacian  —  Function normalized_laplacian(g, T=Float32; add_self_loops=false, dir=:out) Normalized Laplacian matrix of graph  g . Arguments g : A  GNNGraph . T : result element type. add_self_loops : add self-loops while calculating the matrix. dir : the edge directionality considered (:out, :in, :both). source"},{"id":532,"pagetitle":"GNNGraph","title":"GNNGraphs.scaled_laplacian","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/gnngraph/#GNNGraphs.scaled_laplacian","content":" GNNGraphs.scaled_laplacian  —  Function scaled_laplacian(g, T=Float32; dir=:out) Scaled Laplacian matrix of graph  g , defined as  $\\hat{L} = \\frac{2}{\\lambda_{max}} L - I$  where  $L$  is the normalized Laplacian matrix. Arguments g : A  GNNGraph . T : result element type. dir : the edge directionality considered (:out, :in, :both). source"},{"id":533,"pagetitle":"GNNGraph","title":"Graphs.LinAlg.adjacency_matrix","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/gnngraph/#Graphs.LinAlg.adjacency_matrix","content":" Graphs.LinAlg.adjacency_matrix  —  Function adjacency_matrix(g::GNNGraph, T=eltype(g); dir=:out, weighted=true) Return the adjacency matrix  A  for the graph  g .  If  dir=:out ,  A[i,j] > 0  denotes the presence of an edge from node  i  to node  j . If  dir=:in  instead,  A[i,j] > 0  denotes the presence of an edge from node  j  to node  i . User may specify the eltype  T  of the returned matrix.  If  weighted=true , the  A  will contain the edge weights if any, otherwise the elements of  A  will be either 0 or 1. source"},{"id":534,"pagetitle":"GNNGraph","title":"Graphs.degree","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/gnngraph/#Graphs.degree-Union{Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}}, Tuple{TT}, Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}, TT}} where TT<:Union{Nothing, Type{<:Number}}","content":" Graphs.degree  —  Method degree(g::GNNGraph, T=nothing; dir=:out, edge_weight=true) Return a vector containing the degrees of the nodes in  g . The gradient is propagated through this function only if  edge_weight  is  true  or a vector. Arguments g : A graph. T : Element type of the returned vector. If  nothing , is      chosen based on the graph type and will be an integer      if  edge_weight = false . Default  nothing . dir : For  dir = :out  the degree of a node is counted based on the outgoing edges.        For  dir = :in , the ingoing edges are used. If  dir = :both  we have the sum of the two. edge_weight : If  true  and the graph contains weighted edges, the degree will                be weighted. Set to  false  instead to just count the number of               outgoing/ingoing edges.                Finally, you can also pass a vector of weights to be used               instead of the graph's own weights.               Default  true . source"},{"id":535,"pagetitle":"GNNGraph","title":"Graphs.has_self_loops","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/gnngraph/#Graphs.has_self_loops-Tuple{GNNGraph}","content":" Graphs.has_self_loops  —  Method has_self_loops(g::GNNGraph) Return  true  if  g  has any self loops. source"},{"id":536,"pagetitle":"GNNGraph","title":"Graphs.inneighbors","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/gnngraph/#Graphs.inneighbors-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}, Integer}","content":" Graphs.inneighbors  —  Method inneighbors(g::GNNGraph, i::Integer) Return the neighbors of node  i  in the graph  g  through incoming edges. See also  neighbors  and  outneighbors . source"},{"id":537,"pagetitle":"GNNGraph","title":"Graphs.outneighbors","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/gnngraph/#Graphs.outneighbors-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}, Integer}","content":" Graphs.outneighbors  —  Method outneighbors(g::GNNGraph, i::Integer) Return the neighbors of node  i  in the graph  g  through outgoing edges. See also  neighbors  and  inneighbors . source"},{"id":538,"pagetitle":"GNNGraph","title":"Graphs.neighbors","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/gnngraph/#Graphs.neighbors-Tuple{GNNGraph, Integer}","content":" Graphs.neighbors  —  Method neighbors(g::GNNGraph, i::Integer; dir=:out) Return the neighbors of node  i  in the graph  g . If  dir=:out , return the neighbors through outgoing edges. If  dir=:in , return the neighbors through incoming edges. See also  outneighbors ,  inneighbors . source"},{"id":539,"pagetitle":"GNNGraph","title":"Transform","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/gnngraph/#Transform","content":" Transform"},{"id":540,"pagetitle":"GNNGraph","title":"GNNGraphs.add_edges","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/gnngraph/#GNNGraphs.add_edges-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}, AbstractVector, AbstractVector}","content":" GNNGraphs.add_edges  —  Method add_edges(g::GNNGraph, s::AbstractVector, t::AbstractVector; [edata])\nadd_edges(g::GNNGraph, (s, t); [edata])\nadd_edges(g::GNNGraph, (s, t, w); [edata]) Add to graph  g  the edges with source nodes  s  and target nodes  t . Optionally, pass the edge weight  w  and the features   edata  for the new edges. Returns a new graph sharing part of the underlying data with  g . If the  s  or  t  contain nodes that are not already present in the graph, they are added to the graph as well. Examples julia> s, t = [1, 2, 3, 3, 4], [2, 3, 4, 4, 4];\n\njulia> w = Float32[1.0, 2.0, 3.0, 4.0, 5.0];\n\njulia> g = GNNGraph((s, t, w))\nGNNGraph:\n  num_nodes: 4\n  num_edges: 5\n\njulia> add_edges(g, ([2, 3], [4, 1], [10.0, 20.0]))\nGNNGraph:\n  num_nodes: 4\n  num_edges: 7 julia> g = GNNGraph()\nGNNGraph:\n  num_nodes: 0\n  num_edges: 0\n\njulia> add_edges(g, [1,2], [2,3])\nGNNGraph:\n  num_nodes: 3\n  num_edges: 2 source"},{"id":541,"pagetitle":"GNNGraph","title":"GNNGraphs.add_nodes","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/gnngraph/#GNNGraphs.add_nodes-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}, Integer}","content":" GNNGraphs.add_nodes  —  Method add_nodes(g::GNNGraph, n; [ndata]) Add  n  new nodes to graph  g . In the  new graph, these nodes will have indexes from  g.num_nodes + 1  to  g.num_nodes + n . source"},{"id":542,"pagetitle":"GNNGraph","title":"GNNGraphs.add_self_loops","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/gnngraph/#GNNGraphs.add_self_loops-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}}","content":" GNNGraphs.add_self_loops  —  Method add_self_loops(g::GNNGraph) Return a graph with the same features as  g  but also adding edges connecting the nodes to themselves. Nodes with already existing self-loops will obtain a second self-loop. If the graphs has edge weights, the new edges will have weight 1. source"},{"id":543,"pagetitle":"GNNGraph","title":"GNNGraphs.getgraph","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/gnngraph/#GNNGraphs.getgraph-Tuple{GNNGraph, Int64}","content":" GNNGraphs.getgraph  —  Method getgraph(g::GNNGraph, i; nmap=false) Return the subgraph of  g  induced by those nodes  j  for which  g.graph_indicator[j] == i  or, if  i  is a collection,  g.graph_indicator[j] ∈ i .  In other words, it extract the component graphs from a batched graph.  If  nmap=true , return also a vector  v  mapping the new nodes to the old ones.  The node  i  in the subgraph will correspond to the node  v[i]  in  g . source"},{"id":544,"pagetitle":"GNNGraph","title":"GNNGraphs.negative_sample","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/gnngraph/#GNNGraphs.negative_sample-Tuple{GNNGraph}","content":" GNNGraphs.negative_sample  —  Method negative_sample(g::GNNGraph; \n                num_neg_edges = g.num_edges, \n                bidirected = is_bidirected(g)) Return a graph containing random negative edges (i.e. non-edges) from graph  g  as edges. If  bidirected=true , the output graph will be bidirected and there will be no leakage from the origin graph.  See also  is_bidirected . source"},{"id":545,"pagetitle":"GNNGraph","title":"GNNGraphs.perturb_edges","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/gnngraph/#GNNGraphs.perturb_edges-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}, AbstractFloat}","content":" GNNGraphs.perturb_edges  —  Method perturb_edges([rng], g::GNNGraph, perturb_ratio) Return a new graph obtained from  g  by adding random edges, based on a specified  perturb_ratio .  The  perturb_ratio  determines the fraction of new edges to add relative to the current number of edges in the graph.  These new edges are added without creating self-loops.  The function returns a new  GNNGraph  instance that shares some of the underlying data with  g  but includes the additional edges.  The nodes for the new edges are selected randomly, and no edge data ( edata ) or weights ( w ) are assigned to these new edges. Arguments g::GNNGraph : The graph to be perturbed. perturb_ratio : The ratio of the number of new edges to add relative to the current number of edges in the graph. For example, a  perturb_ratio  of 0.1 means that 10% of the current number of edges will be added as new random edges. rng : An optionalrandom number generator to ensure reproducible results. Examples julia> g = GNNGraph((s, t, w))\nGNNGraph:\n  num_nodes: 4\n  num_edges: 5\n\njulia> perturbed_g = perturb_edges(g, 0.2)\nGNNGraph:\n  num_nodes: 4\n  num_edges: 6 source"},{"id":546,"pagetitle":"GNNGraph","title":"GNNGraphs.ppr_diffusion","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/gnngraph/#GNNGraphs.ppr_diffusion-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}}","content":" GNNGraphs.ppr_diffusion  —  Method ppr_diffusion(g::GNNGraph{<:COO_T}, alpha =0.85f0) -> GNNGraph Calculates the Personalized PageRank (PPR) diffusion based on the edge weight matrix of a GNNGraph and updates the graph with new edge weights derived from the PPR matrix. References paper:  The pagerank citation ranking: Bringing order to the web The function performs the following steps: Constructs a modified adjacency matrix  A  using the graph's edge weights, where  A  is adjusted by  (α - 1) * A + I , with  α  being the damping factor ( alpha_f32 ) and  I  the identity matrix. Normalizes  A  to ensure each column sums to 1, representing transition probabilities. Applies the PPR formula  α * (I + (α - 1) * A)^-1  to compute the diffusion matrix. Updates the original edge weights of the graph based on the PPR diffusion matrix, assigning new weights for each edge from the PPR matrix. Arguments g::GNNGraph : The input graph for which PPR diffusion is to be calculated. It should have edge weights available. alpha_f32::Float32 : The damping factor used in PPR calculation, controlling the teleport probability in the random walk. Defaults to  0.85f0 . Returns A new  GNNGraph  instance with the same structure as  g  but with updated edge weights according to the PPR diffusion calculation. source"},{"id":547,"pagetitle":"GNNGraph","title":"GNNGraphs.rand_edge_split","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/gnngraph/#GNNGraphs.rand_edge_split-Tuple{GNNGraph, Any}","content":" GNNGraphs.rand_edge_split  —  Method rand_edge_split(g::GNNGraph, frac; bidirected=is_bidirected(g)) -> g1, g2 Randomly partition the edges in  g  to form two graphs,  g1  and  g2 . Both will have the same number of nodes as  g .  g1  will contain a fraction  frac  of the original edges,  while  g2  wil contain the rest. If  bidirected = true  makes sure that an edge and its reverse go into the same split. This option is supported only for bidirected graphs with no self-loops and multi-edges. rand_edge_split  is tipically used to create train/test splits in link prediction tasks. source"},{"id":548,"pagetitle":"GNNGraph","title":"GNNGraphs.random_walk_pe","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/gnngraph/#GNNGraphs.random_walk_pe-Tuple{GNNGraph, Int64}","content":" GNNGraphs.random_walk_pe  —  Method random_walk_pe(g, walk_length) Return the random walk positional encoding from the paper  Graph Neural Networks with Learnable Structural and Positional Representations  of the given graph  g  and the length of the walk  walk_length  as a matrix of size  (walk_length, g.num_nodes) .  source"},{"id":549,"pagetitle":"GNNGraph","title":"GNNGraphs.remove_edges","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/gnngraph/#GNNGraphs.remove_edges-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}, AbstractVector{<:Integer}}","content":" GNNGraphs.remove_edges  —  Method remove_edges(g::GNNGraph, edges_to_remove::AbstractVector{<:Integer})\nremove_edges(g::GNNGraph, p=0.5) Remove specified edges from a GNNGraph, either by specifying edge indices or by randomly removing edges with a given probability. Arguments g : The input graph from which edges will be removed. edges_to_remove : Vector of edge indices to be removed. This argument is only required for the first method. p : Probability of removing each edge. This argument is only required for the second method and defaults to 0.5. Returns A new GNNGraph with the specified edges removed. Example julia> using GNNGraphs\n\n# Construct a GNNGraph\njulia> g = GNNGraph([1, 1, 2, 2, 3], [2, 3, 1, 3, 1])\nGNNGraph:\n  num_nodes: 3\n  num_edges: 5\n  \n# Remove the second edge\njulia> g_new = remove_edges(g, [2]);\n\njulia> g_new\nGNNGraph:\n  num_nodes: 3\n  num_edges: 4\n\n# Remove edges with a probability of 0.5\njulia> g_new = remove_edges(g, 0.5);\n\njulia> g_new\nGNNGraph:\n  num_nodes: 3\n  num_edges: 2 source"},{"id":550,"pagetitle":"GNNGraph","title":"GNNGraphs.remove_multi_edges","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/gnngraph/#GNNGraphs.remove_multi_edges-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}}","content":" GNNGraphs.remove_multi_edges  —  Method remove_multi_edges(g::GNNGraph; aggr=+) Remove multiple edges (also called parallel edges or repeated edges) from graph  g . Possible edge features are aggregated according to  aggr , that can take value   + , min ,  max  or  mean . See also  remove_self_loops ,  has_multi_edges , and  to_bidirected . source"},{"id":551,"pagetitle":"GNNGraph","title":"GNNGraphs.remove_nodes","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/gnngraph/#GNNGraphs.remove_nodes-Tuple{GNNGraph, AbstractFloat}","content":" GNNGraphs.remove_nodes  —  Method remove_nodes(g::GNNGraph, p) Returns a new graph obtained by dropping nodes from  g  with independent probabilities  p .  Examples julia> g = GNNGraph([1, 1, 2, 2, 3, 4], [1, 2, 3, 1, 3, 1])\nGNNGraph:\n  num_nodes: 4\n  num_edges: 6\n\njulia> g_new = remove_nodes(g, 0.5)\nGNNGraph:\n  num_nodes: 2\n  num_edges: 2 source"},{"id":552,"pagetitle":"GNNGraph","title":"GNNGraphs.remove_nodes","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/gnngraph/#GNNGraphs.remove_nodes-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}, AbstractVector}","content":" GNNGraphs.remove_nodes  —  Method remove_nodes(g::GNNGraph, nodes_to_remove::AbstractVector) Remove specified nodes, and their associated edges, from a GNNGraph. This operation reindexes the remaining nodes to maintain a continuous sequence of node indices, starting from 1. Similarly, edges are reindexed to account for the removal of edges connected to the removed nodes. Arguments g : The input graph from which nodes (and their edges) will be removed. nodes_to_remove : Vector of node indices to be removed. Returns A new GNNGraph with the specified nodes and all edges associated with these nodes removed.  Example using GNNGraphs\n\ng = GNNGraph([1, 1, 2, 2, 3], [2, 3, 1, 3, 1])\n\n# Remove nodes with indices 2 and 3, for example\ng_new = remove_nodes(g, [2, 3])\n\n# g_new now does not contain nodes 2 and 3, and any edges that were connected to these nodes.\nprintln(g_new) source"},{"id":553,"pagetitle":"GNNGraph","title":"GNNGraphs.remove_self_loops","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/gnngraph/#GNNGraphs.remove_self_loops-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}}","content":" GNNGraphs.remove_self_loops  —  Method remove_self_loops(g::GNNGraph) Return a graph constructed from  g  where self-loops (edges from a node to itself) are removed.  See also  add_self_loops  and  remove_multi_edges . source"},{"id":554,"pagetitle":"GNNGraph","title":"GNNGraphs.set_edge_weight","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/gnngraph/#GNNGraphs.set_edge_weight-Tuple{GNNGraph, AbstractVector}","content":" GNNGraphs.set_edge_weight  —  Method set_edge_weight(g::GNNGraph, w::AbstractVector) Set  w  as edge weights in the returned graph.  source"},{"id":555,"pagetitle":"GNNGraph","title":"GNNGraphs.to_bidirected","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/gnngraph/#GNNGraphs.to_bidirected-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}}","content":" GNNGraphs.to_bidirected  —  Method to_bidirected(g) Adds a reverse edge for each edge in the graph, then calls   remove_multi_edges  with  mean  aggregation to simplify the graph.  See also  is_bidirected .  Examples julia> s, t = [1, 2, 3, 3, 4], [2, 3, 4, 4, 4];\n\njulia> w = [1.0, 2.0, 3.0, 4.0, 5.0];\n\njulia> e = [10.0, 20.0, 30.0, 40.0, 50.0];\n\njulia> g = GNNGraph(s, t, w, edata = e)\nGNNGraph:\n  num_nodes: 4\n  num_edges: 5\n  edata:\n        e = 5-element Vector{Float64}\n\njulia> g2 = to_bidirected(g)\nGNNGraph:\n  num_nodes: 4\n  num_edges: 7\n  edata:\n        e = 7-element Vector{Float64}\n\njulia> edge_index(g2)\n([1, 2, 2, 3, 3, 4, 4], [2, 1, 3, 2, 4, 3, 4])\n\njulia> get_edge_weight(g2)\n7-element Vector{Float64}:\n 1.0\n 1.0\n 2.0\n 2.0\n 3.5\n 3.5\n 5.0\n\njulia> g2.edata.e\n7-element Vector{Float64}:\n 10.0\n 10.0\n 20.0\n 20.0\n 35.0\n 35.0\n 50.0 source"},{"id":556,"pagetitle":"GNNGraph","title":"GNNGraphs.to_unidirected","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/gnngraph/#GNNGraphs.to_unidirected-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}}","content":" GNNGraphs.to_unidirected  —  Method to_unidirected(g::GNNGraph) Return a graph that for each multiple edge between two nodes in  g  keeps only an edge in one direction. source"},{"id":557,"pagetitle":"GNNGraph","title":"MLUtils.batch","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/gnngraph/#MLUtils.batch-Tuple{AbstractVector{<:GNNGraph}}","content":" MLUtils.batch  —  Method batch(gs::Vector{<:GNNGraph}) Batch together multiple  GNNGraph s into a single one  containing the total number of original nodes and edges. Equivalent to  SparseArrays.blockdiag . See also  MLUtils.unbatch . Examples julia> g1 = rand_graph(4, 4, ndata=ones(Float32, 3, 4))\nGNNGraph:\n  num_nodes: 4\n  num_edges: 4\n  ndata:\n        x = 3×4 Matrix{Float32}\n\njulia> g2 = rand_graph(5, 4, ndata=zeros(Float32, 3, 5))\nGNNGraph:\n  num_nodes: 5\n  num_edges: 4\n  ndata:\n        x = 3×5 Matrix{Float32}\n\njulia> g12 = MLUtils.batch([g1, g2])\nGNNGraph:\n  num_nodes: 9\n  num_edges: 8\n  num_graphs: 2\n  ndata:\n        x = 3×9 Matrix{Float32}\n\njulia> g12.ndata.x\n3×9 Matrix{Float32}:\n 1.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0\n 1.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0\n 1.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0 source"},{"id":558,"pagetitle":"GNNGraph","title":"MLUtils.unbatch","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/gnngraph/#MLUtils.unbatch-Union{Tuple{GNNGraph{T}}, Tuple{T}} where T<:(Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}})","content":" MLUtils.unbatch  —  Method unbatch(g::GNNGraph) Opposite of the  MLUtils.batch  operation, returns  an array of the individual graphs batched together in  g . See also  MLUtils.batch  and  getgraph . Examples julia> using MLUtils\n\njulia> gbatched = MLUtils.batch([rand_graph(5, 6), rand_graph(10, 8), rand_graph(4,2)])\nGNNGraph:\n  num_nodes: 19\n  num_edges: 16\n  num_graphs: 3\n\njulia> MLUtils.unbatch(gbatched)\n3-element Vector{GNNGraph{Tuple{Vector{Int64}, Vector{Int64}, Nothing}}}:\n GNNGraph(5, 6) with no data\n GNNGraph(10, 8) with no data\n GNNGraph(4, 2) with no data source"},{"id":559,"pagetitle":"GNNGraph","title":"SparseArrays.blockdiag","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/gnngraph/#SparseArrays.blockdiag-Tuple{GNNGraph, Vararg{GNNGraph}}","content":" SparseArrays.blockdiag  —  Method blockdiag(xs::GNNGraph...) Equivalent to  MLUtils.batch . source"},{"id":560,"pagetitle":"GNNGraph","title":"Utils","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/gnngraph/#Utils","content":" Utils"},{"id":561,"pagetitle":"GNNGraph","title":"GNNGraphs.sort_edge_index","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/gnngraph/#GNNGraphs.sort_edge_index","content":" GNNGraphs.sort_edge_index  —  Function sort_edge_index(ei::Tuple) -> u', v'\nsort_edge_index(u, v) -> u', v' Return a sorted version of the tuple of vectors  ei = (u, v) , applying a common permutation to  u  and  v . The sorting is lexycographic, that is the pairs  (ui, vi)   are sorted first according to the  ui  and then according to  vi .  source"},{"id":562,"pagetitle":"GNNGraph","title":"GNNGraphs.color_refinement","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/gnngraph/#GNNGraphs.color_refinement","content":" GNNGraphs.color_refinement  —  Function color_refinement(g::GNNGraph, [x0]) -> x, num_colors, niters The color refinement algorithm for graph coloring.  Given a graph  g  and an initial coloring  x0 , the algorithm  iteratively refines the coloring until a fixed point is reached. At each iteration the algorithm computes a hash of the coloring and the sorted list of colors of the neighbors of each node. This hash is used to determine if the coloring has changed. math x_i' = hashmap((x_i, sort([x_j for j \\in N(i)]))). ` This algorithm is related to the 1-Weisfeiler-Lehman algorithm for graph isomorphism testing. Arguments g::GNNGraph : The graph to color. x0::AbstractVector{<:Integer} : The initial coloring. If not provided, all nodes are colored with 1. Returns x::AbstractVector{<:Integer} : The final coloring. num_colors::Int : The number of colors used. niters::Int : The number of iterations until convergence. source"},{"id":563,"pagetitle":"GNNGraph","title":"Generate","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/gnngraph/#Generate","content":" Generate"},{"id":564,"pagetitle":"GNNGraph","title":"GNNGraphs.knn_graph","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/gnngraph/#GNNGraphs.knn_graph-Tuple{AbstractMatrix, Int64}","content":" GNNGraphs.knn_graph  —  Method knn_graph(points::AbstractMatrix, \n          k::Int; \n          graph_indicator = nothing,\n          self_loops = false, \n          dir = :in, \n          kws...) Create a  k -nearest neighbor graph where each node is linked  to its  k  closest  points .   Arguments points : A num features × num nodes matrix storing the Euclidean positions of the nodes. k : The number of neighbors considered in the kNN algorithm. graph_indicator : Either nothing or a vector containing the graph assignment of each node,                     in which case the returned graph will be a batch of graphs.  self_loops : If  true , consider the node itself among its  k  nearest neighbors, in which               case the graph will contain self-loops.  dir : The direction of the edges. If  dir=:in  edges go from the  k          neighbors to the central node. If  dir=:out  we have the opposite        direction. kws : Further keyword arguments will be passed to the  GNNGraph  constructor. Examples julia> n, k = 10, 3;\n\njulia> x = rand(Float32, 3, n);\n\njulia> g = knn_graph(x, k)\nGNNGraph:\n    num_nodes = 10\n    num_edges = 30\n\njulia> graph_indicator = [1,1,1,1,1,2,2,2,2,2];\n\njulia> g = knn_graph(x, k; graph_indicator)\nGNNGraph:\n    num_nodes = 10\n    num_edges = 30\n    num_graphs = 2 source"},{"id":565,"pagetitle":"GNNGraph","title":"GNNGraphs.radius_graph","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/gnngraph/#GNNGraphs.radius_graph-Tuple{AbstractMatrix, AbstractFloat}","content":" GNNGraphs.radius_graph  —  Method radius_graph(points::AbstractMatrix, \n             r::AbstractFloat; \n             graph_indicator = nothing,\n             self_loops = false, \n             dir = :in, \n             kws...) Create a graph where each node is linked  to its neighbors within a given distance  r .   Arguments points : A num features × num nodes matrix storing the Euclidean positions of the nodes. r : The radius. graph_indicator : Either nothing or a vector containing the graph assignment of each node,                     in which case the returned graph will be a batch of graphs.  self_loops : If  true , consider the node itself among its neighbors, in which               case the graph will contain self-loops.  dir : The direction of the edges. If  dir=:in  edges go from the        neighbors to the central node. If  dir=:out  we have the opposite        direction. kws : Further keyword arguments will be passed to the  GNNGraph  constructor. Examples julia> n, r = 10, 0.75;\n\njulia> x = rand(Float32, 3, n);\n\njulia> g = radius_graph(x, r)\nGNNGraph:\n    num_nodes = 10\n    num_edges = 46\n\njulia> graph_indicator = [1,1,1,1,1,2,2,2,2,2];\n\njulia> g = radius_graph(x, r; graph_indicator)\nGNNGraph:\n    num_nodes = 10\n    num_edges = 20\n    num_graphs = 2 References Section B paragraphs 1 and 2 of the paper  Dynamic Hidden-Variable Network Models source"},{"id":566,"pagetitle":"GNNGraph","title":"GNNGraphs.rand_graph","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/gnngraph/#GNNGraphs.rand_graph-Tuple{Integer, Integer}","content":" GNNGraphs.rand_graph  —  Method rand_graph([rng,] n, m; bidirected=true, edge_weight = nothing, kws...) Generate a random (Erdós-Renyi)  GNNGraph  with  n  nodes and  m  edges. If  bidirected=true  the reverse edge of each edge will be present. If  bidirected=false  instead,  m  unrelated edges are generated. In any case, the output graph will contain no self-loops or multi-edges. A vector can be passed  as  edge_weight . Its length has to be equal to  m  in the directed case, and  m÷2  in the bidirected one. Pass a random number generator as the first argument to make the generation reproducible. Additional keyword arguments will be passed to the  GNNGraph  constructor. Examples julia> g = rand_graph(5, 4, bidirected=false)\nGNNGraph:\n  num_nodes: 5\n  num_edges: 4\n\njulia> edge_index(g)\n([4, 3, 2, 1], [5, 4, 3, 2])\n\n# In the bidirected case, edge data will be duplicated on the reverse edges if needed.\njulia> g = rand_graph(5, 4, edata=rand(Float32, 16, 2))\nGNNGraph:\n  num_nodes: 5\n  num_edges: 4\n  edata:\n        e = 16×4 Matrix{Float32}\n\n# Each edge has a reverse\njulia> edge_index(g)\n([1, 1, 5, 3], [5, 3, 1, 1]) source"},{"id":567,"pagetitle":"GNNGraph","title":"Operators","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/gnngraph/#Operators","content":" Operators"},{"id":568,"pagetitle":"GNNGraph","title":"Base.intersect","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/gnngraph/#Base.intersect","content":" Base.intersect  —  Function intersect(g1::GNNGraph, g2::GNNGraph) Intersect two graphs by keeping only the common edges. source"},{"id":569,"pagetitle":"GNNGraph","title":"Sampling","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/gnngraph/#Sampling","content":" Sampling"},{"id":570,"pagetitle":"GNNGraph","title":"GNNGraphs.sample_neighbors","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/gnngraph/#GNNGraphs.sample_neighbors","content":" GNNGraphs.sample_neighbors  —  Function sample_neighbors(g, nodes, K=-1; dir=:in, replace=false, dropnodes=false) Sample neighboring edges of the given nodes and return the induced subgraph. For each node, a number of inbound (or outbound when  dir = :out ) edges will be randomly chosen.  If dropnodes=false`, the graph returned will then contain all the nodes in the original graph,  but only the sampled edges. The returned graph will contain an edge feature  EID  corresponding to the id of the edge in the original graph. If  dropnodes=true , it will also contain a node feature  NID  with the node ids in the original graph. Arguments g . The graph. nodes . A list of node IDs to sample neighbors from. K . The maximum number of edges to be sampled for each node.      If -1, all the neighboring edges will be selected. dir . Determines whether to sample inbound ( :in ) or outbound (` :out ) edges (Default  :in ). replace . If  true , sample with replacement. dropnodes . If  true , the resulting subgraph will contain only the nodes involved in the sampled edges. Examples julia> g = rand_graph(20, 100)\nGNNGraph:\n    num_nodes = 20\n    num_edges = 100\n\njulia> sample_neighbors(g, 2:3)\nGNNGraph:\n    num_nodes = 20\n    num_edges = 9\n    edata:\n        EID => (9,)\n\njulia> sg = sample_neighbors(g, 2:3, dropnodes=true)\nGNNGraph:\n    num_nodes = 10\n    num_edges = 9\n    ndata:\n        NID => (10,)\n    edata:\n        EID => (9,)\n\njulia> sg.ndata.NID\n10-element Vector{Int64}:\n  2\n  3\n 17\n 14\n 18\n 15\n 16\n 20\n  7\n 10\n\njulia> sample_neighbors(g, 2:3, 5, replace=true)\nGNNGraph:\n    num_nodes = 20\n    num_edges = 10\n    edata:\n        EID => (10,) source"},{"id":571,"pagetitle":"GNNGraph","title":"Graphs.induced_subgraph","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/gnngraph/#Graphs.induced_subgraph-Tuple{GNNGraph, Vector{Int64}}","content":" Graphs.induced_subgraph  —  Method induced_subgraph(graph, nodes) Generates a subgraph from the original graph using the provided  nodes .  The function includes the nodes' neighbors and creates edges between nodes that are connected in the original graph.  If a node has no neighbors, an isolated node will be added to the subgraph.  Returns A new  GNNGraph  containing the subgraph with the specified nodes and their features. Arguments graph . The original GNNGraph containing nodes, edges, and node features. nodes `. A vector of node indices to include in the subgraph. Examples julia> s = [1, 2]\n2-element Vector{Int64}:\n 1\n 2\n\njulia> t = [2, 3]\n2-element Vector{Int64}:\n 2\n 3\n\njulia> graph = GNNGraph((s, t), ndata = (; x=rand(Float32, 32, 3), y=rand(Float32, 3)), edata = rand(Float32, 2))\nGNNGraph:\n  num_nodes: 3\n  num_edges: 2\n  ndata:\n        y = 3-element Vector{Float32}\n        x = 32×3 Matrix{Float32}\n  edata:\n        e = 2-element Vector{Float32}\n\njulia> nodes = [1, 2]\n2-element Vector{Int64}:\n 1\n 2\n\njulia> subgraph = Graphs.induced_subgraph(graph, nodes)\nGNNGraph:\n  num_nodes: 2\n  num_edges: 1\n  ndata:\n        y = 2-element Vector{Float32}\n        x = 32×2 Matrix{Float32}\n  edata:\n        e = 1-element Vector{Float32} source"},{"id":574,"pagetitle":"GNNHeteroGraph","title":"Heterogeneous Graphs","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/heterograph/#Heterogeneous-Graphs","content":" Heterogeneous Graphs"},{"id":575,"pagetitle":"GNNHeteroGraph","title":"GNNHeteroGraph","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/heterograph/#GNNHeteroGraph","content":" GNNHeteroGraph Documentation page for the type  GNNHeteroGraph  representing heterogeneous graphs, where  nodes and edges can have different types."},{"id":576,"pagetitle":"GNNHeteroGraph","title":"GNNGraphs.GNNHeteroGraph","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/heterograph/#GNNGraphs.GNNHeteroGraph","content":" GNNGraphs.GNNHeteroGraph  —  Type GNNHeteroGraph(data; [ndata, edata, gdata, num_nodes])\nGNNHeteroGraph(pairs...; [ndata, edata, gdata, num_nodes]) A type representing a heterogeneous graph structure. It is similar to  GNNGraph  but nodes and edges are of different types. Constructor Arguments data : A dictionary or an iterable object that maps  (source_type, edge_type, target_type)          triples to  (source, target)  index vectors (or to  (source, target, weight)  if also edge weights are present). pairs : Passing multiple relations as pairs is equivalent to passing  data=Dict(pairs...) . ndata : Node features. A dictionary of arrays or named tuple of arrays.          The size of the last dimension of each array must be given by  g.num_nodes . edata : Edge features. A dictionary of arrays or named tuple of arrays. Default  nothing .          The size of the last dimension of each array must be given by  g.num_edges . Default  nothing . gdata : Graph features. An array or named tuple of arrays whose last dimension has size  num_graphs . Default  nothing . num_nodes : The number of nodes for each type. If not specified, inferred from  data . Default  nothing . Fields graph : A dictionary that maps  (source_type, edge_type, target_type)  triples to  (source, target)  index vectors. num_nodes : The number of nodes for each type. num_edges : The number of edges for each type. ndata : Node features. edata : Edge features. gdata : Graph features. ntypes : The node types. etypes : The edge types. Examples julia> using GNNGraphs\n\njulia> nA, nB = 10, 20;\n\njulia> num_nodes = Dict(:A => nA, :B => nB);\n\njulia> edges1 = (rand(1:nA, 20), rand(1:nB, 20))\n([4, 8, 6, 3, 4, 7, 2, 7, 3, 2, 3, 4, 9, 4, 2, 9, 10, 1, 3, 9], [6, 4, 20, 8, 16, 7, 12, 16, 5, 4, 6, 20, 11, 19, 17, 9, 12, 2, 18, 12])\n\njulia> edges2 = (rand(1:nB, 30), rand(1:nA, 30))\n([17, 5, 2, 4, 5, 3, 8, 7, 9, 7  …  19, 8, 20, 7, 16, 2, 9, 15, 8, 13], [1, 1, 3, 1, 1, 3, 2, 7, 4, 4  …  7, 10, 6, 3, 4, 9, 1, 5, 8, 5])\n\njulia> data = ((:A, :rel1, :B) => edges1, (:B, :rel2, :A) => edges2);\n\njulia> hg = GNNHeteroGraph(data; num_nodes)\nGNNHeteroGraph:\n  num_nodes: (:A => 10, :B => 20)\n  num_edges: ((:A, :rel1, :B) => 20, (:B, :rel2, :A) => 30)\n\njulia> hg.num_edges\nDict{Tuple{Symbol, Symbol, Symbol}, Int64} with 2 entries:\n(:A, :rel1, :B) => 20\n(:B, :rel2, :A) => 30\n\n# Let's add some node features\njulia> ndata = Dict(:A => (x = rand(2, nA), y = rand(3, num_nodes[:A])),\n                    :B => rand(10, nB));\n\njulia> hg = GNNHeteroGraph(data; num_nodes, ndata)\nGNNHeteroGraph:\n    num_nodes: (:A => 10, :B => 20)\n    num_edges: ((:A, :rel1, :B) => 20, (:B, :rel2, :A) => 30)\n    ndata:\n    :A  =>  (x = 2×10 Matrix{Float64}, y = 3×10 Matrix{Float64})\n    :B  =>  x = 10×20 Matrix{Float64}\n\n# Access features of nodes of type :A\njulia> hg.ndata[:A].x\n2×10 Matrix{Float64}:\n    0.825882  0.0797502  0.245813  0.142281  0.231253  0.685025  0.821457  0.888838  0.571347   0.53165\n    0.631286  0.316292   0.705325  0.239211  0.533007  0.249233  0.473736  0.595475  0.0623298  0.159307 See also  GNNGraph  for a homogeneous graph type and  rand_heterograph  for a function to generate random heterographs. source"},{"id":577,"pagetitle":"GNNHeteroGraph","title":"GNNGraphs.edge_type_subgraph","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/heterograph/#GNNGraphs.edge_type_subgraph-Tuple{GNNHeteroGraph, Tuple{Symbol, Symbol, Symbol}}","content":" GNNGraphs.edge_type_subgraph  —  Method edge_type_subgraph(g::GNNHeteroGraph, edge_ts) Return a subgraph of  g  that contains only the edges of type  edge_ts . Edge types can be specified as a single edge type (i.e. a tuple containing 3 symbols) or a vector of edge types. source"},{"id":578,"pagetitle":"GNNHeteroGraph","title":"GNNGraphs.num_edge_types","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/heterograph/#GNNGraphs.num_edge_types-Tuple{GNNGraph}","content":" GNNGraphs.num_edge_types  —  Method num_edge_types(g) Return the number of edge types in the graph. For  GNNGraph s, this is always 1. For  GNNHeteroGraph s, this is the number of unique edge types. source"},{"id":579,"pagetitle":"GNNHeteroGraph","title":"GNNGraphs.num_node_types","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/heterograph/#GNNGraphs.num_node_types-Tuple{GNNGraph}","content":" GNNGraphs.num_node_types  —  Method num_node_types(g) Return the number of node types in the graph. For  GNNGraph s, this is always 1. For  GNNHeteroGraph s, this is the number of unique node types. source"},{"id":580,"pagetitle":"GNNHeteroGraph","title":"Query","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/heterograph/#Query","content":" Query"},{"id":581,"pagetitle":"GNNHeteroGraph","title":"GNNGraphs.edge_index","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/heterograph/#GNNGraphs.edge_index-Tuple{GNNHeteroGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}, Tuple{Symbol, Symbol, Symbol}}","content":" GNNGraphs.edge_index  —  Method edge_index(g::GNNHeteroGraph, [edge_t]) Return a tuple containing two vectors, respectively storing the source and target nodes for each edges in  g  of type  edge_t = (src_t, rel_t, trg_t) . If  edge_t  is not provided, it will error if  g  has more than one edge type. source"},{"id":582,"pagetitle":"GNNHeteroGraph","title":"GNNGraphs.graph_indicator","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/heterograph/#GNNGraphs.graph_indicator-Tuple{GNNHeteroGraph}","content":" GNNGraphs.graph_indicator  —  Method graph_indicator(g::GNNHeteroGraph, [node_t]) Return a Dict of vectors containing the graph membership (an integer from  1  to  g.num_graphs ) of each node in the graph for each node type. If  node_t  is provided, return the graph membership of each node of type  node_t  instead. See also  batch . source"},{"id":583,"pagetitle":"GNNHeteroGraph","title":"Graphs.degree","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/heterograph/#Graphs.degree-Union{Tuple{TT}, Tuple{GNNHeteroGraph, Tuple{Symbol, Symbol, Symbol}}, Tuple{GNNHeteroGraph, Tuple{Symbol, Symbol, Symbol}, TT}} where TT<:Union{Nothing, Type{<:Number}}","content":" Graphs.degree  —  Method degree(g::GNNHeteroGraph, edge_type::EType; dir = :in) Return a vector containing the degrees of the nodes in  g  GNNHeteroGraph given  edge_type . Arguments g : A graph. edge_type : A tuple of symbols  (source_t, edge_t, target_t)  representing the edge type. T : Element type of the returned vector. If  nothing , is      chosen based on the graph type. Default  nothing . dir : For  dir = :out  the degree of a node is counted based on the outgoing edges.        For  dir = :in , the ingoing edges are used. If  dir = :both  we have the sum of the two.        Default  dir = :out . source"},{"id":584,"pagetitle":"GNNHeteroGraph","title":"Graphs.has_edge","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/heterograph/#Graphs.has_edge-Tuple{GNNHeteroGraph, Tuple{Symbol, Symbol, Symbol}, Integer, Integer}","content":" Graphs.has_edge  —  Method has_edge(g::GNNHeteroGraph, edge_t, i, j) Return  true  if there is an edge of type  edge_t  from node  i  to node  j  in  g . Examples julia> g = rand_bipartite_heterograph((2, 2), (4, 0), bidirected=false)\nGNNHeteroGraph:\n  num_nodes: Dict(:A => 2, :B => 2)\n  num_edges: Dict((:A, :to, :B) => 4, (:B, :to, :A) => 0)\n\njulia> has_edge(g, (:A,:to,:B), 1, 1)\ntrue\n\njulia> has_edge(g, (:B,:to,:A), 1, 1)\nfalse source"},{"id":585,"pagetitle":"GNNHeteroGraph","title":"Transform","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/heterograph/#Transform","content":" Transform"},{"id":586,"pagetitle":"GNNHeteroGraph","title":"GNNGraphs.add_edges","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/heterograph/#GNNGraphs.add_edges-Tuple{GNNHeteroGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}, Tuple{Symbol, Symbol, Symbol}, AbstractVector, AbstractVector}","content":" GNNGraphs.add_edges  —  Method add_edges(g::GNNHeteroGraph, edge_t, s, t; [edata, num_nodes])\nadd_edges(g::GNNHeteroGraph, edge_t => (s, t); [edata, num_nodes])\nadd_edges(g::GNNHeteroGraph, edge_t => (s, t, w); [edata, num_nodes]) Add to heterograph  g  edges of type  edge_t  with source node vector  s  and target node vector  t . Optionally, pass the  edge weights  w  or the features   edata  for the new edges.  edge_t  is a triplet of symbols  (src_t, rel_t, dst_t) .  If the edge type is not already present in the graph, it is added.  If it involves new node types, they are added to the graph as well. In this case, a dictionary or named tuple of  num_nodes  can be passed to specify the number of nodes of the new types, otherwise the number of nodes is inferred from the maximum node id in  s  and  t . source"},{"id":587,"pagetitle":"GNNHeteroGraph","title":"GNNGraphs.add_self_loops","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/heterograph/#GNNGraphs.add_self_loops-Tuple{GNNHeteroGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}, Tuple{Symbol, Symbol, Symbol}}","content":" GNNGraphs.add_self_loops  —  Method add_self_loops(g::GNNHeteroGraph, edge_t::EType)\nadd_self_loops(g::GNNHeteroGraph) If the source node type is the same as the destination node type in  edge_t , return a graph with the same features as  g  but also add self-loops  of the specified type,  edge_t . Otherwise, it returns  g  unchanged. Nodes with already existing self-loops of type  edge_t  will obtain  a second set of self-loops of the same type. If the graph has edge weights for edges of type  edge_t , the new edges will have weight 1. If no edges of type  edge_t  exist, or all existing edges have no weight,  then all new self loops will have no weight. If  edge_t  is not passed as argument, for the entire graph self-loop is added to each node for every edge type in the graph where the source and destination node types are the same.  This iterates over all edge types present in the graph, applying the self-loop addition logic to each applicable edge type. source"},{"id":588,"pagetitle":"GNNHeteroGraph","title":"Generate","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/heterograph/#Generate","content":" Generate"},{"id":589,"pagetitle":"GNNHeteroGraph","title":"GNNGraphs.rand_bipartite_heterograph","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/heterograph/#GNNGraphs.rand_bipartite_heterograph-Tuple{Any, Any}","content":" GNNGraphs.rand_bipartite_heterograph  —  Method rand_bipartite_heterograph([rng,] \n                           (n1, n2), (m12, m21); \n                           bidirected = true, \n                           node_t = (:A, :B), \n                           edge_t = :to, \n                           kws...) Construct an  GNNHeteroGraph  with random edges representing a bipartite graph. The graph will have two types of nodes, and edges will only connect nodes of different types. The first argument is a tuple  (n1, n2)  specifying the number of nodes of each type. The second argument is a tuple  (m12, m21)  specifying the number of edges connecting nodes of type  1  to nodes of type  2   and vice versa. The type of nodes and edges can be specified with the  node_t  and  edge_t  keyword arguments, which default to  (:A, :B)  and  :to  respectively. If  bidirected=true  (default), the reverse edge of each edge will be present. In this case  m12 == m21  is required. A random number generator can be passed as the first argument to make the generation reproducible. Additional keyword arguments will be passed to the  GNNHeteroGraph  constructor. See  rand_heterograph  for a more general version. Examples julia> g = rand_bipartite_heterograph((10, 15), 20)\nGNNHeteroGraph:\n  num_nodes: (:A => 10, :B => 15)\n  num_edges: ((:A, :to, :B) => 20, (:B, :to, :A) => 20)\n\njulia> g = rand_bipartite_heterograph((10, 15), (20, 0), node_t=(:user, :item), edge_t=:-, bidirected=false)\nGNNHeteroGraph:\n  num_nodes: Dict(:item => 15, :user => 10)\n  num_edges: Dict((:item, :-, :user) => 0, (:user, :-, :item) => 20) source"},{"id":590,"pagetitle":"GNNHeteroGraph","title":"GNNGraphs.rand_heterograph","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/heterograph/#GNNGraphs.rand_heterograph","content":" GNNGraphs.rand_heterograph  —  Function rand_heterograph([rng,] n, m; bidirected=false, kws...) Construct an  GNNHeteroGraph  with random edges and with number of nodes and edges  specified by  n  and  m  respectively.  n  and  m  can be any iterable of pairs specifing node/edge types and their numbers. Pass a random number generator as a first argument to make the generation reproducible. Setting  bidirected=true  will generate a bidirected graph, i.e. each edge will have a reverse edge. Therefore, for each edge type  (:A, :rel, :B)  a corresponding reverse edge type  (:B, :rel, :A)  will be generated. Additional keyword arguments will be passed to the  GNNHeteroGraph  constructor. Examples julia> g = rand_heterograph((:user => 10, :movie => 20),\n                            (:user, :rate, :movie) => 30)\nGNNHeteroGraph:\n  num_nodes: Dict(:movie => 20, :user => 10)\n  num_edges: Dict((:user, :rate, :movie) => 30) source"},{"id":593,"pagetitle":"Samplers","title":"Samplers","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/samplers/#Samplers","content":" Samplers"},{"id":594,"pagetitle":"Samplers","title":"GNNGraphs.NeighborLoader","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/samplers/#GNNGraphs.NeighborLoader","content":" GNNGraphs.NeighborLoader  —  Type NeighborLoader(graph; num_neighbors, input_nodes, num_layers, [batch_size]) A data structure for sampling neighbors from a graph for training Graph Neural Networks (GNNs).  It supports multi-layer sampling of neighbors for a batch of input nodes, useful for mini-batch training originally introduced in [\"Inductive Representation Learning on Large Graphs\"}(https://arxiv.org/abs/1706.02216) paper. Fields graph::GNNGraph : The input graph. num_neighbors::Vector{Int} : A vector specifying the number of neighbors to sample per node at each GNN layer. input_nodes::Vector{Int} : A vector containing the starting nodes for neighbor sampling. num_layers::Int : The number of layers for neighborhood expansion (how far to sample neighbors). batch_size::Union{Int, Nothing} : The size of the batch. If not specified, it defaults to the number of  input_nodes . Examples julia> loader = NeighborLoader(graph; num_neighbors=[10, 5], input_nodes=[1, 2, 3], num_layers=2)\n\njulia> batch_counter = 0\n\njulia> for mini_batch_gnn in loader\n            batch_counter += 1\n            println(\"Batch \", batch_counter, \": Nodes in mini-batch graph: \", nv(mini_batch_gnn))\n        end source"},{"id":597,"pagetitle":"TemporalSnapshotsGNNGraph","title":"Temporal Graphs","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/temporalgraph/#Temporal-Graphs","content":" Temporal Graphs"},{"id":598,"pagetitle":"TemporalSnapshotsGNNGraph","title":"TemporalSnapshotsGNNGraph","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/temporalgraph/#TemporalSnapshotsGNNGraph","content":" TemporalSnapshotsGNNGraph Documentation page for the graph type  TemporalSnapshotsGNNGraph  and related methods, representing time varying graphs with time varying features."},{"id":599,"pagetitle":"TemporalSnapshotsGNNGraph","title":"GNNGraphs.TemporalSnapshotsGNNGraph","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/temporalgraph/#GNNGraphs.TemporalSnapshotsGNNGraph","content":" GNNGraphs.TemporalSnapshotsGNNGraph  —  Type TemporalSnapshotsGNNGraph(snapshots) A type representing a time-varying graph as a sequence of snapshots, each snapshot being a  GNNGraph . The argument  snapshots  is a collection of  GNNGraph s with arbitrary  number of nodes and edges each.  Calling  tg  the temporal graph,  tg[t]  returns the  t -th snapshot. The snapshots can contain node/edge/graph features, while global features for the whole temporal sequence can be stored in  tg.tgdata . See  add_snapshot  and  remove_snapshot  for adding and removing snapshots. Examples julia> snapshots = [rand_graph(i , 2*i) for i in 10:10:50];\n\njulia> tg = TemporalSnapshotsGNNGraph(snapshots)\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 20, 30, 40, 50]\n  num_edges: [20, 40, 60, 80, 100]\n  num_snapshots: 5\n\njulia> tg.num_snapshots\n5\n\njulia> tg.num_nodes\n5-element Vector{Int64}:\n 10\n 20\n 30\n 40\n 50\n\njulia> tg[1]\nGNNGraph:\n  num_nodes: 10\n  num_edges: 20\n\njulia> tg[2:3]\nTemporalSnapshotsGNNGraph:\n  num_nodes: [20, 30]\n  num_edges: [40, 60]\n  num_snapshots: 2\n\njulia> tg[1] = rand_graph(10, 16)\nGNNGraph:\n  num_nodes: 10\n  num_edges: 16 source"},{"id":600,"pagetitle":"TemporalSnapshotsGNNGraph","title":"GNNGraphs.add_snapshot","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/temporalgraph/#GNNGraphs.add_snapshot-Tuple{TemporalSnapshotsGNNGraph, Int64, GNNGraph}","content":" GNNGraphs.add_snapshot  —  Method add_snapshot(tg::TemporalSnapshotsGNNGraph, t::Int, g::GNNGraph) Return a  TemporalSnapshotsGNNGraph  created starting from  tg  by adding the snapshot  g  at time index  t . Examples julia> snapshots = [rand_graph(10, 20) for i in 1:5];\n\njulia> tg = TemporalSnapshotsGNNGraph(snapshots)\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 10, 10, 10, 10]\n  num_edges: [20, 20, 20, 20, 20]\n  num_snapshots: 5\n\njulia> new_tg = add_snapshot(tg, 3, rand_graph(10, 16)) # add a new snapshot at time 3\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 10, 10, 10, 10, 10]\n  num_edges: [20, 20, 16, 20, 20, 20]\n  num_snapshots: 6 source"},{"id":601,"pagetitle":"TemporalSnapshotsGNNGraph","title":"GNNGraphs.remove_snapshot","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/temporalgraph/#GNNGraphs.remove_snapshot-Tuple{TemporalSnapshotsGNNGraph, Int64}","content":" GNNGraphs.remove_snapshot  —  Method remove_snapshot(tg::TemporalSnapshotsGNNGraph, t::Int) Return a  TemporalSnapshotsGNNGraph  created starting from  tg  by removing the snapshot at time index  t . Examples julia> using GNNGraphs\n\njulia> snapshots = [rand_graph(10,20), rand_graph(10,14), rand_graph(10,22)];\n\njulia> tg = TemporalSnapshotsGNNGraph(snapshots)\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 10, 10]\n  num_edges: [20, 14, 22]\n  num_snapshots: 3\n\njulia> new_tg = remove_snapshot(tg, 2) # remove snapshot at time 2\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 10]\n  num_edges: [20, 22]\n  num_snapshots: 2 source"},{"id":602,"pagetitle":"TemporalSnapshotsGNNGraph","title":"Random Generators","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/temporalgraph/#Random-Generators","content":" Random Generators"},{"id":603,"pagetitle":"TemporalSnapshotsGNNGraph","title":"GNNGraphs.rand_temporal_radius_graph","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/temporalgraph/#GNNGraphs.rand_temporal_radius_graph","content":" GNNGraphs.rand_temporal_radius_graph  —  Function rand_temporal_radius_graph(number_nodes::Int, \n                           number_snapshots::Int,\n                           speed::AbstractFloat,\n                           r::AbstractFloat;\n                           self_loops = false,\n                           dir = :in,\n                           kws...) Create a random temporal graph given  number_nodes  nodes and  number_snapshots  snapshots. First, the positions of the nodes are randomly generated in the unit square. Two nodes are connected if their distance is less than a given radius  r . Each following snapshot is obtained by applying the same construction to new positions obtained as follows. For each snapshot, the new positions of the points are determined by applying random independent displacement vectors to the previous positions. The direction of the displacement is chosen uniformly at random and its length is chosen uniformly in  [0, speed] . Then the connections are recomputed. If a point happens to move outside the boundary, its position is updated as if it had bounced off the boundary. Arguments number_nodes : The number of nodes of each snapshot. number_snapshots : The number of snapshots. speed : The speed to update the nodes. r : The radius of connection. self_loops : If  true , consider the node itself among its neighbors, in which               case the graph will contain self-loops.  dir : The direction of the edges. If  dir=:in  edges go from the        neighbors to the central node. If  dir=:out  we have the opposite        direction. kws : Further keyword arguments will be passed to the  GNNGraph  constructor of each snapshot. Example julia> n, snaps, s, r = 10, 5, 0.1, 1.5;\n\njulia> tg = rand_temporal_radius_graph(n,snaps,s,r) # complete graph at each snapshot\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 10, 10, 10, 10]\n  num_edges: [90, 90, 90, 90, 90]\n  num_snapshots: 5 source"},{"id":604,"pagetitle":"TemporalSnapshotsGNNGraph","title":"GNNGraphs.rand_temporal_hyperbolic_graph","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/api/temporalgraph/#GNNGraphs.rand_temporal_hyperbolic_graph","content":" GNNGraphs.rand_temporal_hyperbolic_graph  —  Function rand_temporal_hyperbolic_graph(number_nodes::Int, \n                               number_snapshots::Int;\n                               α::Real,\n                               R::Real,\n                               speed::Real,\n                               ζ::Real=1,\n                               self_loop = false,\n                               kws...) Create a random temporal graph given  number_nodes  nodes and  number_snapshots  snapshots. First, the positions of the nodes are generated with a quasi-uniform distribution (depending on the parameter  α ) in hyperbolic space within a disk of radius  R . Two nodes are connected if their hyperbolic distance is less than  R . Each following snapshot is created in order to keep the same initial distribution. Arguments number_nodes : The number of nodes of each snapshot. number_snapshots : The number of snapshots. α : The parameter that controls the position of the points. If  α=ζ , the points are uniformly distributed on the disk of radius  R . If  α>ζ , the points are more concentrated in the center of the disk. If  α<ζ , the points are more concentrated at the boundary of the disk. R : The radius of the disk and of connection. speed : The speed to update the nodes. ζ : The parameter that controls the curvature of the disk. self_loops : If  true , consider the node itself among its neighbors, in which               case the graph will contain self-loops. kws : Further keyword arguments will be passed to the  GNNGraph  constructor of each snapshot. Example julia> n, snaps, α, R, speed, ζ = 10, 5, 1.0, 4.0, 0.1, 1.0;\n\njulia> thg = rand_temporal_hyperbolic_graph(n, snaps; α, R, speed, ζ)\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 10, 10, 10, 10]\n  num_edges: [44, 46, 48, 42, 38]\n  num_snapshots: 5 References Section D of the paper  Dynamic Hidden-Variable Network Models  and the paper   Hyperbolic Geometry of Complex Networks source"},{"id":607,"pagetitle":"Datasets","title":"Datasets","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/guides/datasets/#Datasets","content":" Datasets GNNGraphs.jl doesn't come with its own datasets, but leverages those available in the Julia (and non-Julia) ecosystem. In particular, the  examples in the GraphNeuralNetworks.jl repository  make use of the  MLDatasets.jl  package. There you will find common graph datasets such as Cora, PubMed, Citeseer, TUDataset and  many others . For graphs with static structures and temporal features, datasets such as METRLA, PEMSBAY, ChickenPox, and WindMillEnergy are available. For graphs featuring both temporal structures and temporal features, the TemporalBrains dataset is suitable. GraphNeuralNetworks.jl provides the  mldataset2gnngraph  method for interfacing with MLDatasets.jl."},{"id":610,"pagetitle":"Graphs","title":"Static Graphs","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/guides/gnngraph/#Static-Graphs","content":" Static Graphs The fundamental graph type in GNNGraphs.jl is the  GNNGraph . A GNNGraph  g  is a directed graph with nodes labeled from 1 to  g.num_nodes . The underlying implementation allows for efficient application of graph neural network operators, gpu movement, and storage of node/edge/graph related feature arrays. GNNGraph  inherits from  Graphs.jl 's  AbstractGraph , therefore it supports most functionality from that library. "},{"id":611,"pagetitle":"Graphs","title":"Graph Creation","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/guides/gnngraph/#Graph-Creation","content":" Graph Creation A GNNGraph can be created from several different data sources encoding the graph topology: using GNNGraphs, Graphs, SparseArrays\n\n\n# Construct a GNNGraph from from a Graphs.jl's graph\nlg = erdos_renyi(10, 30)\ng = GNNGraph(lg)\n\n# Same as above using convenience method rand_graph\ng = rand_graph(10, 60)\n\n# From an adjacency matrix\nA = sprand(10, 10, 0.3)\ng = GNNGraph(A)\n\n# From an adjacency list\nadjlist = [[2,3], [1,3], [1,2,4], [3]]\ng = GNNGraph(adjlist)\n\n# From COO representation\nsource = [1,1,2,2,3,3,3,4]\ntarget = [2,3,1,3,1,2,4,3]\ng = GNNGraph(source, target) See also the related methods  Graphs.adjacency_matrix ,  edge_index , and  adjacency_list ."},{"id":612,"pagetitle":"Graphs","title":"Basic Queries","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/guides/gnngraph/#Basic-Queries","content":" Basic Queries julia> source = [1,1,2,2,3,3,3,4];\n\njulia> target = [2,3,1,3,1,2,4,3];\n\njulia> g = GNNGraph(source, target)\nGNNGraph:\n  num_nodes: 4\n  num_edges: 8\n\n\njulia> @assert g.num_nodes == 4   # number of nodes\n\njulia> @assert g.num_edges == 8   # number of edges\n\njulia> @assert g.num_graphs == 1  # number of subgraphs (a GNNGraph can batch many graphs together)\n\njulia> is_directed(g)      # a GNNGraph is always directed\ntrue\n\njulia> is_bidirected(g)      # for each edge, also the reverse edge is present\ntrue\n\njulia> has_self_loops(g)\nfalse\n\njulia> has_multi_edges(g)      \nfalse"},{"id":613,"pagetitle":"Graphs","title":"Data Features","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/guides/gnngraph/#Data-Features","content":" Data Features One or more arrays can be associated to nodes, edges, and (sub)graphs of a  GNNGraph . They will be stored in the fields  g.ndata ,  g.edata , and  g.gdata  respectively. The data fields are  DataStore  objects.  DataStore s conveniently offer an interface similar to both dictionaries and named tuples. Similarly to dictionaries, DataStores support addition of new features after creation time. The array contained in the datastores have last dimension equal to  num_nodes  (in  ndata ),  num_edges  (in  edata ), or  num_graphs  (in  gdata ) respectively. # Create a graph with a single feature array `x` associated to nodes\ng = rand_graph(10,  60, ndata = (; x = rand(Float32, 32, 10)))\n\ng.ndata.x  # access the features\n\n# Equivalent definition passing directly the array\ng = rand_graph(10,  60, ndata = rand(Float32, 32, 10))\n\ng.ndata.x  # `:x` is the default name for node features\n\ng.ndata.z = rand(Float32, 3, 10)  # add new feature array `z`\n\n# For convenience, we can access the features through the shortcut\ng.x \n\n# You can have multiple feature arrays\ng = rand_graph(10,  60, ndata = (; x=rand(Float32, 32, 10), y=rand(Float32, 10)))\n\ng.ndata.y, g.ndata.x   # or g.x, g.y\n\n# Attach an array with edge features.\n# Since `GNNGraph`s are directed, the number of edges\n# will be double that of the original Graphs' undirected graph.\ng = GNNGraph(erdos_renyi(10,  30), edata = rand(Float32, 60))\n@assert g.num_edges == 60\n\ng.edata.e  # or g.e\n\n# If we pass only half of the edge features, they will be copied\n# on the reversed edges.\ng = GNNGraph(erdos_renyi(10,  30), edata = rand(Float32, 30))\n\n\n# Create a new graph from previous one, inheriting edge data\n# but replacing node data\ng′ = GNNGraph(g, ndata =(; z = ones(Float32, 16, 10)))\n\ng′.z\ng′.e"},{"id":614,"pagetitle":"Graphs","title":"Edge weights","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/guides/gnngraph/#Edge-weights","content":" Edge weights It is common to denote scalar edge features as edge weights. The  GNNGraph  has specific support for edge weights: they can be stored as part of internal representations of the graph (COO or adjacency matrix). Some graph convolutional layers, most notably the  GCNConv , can use the edge weights to perform weighted sums over the nodes' neighborhoods. julia> source = [1, 1, 2, 2, 3, 3];\n\njulia> target = [2, 3, 1, 3, 1, 2];\n\njulia> weight = [1.0, 0.5, 2.1, 2.3, 4, 4.1];\n\njulia> g = GNNGraph(source, target, weight)\nGNNGraph:\n  num_nodes: 3\n  num_edges: 6\n\njulia> get_edge_weight(g)\n6-element Vector{Float64}:\n 1.0\n 0.5\n 2.1\n 2.3\n 4.0\n 4.1"},{"id":615,"pagetitle":"Graphs","title":"Batches and Subgraphs","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/guides/gnngraph/#Batches-and-Subgraphs","content":" Batches and Subgraphs Multiple  GNNGraph s can be batched together into a single graph that contains the total number of the original nodes  and where the original graphs are disjoint subgraphs. using MLUtils\n\ndata = [rand_graph(10, 30, ndata=rand(Float32, 3, 10)) for _ in 1:160]\ngall = MLUtils.batch(data)\n\n# gall is a GNNGraph containing many graphs\n@assert gall.num_graphs == 160 \n@assert gall.num_nodes == 1600   # 10 nodes x 160 graphs\n@assert gall.num_edges == 4800  # 30 undirected edges x 160 graphs\n\n# Let's create a mini-batch from gall\ng23 = getgraph(gall, 2:3)\n@assert g23.num_graphs == 2\n@assert g23.num_nodes == 20   # 10 nodes x 2 graphs\n@assert g23.num_edges == 60  # 30 undirected edges X 2 graphs\n\n# We can pass a GNNGraph to MLUtils' DataLoader\ntrain_loader = DataLoader(gall, batchsize=16, shuffle=true)\n\nfor g in train_loader\n    @assert g.num_graphs == 16\n    @assert g.num_nodes == 160\n    @assert size(g.ndata.x) = (3, 160)    \n    # .....\nend\n\n# Access the nodes' graph memberships \ngraph_indicator(gall)"},{"id":616,"pagetitle":"Graphs","title":"DataLoader and mini-batch iteration","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/guides/gnngraph/#DataLoader-and-mini-batch-iteration","content":" DataLoader and mini-batch iteration While constructing a batched graph and passing it to the  DataLoader  is always  an option for mini-batch iteration, the recommended way for better performance is to pass an array of graphs directly and set the  collate  option to  true : using MLUtils: DataLoader\n\ndata = [rand_graph(10, 30, ndata=rand(Float32, 3, 10)) for _ in 1:320]\n\ntrain_loader = DataLoader(data, batchsize=16, shuffle=true, collate=true)\n\nfor g in train_loader\n    @assert g.num_graphs == 16\n    @assert g.num_nodes == 160\n    @assert size(g.ndata.x) = (3, 160)    \n    # .....\nend"},{"id":617,"pagetitle":"Graphs","title":"Graph Manipulation","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/guides/gnngraph/#Graph-Manipulation","content":" Graph Manipulation g′ = add_self_loops(g)\ng′ = remove_self_loops(g)\ng′ = add_edges(g, [1, 2], [2, 3]) # add edges 1->2 and 2->3"},{"id":618,"pagetitle":"Graphs","title":"GPU movement","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/guides/gnngraph/#GPU-movement","content":" GPU movement Move a  GNNGraph  to a CUDA device using  Flux.gpu  method.  using Flux, CUDA # or using Metal or using AMDGPU \n\ng_gpu = g |> Flux.gpu"},{"id":619,"pagetitle":"Graphs","title":"Integration with Graphs.jl","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/guides/gnngraph/#Integration-with-Graphs.jl","content":" Integration with Graphs.jl Since  GNNGraph <: Graphs.AbstractGraph , we can use any functionality from  Graphs.jl  for querying and analyzing the graph structure.  Moreover, a  GNNGraph  can be easily constructed from a  Graphs.Graph  or a  Graphs.DiGraph : julia> import Graphs\n\njulia> using GNNGraphs\n\n# A Graphs.jl undirected graph\njulia> gu = Graphs.erdos_renyi(10, 20)    \n{10, 20} undirected simple Int64 graph\n\n# Since GNNGraphs are undirected, the edges are doubled when converting \n# to GNNGraph\njulia> GNNGraph(gu)\nGNNGraph:\n  num_nodes: 10\n  num_edges: 40\n\n# A Graphs.jl directed graph\njulia> gd = Graphs.erdos_renyi(10, 20, is_directed=true)\n{10, 20} directed simple Int64 graph\n\njulia> GNNGraph(gd)\nGNNGraph:\n  num_nodes: 10\n  num_edges: 20"},{"id":622,"pagetitle":"Heterogeneous Graphs","title":"Heterogeneous Graphs","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/guides/heterograph/#Heterogeneous-Graphs","content":" Heterogeneous Graphs Heterogeneous graphs (also called heterographs), are graphs where each node has a type, that we denote with symbols such as  :user  and  :movie . Relations such as  :rate  or  :like  can connect nodes of different types. We call a triplet  (source_node_type, relation_type, target_node_type)  the type of a edge, e.g.  (:user, :rate, :movie) . Different node/edge types can store different groups of features and this makes heterographs a very flexible modeling tools  and data containers. In GNNGraphs.jl heterographs are implemented in  the type  GNNHeteroGraph ."},{"id":623,"pagetitle":"Heterogeneous Graphs","title":"Creating a Heterograph","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/guides/heterograph/#Creating-a-Heterograph","content":" Creating a Heterograph A heterograph can be created empty or by passing pairs  edge_type => data  to the constructor. julia> using GNNGraphs\n\njulia> g = GNNHeteroGraph()\nGNNHeteroGraph:\n  num_nodes: Dict()\n  num_edges: Dict()\n  \njulia> g = GNNHeteroGraph((:user, :like, :actor) => ([1,2,2,3], [1,3,2,9]),\n                          (:user, :rate, :movie) => ([1,1,2,3], [7,13,5,7]))\nGNNHeteroGraph:\n  num_nodes: Dict(:actor => 9, :movie => 13, :user => 3)\n  num_edges: Dict((:user, :like, :actor) => 4, (:user, :rate, :movie) => 4)\n\njulia> g = GNNHeteroGraph((:user, :rate, :movie) => ([1,1,2,3], [7,13,5,7]))\nGNNHeteroGraph:\n  num_nodes: Dict(:movie => 13, :user => 3)\n  num_edges: Dict((:user, :rate, :movie) => 4) New relations, possibly with new node types, can be added with the function  add_edges . julia> g = add_edges(g, (:user, :like, :actor) => ([1,2,3,3,3], [3,5,1,9,4]))\nGNNHeteroGraph:\n  num_nodes: Dict(:actor => 9, :movie => 13, :user => 3)\n  num_edges: Dict((:user, :like, :actor) => 5, (:user, :rate, :movie) => 4) See  rand_heterograph ,  rand_bipartite_heterograph  for generating random heterographs.  julia> g = rand_bipartite_heterograph((10, 15), 20)\nGNNHeteroGraph:\n  num_nodes: Dict(:A => 10, :B => 15)\n  num_edges: Dict((:A, :to, :B) => 20, (:B, :to, :A) => 20)"},{"id":624,"pagetitle":"Heterogeneous Graphs","title":"Basic Queries","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/guides/heterograph/#Basic-Queries","content":" Basic Queries Basic queries are similar to those for homogeneous graphs: julia> g = GNNHeteroGraph((:user, :rate, :movie) => ([1,1,2,3], [7,13,5,7]))\nGNNHeteroGraph:\n  num_nodes: Dict(:movie => 13, :user => 3)\n  num_edges: Dict((:user, :rate, :movie) => 4)\n\njulia> g.num_nodes\nDict{Symbol, Int64} with 2 entries:\n  :user  => 3\n  :movie => 13\n\njulia> g.num_edges\nDict{Tuple{Symbol, Symbol, Symbol}, Int64} with 1 entry:\n  (:user, :rate, :movie) => 4\n\njulia> edge_index(g, (:user, :rate, :movie)) # source and target node for a given relation\n([1, 1, 2, 3], [7, 13, 5, 7])\n\njulia> g.ntypes  # node types\n2-element Vector{Symbol}:\n :user\n :movie\n\njulia> g.etypes  # edge types\n1-element Vector{Tuple{Symbol, Symbol, Symbol}}:\n (:user, :rate, :movie)"},{"id":625,"pagetitle":"Heterogeneous Graphs","title":"Data Features","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/guides/heterograph/#Data-Features","content":" Data Features Node, edge, and graph features can be added at construction time or later using: # equivalent to g.ndata[:user][:x] = ...\njulia> g[:user].x = rand(Float32, 64, 3);\n\njulia> g[:movie].z = rand(Float32, 64, 13);\n\n# equivalent to g.edata[(:user, :rate, :movie)][:e] = ...\njulia> g[:user, :rate, :movie].e = rand(Float32, 64, 4);\n\njulia> g\nGNNHeteroGraph:\n  num_nodes: Dict(:movie => 13, :user => 3)\n  num_edges: Dict((:user, :rate, :movie) => 4)\n  ndata:\n        :movie  =>  DataStore(z = [64×13 Matrix{Float32}])\n        :user  =>  DataStore(x = [64×3 Matrix{Float32}])\n  edata:\n        (:user, :rate, :movie)  =>  DataStore(e = [64×4 Matrix{Float32}])"},{"id":626,"pagetitle":"Heterogeneous Graphs","title":"Batching","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/guides/heterograph/#Batching","content":" Batching Similarly to graphs, also heterographs can be batched together. julia> gs = [rand_bipartite_heterograph((5, 10), 20) for _ in 1:32];\n\njulia> MLUtils.batch(gs)\nGNNHeteroGraph:\n  num_nodes: Dict(:A => 160, :B => 320)\n  num_edges: Dict((:A, :to, :B) => 640, (:B, :to, :A) => 640)\n  num_graphs: 32 Batching is automatically performed by the  DataLoader  iterator when the  collate  option is set to  true . using MLUtils: DataLoader\n\ndata = [rand_bipartite_heterograph((5, 10), 20, \n            ndata=Dict(:A=>rand(Float32, 3, 5))) \n        for _ in 1:320];\n\ntrain_loader = DataLoader(data, batchsize=16, shuffle=true, collate=true)\n\nfor g in train_loader\n    @assert g.num_graphs == 16\n    @assert g.num_nodes[:A] == 80\n    @assert size(g.ndata[:A].x) == (3, 80)    \n    # ...\nend"},{"id":627,"pagetitle":"Heterogeneous Graphs","title":"Graph convolutions on heterographs","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/guides/heterograph/#Graph-convolutions-on-heterographs","content":" Graph convolutions on heterographs See  HeteroGraphConv  for how to perform convolutions on heterogeneous graphs."},{"id":630,"pagetitle":"Temporal Graphs","title":"Temporal Graphs","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/guides/temporalgraph/#Temporal-Graphs","content":" Temporal Graphs Temporal graphs are graphs with time-varying topologies and features. In GNNGraphs.jl, they are represented by the  TemporalSnapshotsGNNGraph  type."},{"id":631,"pagetitle":"Temporal Graphs","title":"Creating a TemporalSnapshotsGNNGraph","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/guides/temporalgraph/#Creating-a-TemporalSnapshotsGNNGraph","content":" Creating a TemporalSnapshotsGNNGraph A temporal graph can be created by passing a list of snapshots to the constructor. Each snapshot is a  GNNGraph .  julia> using GNNGraphs\n\njulia> snapshots = [rand_graph(10, 20) for i in 1:5];\n\njulia> tg = TemporalSnapshotsGNNGraph(snapshots)\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 10, 10, 10, 10]\n  num_edges: [20, 20, 20, 20, 20]\n  num_snapshots: 5 A new temporal graph can be created by adding or removing snapshots to an existing temporal graph.  julia> new_tg = add_snapshot(tg, 3, rand_graph(10, 16)) # add a new snapshot at time 3\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 10, 10, 10, 10, 10]\n  num_edges: [20, 20, 16, 20, 20, 20]\n  num_snapshots: 6 julia> snapshots = [rand_graph(10,20), rand_graph(10,14), rand_graph(10,22)];\n\njulia> tg = TemporalSnapshotsGNNGraph(snapshots)\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 10, 10]\n  num_edges: [20, 14, 22]\n  num_snapshots: 3\n\njulia> new_tg = remove_snapshot(tg, 2) # remove snapshot at time 2\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 10]\n  num_edges: [20, 22]\n  num_snapshots: 2 See  rand_temporal_radius_graph  and  rand_temporal_hyperbolic_graph  for generating random temporal graphs.  julia> tg = rand_temporal_radius_graph(10, 3, 0.1, 0.5)\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 10, 10]\n  num_edges: [32, 30, 34]\n  num_snapshots: 3"},{"id":632,"pagetitle":"Temporal Graphs","title":"Indexing","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/guides/temporalgraph/#Indexing","content":" Indexing Snapshots in a temporal graph can be accessed using indexing: julia> snapshots = [rand_graph(10, 20), rand_graph(10, 14), rand_graph(10, 22)];\n\njulia> tg = TemporalSnapshotsGNNGraph(snapshots)\n\njulia> tg[1] # first snapshot\nGNNGraph:\n  num_nodes: 10\n  num_edges: 20\n\njulia> tg[2:3] # snapshots 2 and 3\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 10]\n  num_edges: [14, 22]\n  num_snapshots: 2 A snapshot can be modified by assigning a new snapshot to the temporal graph: julia> tg[1] = rand_graph(10, 16) # replace first snapshot\nGNNGraph:\n  num_nodes: 10\n  num_edges: 16"},{"id":633,"pagetitle":"Temporal Graphs","title":"Iteration and Broadcasting","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/guides/temporalgraph/#Iteration-and-Broadcasting","content":" Iteration and Broadcasting Iteration and broadcasting over a temporal graph is similar to that of a vector of snapshots: julia> snapshots = [rand_graph(10, 20), rand_graph(10, 14), rand_graph(10, 22)];\n\njulia> tg = TemporalSnapshotsGNNGraph(snapshots);\n\njulia> [g for g in tg] # iterate over snapshots\n3-element Vector{GNNGraph{Tuple{Vector{Int64}, Vector{Int64}, Nothing}}}:\n GNNGraph(10, 20) with no data\n GNNGraph(10, 14) with no data\n GNNGraph(10, 22) with no data\n\njulia> f(g) = g isa GNNGraph;\n\njulia> f.(tg) # broadcast over snapshots\n3-element BitVector:\n 1\n 1\n 1"},{"id":634,"pagetitle":"Temporal Graphs","title":"Basic Queries","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/guides/temporalgraph/#Basic-Queries","content":" Basic Queries Basic queries are similar to those for  GNNGraph s: julia> snapshots = [rand_graph(10,20), rand_graph(12,14), rand_graph(14,22)];\n\njulia> tg = TemporalSnapshotsGNNGraph(snapshots)\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 12, 14]\n  num_edges: [20, 14, 22]\n  num_snapshots: 3\n\njulia> tg.num_nodes         # number of nodes in each snapshot\n3-element Vector{Int64}:\n 10\n 12\n 14\n\njulia> tg.num_edges         # number of edges in each snapshot\n3-element Vector{Int64}:\n 20\n 14\n 22\n\njulia> tg.num_snapshots     # number of snapshots\n3\n\njulia> tg.snapshots         # list of snapshots\n3-element Vector{GNNGraph{Tuple{Vector{Int64}, Vector{Int64}, Nothing}}}:\n GNNGraph(10, 20) with no data\n GNNGraph(12, 14) with no data\n GNNGraph(14, 22) with no data\n\njulia> tg.snapshots[1]      # first snapshot, same as tg[1]\nGNNGraph:\n  num_nodes: 10\n  num_edges: 20"},{"id":635,"pagetitle":"Temporal Graphs","title":"Data Features","ref":"/GraphNeuralNetworks.jl/docs/GNNGraphs.jl/stable/guides/temporalgraph/#Data-Features","content":" Data Features A temporal graph can store global feature for the entire time series in the  tgdata  field. Also, each snapshot can store node, edge, and graph features in the  ndata ,  edata , and  gdata  fields, respectively.  julia> snapshots = [rand_graph(10, 20; ndata = rand(Float32, 3, 10)), \n                    rand_graph(10, 14; ndata = rand(Float32, 4, 10)), \n                    rand_graph(10, 22; ndata = rand(Float32, 5, 10))]; # node features at construction time\n\njulia> tg = TemporalSnapshotsGNNGraph(snapshots);\n\njulia> tg.tgdata.y = rand(Float32, 3, 1); # add global features after construction\n\njulia> tg\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 10, 10]\n  num_edges: [20, 14, 22]\n  num_snapshots: 3\n  tgdata:\n        y = 3×1 Matrix{Float32}\n\njulia> tg.ndata # vector of DataStore containing node features for each snapshot\n3-element Vector{DataStore}:\n DataStore(10) with 1 element:\n  x = 3×10 Matrix{Float32}\n DataStore(10) with 1 element:\n  x = 4×10 Matrix{Float32}\n DataStore(10) with 1 element:\n  x = 5×10 Matrix{Float32}\n\njulia> [ds.x for ds in tg.ndata]; # vector containing the x feature of each snapshot\n\njulia> [g.x for g in tg.snapshots]; # same vector as above, now accessing \n                                   # the x feature directly from the snapshots"},{"id":638,"pagetitle":"Home","title":"GNNlib.jl","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/#GNNlib.jl","content":" GNNlib.jl GNNlib.jl is a package that provides the implementation of the basic message passing functions and  functional implementation of graph convolutional layers, which are used to build graph neural networks in both the  Flux.jl  and  Lux.jl  machine learning frameworks, created in the GraphNeuralNetworks.jl and GNNLux.jl packages, respectively. This package depends on GNNGraphs.jl and NNlib.jl, and is primarily intended for developers looking to create new GNN architectures. For most users, the higher-level GraphNeuralNetworks.jl and GNNLux.jl packages are recommended."},{"id":639,"pagetitle":"Home","title":"Installation","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/#Installation","content":" Installation The package can be installed with the Julia package manager. From the Julia REPL, type  ]  to enter the Pkg REPL mode and run: pkg> add GNNlib"},{"id":642,"pagetitle":"GNNGraphs.jl","title":"GNNGraphs.jl","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/#GNNGraphs.jl","content":" GNNGraphs.jl GNNGraphs.jl is a package that provides graph data structures and helper functions specifically designed for working with graph neural networks. This package allows to store not only the graph structure, but also features associated with nodes, edges, and the graph itself. It is the core foundation for the GNNlib.jl, GraphNeuralNetworks.jl, and GNNLux.jl packages. It supports three types of graphs:  Static graph  is the basic graph type represented by  GNNGraph , where each node and edge can have associated features. This type of graph is used in typical graph neural network applications, where neural networks operate on both the structure of the graph and the features stored in it. It can be used to represent a graph where the structure does not change over time, but the features of the nodes and edges can change over time. Heterogeneous graph  is a graph that supports multiple types of nodes and edges, and is represented by  GNNHeteroGraph . Each type can have its own properties and relationships. This is useful in scenarios with different entities and interactions, such as in citation graphs or multi-relational data. Temporal graph  is a graph that changes over time, and is represented by  TemporalSnapshotsGNNGraph . Edges and features can change dynamically. This type of graph is useful for applications that involve tracking time-dependent relationships, such as social networks. This package depends on the package  Graphs.jl ."},{"id":643,"pagetitle":"GNNGraphs.jl","title":"Installation","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/#Installation","content":" Installation The package can be installed with the Julia package manager. From the Julia REPL, type  ]  to enter the Pkg REPL mode and run: pkg> add GNNGraphs"},{"id":646,"pagetitle":"Datasets","title":"Datasets","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/datasets/#Datasets","content":" Datasets"},{"id":647,"pagetitle":"Datasets","title":"GNNGraphs.mldataset2gnngraph","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/datasets/#GNNGraphs.mldataset2gnngraph","content":" GNNGraphs.mldataset2gnngraph  —  Function mldataset2gnngraph(dataset) Convert a graph dataset from the package MLDatasets.jl into one or many  GNNGraph s. Examples julia> using MLDatasets, GNNGraphs\n\njulia> mldataset2gnngraph(Cora())\nGNNGraph:\n  num_nodes: 2708\n  num_edges: 10556\n  ndata:\n        val_mask = 2708-element BitVector\n        targets = 2708-element Vector{Int64}\n        test_mask = 2708-element BitVector\n        features = 1433×2708 Matrix{Float32}\n        train_mask = 2708-element BitVector source"},{"id":650,"pagetitle":"GNNGraph","title":"GNNGraph","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/gnngraph/#GNNGraph","content":" GNNGraph Documentation page for the graph type  GNNGraph  provided by GNNGraphs.jl and related methods.  Besides the methods documented here, one can rely on the large set of functionalities given by  Graphs.jl  thanks to the fact that  GNNGraph  inherits from  Graphs.AbstractGraph ."},{"id":651,"pagetitle":"GNNGraph","title":"GNNGraph type","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/gnngraph/#GNNGraph-type","content":" GNNGraph type"},{"id":652,"pagetitle":"GNNGraph","title":"GNNGraphs.GNNGraph","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.GNNGraph","content":" GNNGraphs.GNNGraph  —  Type GNNGraph(data; [graph_type, ndata, edata, gdata, num_nodes, graph_indicator, dir])\nGNNGraph(g::GNNGraph; [ndata, edata, gdata]) A type representing a graph structure that also stores feature arrays associated to nodes, edges, and the graph itself. The feature arrays are stored in the fields  ndata ,  edata , and  gdata  as  DataStore  objects offering a convenient dictionary-like  and namedtuple-like interface. The features can be passed at construction time or added later. A  GNNGraph  can be constructed out of different  data  objects expressing the connections inside the graph. The internal representation type is determined by  graph_type . When constructed from another  GNNGraph , the internal graph representation is preserved and shared. The node/edge/graph features are retained as well, unless explicitely set by the keyword arguments  ndata ,  edata , and  gdata . A  GNNGraph  can also represent multiple graphs batched togheter (see  MLUtils.batch  or  SparseArrays.blockdiag ). The field  g.graph_indicator  contains the graph membership of each node. GNNGraph s are always directed graphs, therefore each edge is defined by a source node and a target node (see  edge_index ). Self loops (edges connecting a node to itself) and multiple edges (more than one edge between the same pair of nodes) are supported. A  GNNGraph  is a Graphs.jl's  AbstractGraph , therefore it supports most functionality from that library. Arguments data : Some data representing the graph topology. Possible type are An adjacency matrix An adjacency list. A tuple containing the source and target vectors (COO representation) A Graphs.jl' graph. graph_type : A keyword argument that specifies               the underlying representation used by the GNNGraph.               Currently supported values are :coo . Graph represented as a tuple  (source, target) , such that the  k -th edge         connects the node  source[k]  to node  target[k] .         Optionally, also edge weights can be given:  (source, target, weights) . :sparse . A sparse adjacency matrix representation. :dense . A dense adjacency matrix representation. Defaults to  :coo , currently the most supported type. dir : The assumed edge direction when given adjacency matrix or adjacency list input data  g .       Possible values are  :out  and  :in . Default  :out . num_nodes : The number of nodes. If not specified, inferred from  g . Default  nothing . graph_indicator : For batched graphs, a vector containing the graph assignment of each node. Default  nothing . ndata : Node features. An array or named tuple of arrays whose last dimension has size  num_nodes . edata : Edge features. An array or named tuple of arrays whose last dimension has size  num_edges . gdata : Graph features. An array or named tuple of arrays whose last dimension has size  num_graphs . Examples using GNNGraphs\n\n# Construct from adjacency list representation\ndata = [[2,3], [1,4,5], [1], [2,5], [2,4]]\ng = GNNGraph(data)\n\n# Number of nodes, edges, and batched graphs\ng.num_nodes  # 5\ng.num_edges  # 10\ng.num_graphs # 1\n\n# Same graph in COO representation\ns = [1,1,2,2,2,3,4,4,5,5]\nt = [2,3,1,4,5,3,2,5,2,4]\ng = GNNGraph(s, t)\n\n# From a Graphs' graph\ng = GNNGraph(erdos_renyi(100, 20))\n\n# Add 2 node feature arrays at creation time\ng = GNNGraph(g, ndata = (x=rand(100, g.num_nodes), y=rand(g.num_nodes)))\n\n# Add 1 edge feature array, after the graph creation\ng.edata.z = rand(16, g.num_edges)\n\n# Add node features and edge features with default names `x` and `e`\ng = GNNGraph(g, ndata = rand(100, g.num_nodes), edata = rand(16, g.num_edges))\n\ng.ndata.x # or just g.x\ng.edata.e # or just g.e\n\n# Collect edges' source and target nodes.\n# Both source and target are vectors of length num_edges\nsource, target = edge_index(g) A  GNNGraph  can be sent to the GPU, for example by using Flux.jl's  gpu  function or MLDataDevices.jl's utilities.  ``` source"},{"id":653,"pagetitle":"GNNGraph","title":"Base.copy","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/gnngraph/#Base.copy","content":" Base.copy  —  Function copy(g::GNNGraph; deep=false) Create a copy of  g . If  deep  is  true , then copy will be a deep copy (equivalent to  deepcopy(g) ), otherwise it will be a shallow copy with the same underlying graph data. source"},{"id":654,"pagetitle":"GNNGraph","title":"DataStore","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/gnngraph/#DataStore","content":" DataStore"},{"id":655,"pagetitle":"GNNGraph","title":"GNNGraphs.DataStore","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.DataStore","content":" GNNGraphs.DataStore  —  Type DataStore([n, data])\nDataStore([n,] k1 = x1, k2 = x2, ...) A container for feature arrays. The optional argument  n  enforces that  numobs(x) == n  for each array contained in the datastore. At construction time, the  data  can be provided as any iterables of pairs of symbols and arrays or as keyword arguments: julia> ds = DataStore(3, x = rand(Float32, 2, 3), y = rand(Float32, 3))\nDataStore(3) with 2 elements:\n  y = 3-element Vector{Float32}\n  x = 2×3 Matrix{Float32}\n\njulia> ds = DataStore(3, Dict(:x => rand(Float32, 2, 3), :y => rand(Float32, 3))); # equivalent to above The  DataStore  has an interface similar to both dictionaries and named tuples. Arrays can be accessed and added using either the indexing or the property syntax: julia> ds = DataStore(x = ones(Float32, 2, 3), y = zeros(Float32, 3))\nDataStore() with 2 elements:\n  y = 3-element Vector{Float32}\n  x = 2×3 Matrix{Float32}\n\njulia> ds.x   # same as `ds[:x]`\n2×3 Matrix{Float32}:\n 1.0  1.0  1.0\n 1.0  1.0  1.0\n\njulia> ds.z = zeros(Float32, 3)  # Add new feature array `z`. Same as `ds[:z] = rand(Float32, 3)`\n3-element Vector{Float32}:\n 0.0\n 0.0\n 0.0 The  DataStore  can be iterated over, and the keys and values can be accessed using  keys(ds)  and  values(ds) .  map(f, ds)  applies the function  f  to each feature array: julia> ds2 = map(x -> x .+ 1, ds)\nDataStore() with 3 elements:\n  y = 3-element Vector{Float32}\n  z = 3-element Vector{Float32}\n  x = 2×3 Matrix{Float32}\n\njulia> ds2.z\n3-element Vector{Float32}:\n 1.0\n 1.0\n 1.0 source"},{"id":656,"pagetitle":"GNNGraph","title":"Query","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/gnngraph/#Query","content":" Query"},{"id":657,"pagetitle":"GNNGraph","title":"GNNGraphs.adjacency_list","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.adjacency_list-Tuple{GNNGraph, Any}","content":" GNNGraphs.adjacency_list  —  Method adjacency_list(g; dir=:out)\nadjacency_list(g, nodes; dir=:out) Return the adjacency list representation (a vector of vectors) of the graph  g . Calling  a  the adjacency list, if  dir=:out  than  a[i]  will contain the neighbors of node  i  through outgoing edges. If  dir=:in , it will contain neighbors from incoming edges instead. If  nodes  is given, return the neighborhood of the nodes in  nodes  only. source"},{"id":658,"pagetitle":"GNNGraph","title":"GNNGraphs.edge_index","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.edge_index-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}}","content":" GNNGraphs.edge_index  —  Method edge_index(g::GNNGraph) Return a tuple containing two vectors, respectively storing  the source and target nodes for each edges in  g . s, t = edge_index(g) source"},{"id":659,"pagetitle":"GNNGraph","title":"GNNGraphs.get_graph_type","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.get_graph_type-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}}","content":" GNNGraphs.get_graph_type  —  Method get_graph_type(g::GNNGraph) Return the underlying representation for the graph  g  as a symbol. Possible values are: :coo : Coordinate list representation. The graph is stored as a tuple of vectors  (s, t, w) ,         where  s  and  t  are the source and target nodes of the edges, and  w  is the edge weights. :sparse : Sparse matrix representation. The graph is stored as a sparse matrix representing the weighted adjacency matrix. :dense : Dense matrix representation. The graph is stored as a dense matrix representing the weighted adjacency matrix. The default representation for graph constructors GNNGraphs.jl is  :coo . The underlying representation can be accessed through the  g.graph  field. See also  GNNGraph . Examples The default representation for graph constructors GNNGraphs.jl is  :coo . julia> g = rand_graph(5, 10)\nGNNGraph:\n  num_nodes: 5\n  num_edges: 10\n\njulia> get_graph_type(g)\n:coo The  GNNGraph  constructor can also be used to create graphs with different representations. julia> g = GNNGraph([2,3,5], [1,2,4], graph_type=:sparse)\nGNNGraph:\n  num_nodes: 5\n  num_edges: 3\n\njulia> g.graph\n5×5 SparseArrays.SparseMatrixCSC{Int64, Int64} with 3 stored entries:\n ⋅  ⋅  ⋅  ⋅  ⋅\n 1  ⋅  ⋅  ⋅  ⋅\n ⋅  1  ⋅  ⋅  ⋅\n ⋅  ⋅  ⋅  ⋅  ⋅\n ⋅  ⋅  ⋅  1  ⋅\n\njulia> get_graph_type(g)\n:sparse\n\njulia> gcoo = GNNGraph(g, graph_type=:coo);\n\njulia> gcoo.graph\n([2, 3, 5], [1, 2, 4], [1, 1, 1]) source"},{"id":660,"pagetitle":"GNNGraph","title":"GNNGraphs.graph_indicator","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.graph_indicator-Tuple{GNNGraph}","content":" GNNGraphs.graph_indicator  —  Method graph_indicator(g::GNNGraph; edges=false) Return a vector containing the graph membership (an integer from  1  to  g.num_graphs ) of each node in the graph. If  edges=true , return the graph membership of each edge instead. source"},{"id":661,"pagetitle":"GNNGraph","title":"GNNGraphs.has_isolated_nodes","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.has_isolated_nodes-Tuple{GNNGraph}","content":" GNNGraphs.has_isolated_nodes  —  Method has_isolated_nodes(g::GNNGraph; dir=:out) Return true if the graph  g  contains nodes with out-degree (if  dir=:out ) or in-degree (if  dir = :in ) equal to zero. source"},{"id":662,"pagetitle":"GNNGraph","title":"GNNGraphs.has_multi_edges","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.has_multi_edges-Tuple{GNNGraph}","content":" GNNGraphs.has_multi_edges  —  Method has_multi_edges(g::GNNGraph) Return  true  if  g  has any multiple edges. source"},{"id":663,"pagetitle":"GNNGraph","title":"GNNGraphs.is_bidirected","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.is_bidirected-Tuple{GNNGraph}","content":" GNNGraphs.is_bidirected  —  Method is_bidirected(g::GNNGraph) Check if the directed graph  g  essentially corresponds to an undirected graph, i.e. if for each edge it also contains the  reverse edge.  source"},{"id":664,"pagetitle":"GNNGraph","title":"GNNGraphs.khop_adj","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.khop_adj","content":" GNNGraphs.khop_adj  —  Function khop_adj(g::GNNGraph,k::Int,T::DataType=eltype(g); dir=:out, weighted=true) Return  $A^k$  where  $A$  is the adjacency matrix of the graph 'g'. source"},{"id":665,"pagetitle":"GNNGraph","title":"GNNGraphs.laplacian_lambda_max","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.laplacian_lambda_max","content":" GNNGraphs.laplacian_lambda_max  —  Function laplacian_lambda_max(g::GNNGraph, T=Float32; add_self_loops=false, dir=:out) Return the largest eigenvalue of the normalized symmetric Laplacian of the graph  g . If the graph is batched from multiple graphs, return the list of the largest eigenvalue for each graph. source"},{"id":666,"pagetitle":"GNNGraph","title":"GNNGraphs.normalized_laplacian","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.normalized_laplacian","content":" GNNGraphs.normalized_laplacian  —  Function normalized_laplacian(g, T=Float32; add_self_loops=false, dir=:out) Normalized Laplacian matrix of graph  g . Arguments g : A  GNNGraph . T : result element type. add_self_loops : add self-loops while calculating the matrix. dir : the edge directionality considered (:out, :in, :both). source"},{"id":667,"pagetitle":"GNNGraph","title":"GNNGraphs.scaled_laplacian","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.scaled_laplacian","content":" GNNGraphs.scaled_laplacian  —  Function scaled_laplacian(g, T=Float32; dir=:out) Scaled Laplacian matrix of graph  g , defined as  $\\hat{L} = \\frac{2}{\\lambda_{max}} L - I$  where  $L$  is the normalized Laplacian matrix. Arguments g : A  GNNGraph . T : result element type. dir : the edge directionality considered (:out, :in, :both). source"},{"id":668,"pagetitle":"GNNGraph","title":"Graphs.LinAlg.adjacency_matrix","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/gnngraph/#Graphs.LinAlg.adjacency_matrix","content":" Graphs.LinAlg.adjacency_matrix  —  Function adjacency_matrix(g::GNNGraph, T=eltype(g); dir=:out, weighted=true) Return the adjacency matrix  A  for the graph  g .  If  dir=:out ,  A[i,j] > 0  denotes the presence of an edge from node  i  to node  j . If  dir=:in  instead,  A[i,j] > 0  denotes the presence of an edge from node  j  to node  i . User may specify the eltype  T  of the returned matrix.  If  weighted=true , the  A  will contain the edge weights if any, otherwise the elements of  A  will be either 0 or 1. source"},{"id":669,"pagetitle":"GNNGraph","title":"Graphs.degree","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/gnngraph/#Graphs.degree-Union{Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}}, Tuple{TT}, Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}, TT}} where TT<:Union{Nothing, Type{<:Number}}","content":" Graphs.degree  —  Method degree(g::GNNGraph, T=nothing; dir=:out, edge_weight=true) Return a vector containing the degrees of the nodes in  g . The gradient is propagated through this function only if  edge_weight  is  true  or a vector. Arguments g : A graph. T : Element type of the returned vector. If  nothing , is      chosen based on the graph type and will be an integer      if  edge_weight = false . Default  nothing . dir : For  dir = :out  the degree of a node is counted based on the outgoing edges.        For  dir = :in , the ingoing edges are used. If  dir = :both  we have the sum of the two. edge_weight : If  true  and the graph contains weighted edges, the degree will                be weighted. Set to  false  instead to just count the number of               outgoing/ingoing edges.                Finally, you can also pass a vector of weights to be used               instead of the graph's own weights.               Default  true . source"},{"id":670,"pagetitle":"GNNGraph","title":"Graphs.has_self_loops","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/gnngraph/#Graphs.has_self_loops-Tuple{GNNGraph}","content":" Graphs.has_self_loops  —  Method has_self_loops(g::GNNGraph) Return  true  if  g  has any self loops. source"},{"id":671,"pagetitle":"GNNGraph","title":"Graphs.inneighbors","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/gnngraph/#Graphs.inneighbors-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}, Integer}","content":" Graphs.inneighbors  —  Method inneighbors(g::GNNGraph, i::Integer) Return the neighbors of node  i  in the graph  g  through incoming edges. See also  neighbors  and  outneighbors . source"},{"id":672,"pagetitle":"GNNGraph","title":"Graphs.outneighbors","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/gnngraph/#Graphs.outneighbors-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}, Integer}","content":" Graphs.outneighbors  —  Method outneighbors(g::GNNGraph, i::Integer) Return the neighbors of node  i  in the graph  g  through outgoing edges. See also  neighbors  and  inneighbors . source"},{"id":673,"pagetitle":"GNNGraph","title":"Graphs.neighbors","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/gnngraph/#Graphs.neighbors-Tuple{GNNGraph, Integer}","content":" Graphs.neighbors  —  Method neighbors(g::GNNGraph, i::Integer; dir=:out) Return the neighbors of node  i  in the graph  g . If  dir=:out , return the neighbors through outgoing edges. If  dir=:in , return the neighbors through incoming edges. See also  outneighbors ,  inneighbors . source"},{"id":674,"pagetitle":"GNNGraph","title":"Transform","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/gnngraph/#Transform","content":" Transform"},{"id":675,"pagetitle":"GNNGraph","title":"GNNGraphs.add_edges","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.add_edges-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}, AbstractVector, AbstractVector}","content":" GNNGraphs.add_edges  —  Method add_edges(g::GNNGraph, s::AbstractVector, t::AbstractVector; [edata])\nadd_edges(g::GNNGraph, (s, t); [edata])\nadd_edges(g::GNNGraph, (s, t, w); [edata]) Add to graph  g  the edges with source nodes  s  and target nodes  t . Optionally, pass the edge weight  w  and the features   edata  for the new edges. Returns a new graph sharing part of the underlying data with  g . If the  s  or  t  contain nodes that are not already present in the graph, they are added to the graph as well. Examples julia> s, t = [1, 2, 3, 3, 4], [2, 3, 4, 4, 4];\n\njulia> w = Float32[1.0, 2.0, 3.0, 4.0, 5.0];\n\njulia> g = GNNGraph((s, t, w))\nGNNGraph:\n  num_nodes: 4\n  num_edges: 5\n\njulia> add_edges(g, ([2, 3], [4, 1], [10.0, 20.0]))\nGNNGraph:\n  num_nodes: 4\n  num_edges: 7 julia> g = GNNGraph()\nGNNGraph:\n  num_nodes: 0\n  num_edges: 0\n\njulia> add_edges(g, [1,2], [2,3])\nGNNGraph:\n  num_nodes: 3\n  num_edges: 2 source"},{"id":676,"pagetitle":"GNNGraph","title":"GNNGraphs.add_nodes","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.add_nodes-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}, Integer}","content":" GNNGraphs.add_nodes  —  Method add_nodes(g::GNNGraph, n; [ndata]) Add  n  new nodes to graph  g . In the  new graph, these nodes will have indexes from  g.num_nodes + 1  to  g.num_nodes + n . source"},{"id":677,"pagetitle":"GNNGraph","title":"GNNGraphs.add_self_loops","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.add_self_loops-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}}","content":" GNNGraphs.add_self_loops  —  Method add_self_loops(g::GNNGraph) Return a graph with the same features as  g  but also adding edges connecting the nodes to themselves. Nodes with already existing self-loops will obtain a second self-loop. If the graphs has edge weights, the new edges will have weight 1. source"},{"id":678,"pagetitle":"GNNGraph","title":"GNNGraphs.getgraph","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.getgraph-Tuple{GNNGraph, Int64}","content":" GNNGraphs.getgraph  —  Method getgraph(g::GNNGraph, i; nmap=false) Return the subgraph of  g  induced by those nodes  j  for which  g.graph_indicator[j] == i  or, if  i  is a collection,  g.graph_indicator[j] ∈ i .  In other words, it extract the component graphs from a batched graph.  If  nmap=true , return also a vector  v  mapping the new nodes to the old ones.  The node  i  in the subgraph will correspond to the node  v[i]  in  g . source"},{"id":679,"pagetitle":"GNNGraph","title":"GNNGraphs.negative_sample","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.negative_sample-Tuple{GNNGraph}","content":" GNNGraphs.negative_sample  —  Method negative_sample(g::GNNGraph; \n                num_neg_edges = g.num_edges, \n                bidirected = is_bidirected(g)) Return a graph containing random negative edges (i.e. non-edges) from graph  g  as edges. If  bidirected=true , the output graph will be bidirected and there will be no leakage from the origin graph.  See also  is_bidirected . source"},{"id":680,"pagetitle":"GNNGraph","title":"GNNGraphs.perturb_edges","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.perturb_edges-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}, AbstractFloat}","content":" GNNGraphs.perturb_edges  —  Method perturb_edges([rng], g::GNNGraph, perturb_ratio) Return a new graph obtained from  g  by adding random edges, based on a specified  perturb_ratio .  The  perturb_ratio  determines the fraction of new edges to add relative to the current number of edges in the graph.  These new edges are added without creating self-loops.  The function returns a new  GNNGraph  instance that shares some of the underlying data with  g  but includes the additional edges.  The nodes for the new edges are selected randomly, and no edge data ( edata ) or weights ( w ) are assigned to these new edges. Arguments g::GNNGraph : The graph to be perturbed. perturb_ratio : The ratio of the number of new edges to add relative to the current number of edges in the graph. For example, a  perturb_ratio  of 0.1 means that 10% of the current number of edges will be added as new random edges. rng : An optionalrandom number generator to ensure reproducible results. Examples julia> g = GNNGraph((s, t, w))\nGNNGraph:\n  num_nodes: 4\n  num_edges: 5\n\njulia> perturbed_g = perturb_edges(g, 0.2)\nGNNGraph:\n  num_nodes: 4\n  num_edges: 6 source"},{"id":681,"pagetitle":"GNNGraph","title":"GNNGraphs.ppr_diffusion","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.ppr_diffusion-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}}","content":" GNNGraphs.ppr_diffusion  —  Method ppr_diffusion(g::GNNGraph{<:COO_T}, alpha =0.85f0) -> GNNGraph Calculates the Personalized PageRank (PPR) diffusion based on the edge weight matrix of a GNNGraph and updates the graph with new edge weights derived from the PPR matrix. References paper:  The pagerank citation ranking: Bringing order to the web The function performs the following steps: Constructs a modified adjacency matrix  A  using the graph's edge weights, where  A  is adjusted by  (α - 1) * A + I , with  α  being the damping factor ( alpha_f32 ) and  I  the identity matrix. Normalizes  A  to ensure each column sums to 1, representing transition probabilities. Applies the PPR formula  α * (I + (α - 1) * A)^-1  to compute the diffusion matrix. Updates the original edge weights of the graph based on the PPR diffusion matrix, assigning new weights for each edge from the PPR matrix. Arguments g::GNNGraph : The input graph for which PPR diffusion is to be calculated. It should have edge weights available. alpha_f32::Float32 : The damping factor used in PPR calculation, controlling the teleport probability in the random walk. Defaults to  0.85f0 . Returns A new  GNNGraph  instance with the same structure as  g  but with updated edge weights according to the PPR diffusion calculation. source"},{"id":682,"pagetitle":"GNNGraph","title":"GNNGraphs.rand_edge_split","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.rand_edge_split-Tuple{GNNGraph, Any}","content":" GNNGraphs.rand_edge_split  —  Method rand_edge_split(g::GNNGraph, frac; bidirected=is_bidirected(g)) -> g1, g2 Randomly partition the edges in  g  to form two graphs,  g1  and  g2 . Both will have the same number of nodes as  g .  g1  will contain a fraction  frac  of the original edges,  while  g2  wil contain the rest. If  bidirected = true  makes sure that an edge and its reverse go into the same split. This option is supported only for bidirected graphs with no self-loops and multi-edges. rand_edge_split  is tipically used to create train/test splits in link prediction tasks. source"},{"id":683,"pagetitle":"GNNGraph","title":"GNNGraphs.random_walk_pe","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.random_walk_pe-Tuple{GNNGraph, Int64}","content":" GNNGraphs.random_walk_pe  —  Method random_walk_pe(g, walk_length) Return the random walk positional encoding from the paper  Graph Neural Networks with Learnable Structural and Positional Representations  of the given graph  g  and the length of the walk  walk_length  as a matrix of size  (walk_length, g.num_nodes) .  source"},{"id":684,"pagetitle":"GNNGraph","title":"GNNGraphs.remove_edges","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.remove_edges-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}, AbstractVector{<:Integer}}","content":" GNNGraphs.remove_edges  —  Method remove_edges(g::GNNGraph, edges_to_remove::AbstractVector{<:Integer})\nremove_edges(g::GNNGraph, p=0.5) Remove specified edges from a GNNGraph, either by specifying edge indices or by randomly removing edges with a given probability. Arguments g : The input graph from which edges will be removed. edges_to_remove : Vector of edge indices to be removed. This argument is only required for the first method. p : Probability of removing each edge. This argument is only required for the second method and defaults to 0.5. Returns A new GNNGraph with the specified edges removed. Example julia> using GNNGraphs\n\n# Construct a GNNGraph\njulia> g = GNNGraph([1, 1, 2, 2, 3], [2, 3, 1, 3, 1])\nGNNGraph:\n  num_nodes: 3\n  num_edges: 5\n  \n# Remove the second edge\njulia> g_new = remove_edges(g, [2]);\n\njulia> g_new\nGNNGraph:\n  num_nodes: 3\n  num_edges: 4\n\n# Remove edges with a probability of 0.5\njulia> g_new = remove_edges(g, 0.5);\n\njulia> g_new\nGNNGraph:\n  num_nodes: 3\n  num_edges: 2 source"},{"id":685,"pagetitle":"GNNGraph","title":"GNNGraphs.remove_multi_edges","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.remove_multi_edges-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}}","content":" GNNGraphs.remove_multi_edges  —  Method remove_multi_edges(g::GNNGraph; aggr=+) Remove multiple edges (also called parallel edges or repeated edges) from graph  g . Possible edge features are aggregated according to  aggr , that can take value   + , min ,  max  or  mean . See also  remove_self_loops ,  has_multi_edges , and  to_bidirected . source"},{"id":686,"pagetitle":"GNNGraph","title":"GNNGraphs.remove_nodes","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.remove_nodes-Tuple{GNNGraph, AbstractFloat}","content":" GNNGraphs.remove_nodes  —  Method remove_nodes(g::GNNGraph, p) Returns a new graph obtained by dropping nodes from  g  with independent probabilities  p .  Examples julia> g = GNNGraph([1, 1, 2, 2, 3, 4], [1, 2, 3, 1, 3, 1])\nGNNGraph:\n  num_nodes: 4\n  num_edges: 6\n\njulia> g_new = remove_nodes(g, 0.5)\nGNNGraph:\n  num_nodes: 2\n  num_edges: 2 source"},{"id":687,"pagetitle":"GNNGraph","title":"GNNGraphs.remove_nodes","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.remove_nodes-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}, AbstractVector}","content":" GNNGraphs.remove_nodes  —  Method remove_nodes(g::GNNGraph, nodes_to_remove::AbstractVector) Remove specified nodes, and their associated edges, from a GNNGraph. This operation reindexes the remaining nodes to maintain a continuous sequence of node indices, starting from 1. Similarly, edges are reindexed to account for the removal of edges connected to the removed nodes. Arguments g : The input graph from which nodes (and their edges) will be removed. nodes_to_remove : Vector of node indices to be removed. Returns A new GNNGraph with the specified nodes and all edges associated with these nodes removed.  Example using GNNGraphs\n\ng = GNNGraph([1, 1, 2, 2, 3], [2, 3, 1, 3, 1])\n\n# Remove nodes with indices 2 and 3, for example\ng_new = remove_nodes(g, [2, 3])\n\n# g_new now does not contain nodes 2 and 3, and any edges that were connected to these nodes.\nprintln(g_new) source"},{"id":688,"pagetitle":"GNNGraph","title":"GNNGraphs.remove_self_loops","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.remove_self_loops-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}}","content":" GNNGraphs.remove_self_loops  —  Method remove_self_loops(g::GNNGraph) Return a graph constructed from  g  where self-loops (edges from a node to itself) are removed.  See also  add_self_loops  and  remove_multi_edges . source"},{"id":689,"pagetitle":"GNNGraph","title":"GNNGraphs.set_edge_weight","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.set_edge_weight-Tuple{GNNGraph, AbstractVector}","content":" GNNGraphs.set_edge_weight  —  Method set_edge_weight(g::GNNGraph, w::AbstractVector) Set  w  as edge weights in the returned graph.  source"},{"id":690,"pagetitle":"GNNGraph","title":"GNNGraphs.to_bidirected","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.to_bidirected-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}}","content":" GNNGraphs.to_bidirected  —  Method to_bidirected(g) Adds a reverse edge for each edge in the graph, then calls   remove_multi_edges  with  mean  aggregation to simplify the graph.  See also  is_bidirected .  Examples julia> s, t = [1, 2, 3, 3, 4], [2, 3, 4, 4, 4];\n\njulia> w = [1.0, 2.0, 3.0, 4.0, 5.0];\n\njulia> e = [10.0, 20.0, 30.0, 40.0, 50.0];\n\njulia> g = GNNGraph(s, t, w, edata = e)\nGNNGraph:\n  num_nodes: 4\n  num_edges: 5\n  edata:\n        e = 5-element Vector{Float64}\n\njulia> g2 = to_bidirected(g)\nGNNGraph:\n  num_nodes: 4\n  num_edges: 7\n  edata:\n        e = 7-element Vector{Float64}\n\njulia> edge_index(g2)\n([1, 2, 2, 3, 3, 4, 4], [2, 1, 3, 2, 4, 3, 4])\n\njulia> get_edge_weight(g2)\n7-element Vector{Float64}:\n 1.0\n 1.0\n 2.0\n 2.0\n 3.5\n 3.5\n 5.0\n\njulia> g2.edata.e\n7-element Vector{Float64}:\n 10.0\n 10.0\n 20.0\n 20.0\n 35.0\n 35.0\n 50.0 source"},{"id":691,"pagetitle":"GNNGraph","title":"GNNGraphs.to_unidirected","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.to_unidirected-Tuple{GNNGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}}","content":" GNNGraphs.to_unidirected  —  Method to_unidirected(g::GNNGraph) Return a graph that for each multiple edge between two nodes in  g  keeps only an edge in one direction. source"},{"id":692,"pagetitle":"GNNGraph","title":"MLUtils.batch","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/gnngraph/#MLUtils.batch-Tuple{AbstractVector{<:GNNGraph}}","content":" MLUtils.batch  —  Method batch(gs::Vector{<:GNNGraph}) Batch together multiple  GNNGraph s into a single one  containing the total number of original nodes and edges. Equivalent to  SparseArrays.blockdiag . See also  MLUtils.unbatch . Examples julia> g1 = rand_graph(4, 4, ndata=ones(Float32, 3, 4))\nGNNGraph:\n  num_nodes: 4\n  num_edges: 4\n  ndata:\n        x = 3×4 Matrix{Float32}\n\njulia> g2 = rand_graph(5, 4, ndata=zeros(Float32, 3, 5))\nGNNGraph:\n  num_nodes: 5\n  num_edges: 4\n  ndata:\n        x = 3×5 Matrix{Float32}\n\njulia> g12 = MLUtils.batch([g1, g2])\nGNNGraph:\n  num_nodes: 9\n  num_edges: 8\n  num_graphs: 2\n  ndata:\n        x = 3×9 Matrix{Float32}\n\njulia> g12.ndata.x\n3×9 Matrix{Float32}:\n 1.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0\n 1.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0\n 1.0  1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0 source"},{"id":693,"pagetitle":"GNNGraph","title":"MLUtils.unbatch","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/gnngraph/#MLUtils.unbatch-Union{Tuple{GNNGraph{T}}, Tuple{T}} where T<:(Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}})","content":" MLUtils.unbatch  —  Method unbatch(g::GNNGraph) Opposite of the  MLUtils.batch  operation, returns  an array of the individual graphs batched together in  g . See also  MLUtils.batch  and  getgraph . Examples julia> using MLUtils\n\njulia> gbatched = MLUtils.batch([rand_graph(5, 6), rand_graph(10, 8), rand_graph(4,2)])\nGNNGraph:\n  num_nodes: 19\n  num_edges: 16\n  num_graphs: 3\n\njulia> MLUtils.unbatch(gbatched)\n3-element Vector{GNNGraph{Tuple{Vector{Int64}, Vector{Int64}, Nothing}}}:\n GNNGraph(5, 6) with no data\n GNNGraph(10, 8) with no data\n GNNGraph(4, 2) with no data source"},{"id":694,"pagetitle":"GNNGraph","title":"SparseArrays.blockdiag","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/gnngraph/#SparseArrays.blockdiag-Tuple{GNNGraph, Vararg{GNNGraph}}","content":" SparseArrays.blockdiag  —  Method blockdiag(xs::GNNGraph...) Equivalent to  MLUtils.batch . source"},{"id":695,"pagetitle":"GNNGraph","title":"Utils","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/gnngraph/#Utils","content":" Utils"},{"id":696,"pagetitle":"GNNGraph","title":"GNNGraphs.sort_edge_index","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.sort_edge_index","content":" GNNGraphs.sort_edge_index  —  Function sort_edge_index(ei::Tuple) -> u', v'\nsort_edge_index(u, v) -> u', v' Return a sorted version of the tuple of vectors  ei = (u, v) , applying a common permutation to  u  and  v . The sorting is lexycographic, that is the pairs  (ui, vi)   are sorted first according to the  ui  and then according to  vi .  source"},{"id":697,"pagetitle":"GNNGraph","title":"GNNGraphs.color_refinement","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.color_refinement","content":" GNNGraphs.color_refinement  —  Function color_refinement(g::GNNGraph, [x0]) -> x, num_colors, niters The color refinement algorithm for graph coloring.  Given a graph  g  and an initial coloring  x0 , the algorithm  iteratively refines the coloring until a fixed point is reached. At each iteration the algorithm computes a hash of the coloring and the sorted list of colors of the neighbors of each node. This hash is used to determine if the coloring has changed. math x_i' = hashmap((x_i, sort([x_j for j \\in N(i)]))). ` This algorithm is related to the 1-Weisfeiler-Lehman algorithm for graph isomorphism testing. Arguments g::GNNGraph : The graph to color. x0::AbstractVector{<:Integer} : The initial coloring. If not provided, all nodes are colored with 1. Returns x::AbstractVector{<:Integer} : The final coloring. num_colors::Int : The number of colors used. niters::Int : The number of iterations until convergence. source"},{"id":698,"pagetitle":"GNNGraph","title":"Generate","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/gnngraph/#Generate","content":" Generate"},{"id":699,"pagetitle":"GNNGraph","title":"GNNGraphs.knn_graph","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.knn_graph-Tuple{AbstractMatrix, Int64}","content":" GNNGraphs.knn_graph  —  Method knn_graph(points::AbstractMatrix, \n          k::Int; \n          graph_indicator = nothing,\n          self_loops = false, \n          dir = :in, \n          kws...) Create a  k -nearest neighbor graph where each node is linked  to its  k  closest  points .   Arguments points : A num features × num nodes matrix storing the Euclidean positions of the nodes. k : The number of neighbors considered in the kNN algorithm. graph_indicator : Either nothing or a vector containing the graph assignment of each node,                     in which case the returned graph will be a batch of graphs.  self_loops : If  true , consider the node itself among its  k  nearest neighbors, in which               case the graph will contain self-loops.  dir : The direction of the edges. If  dir=:in  edges go from the  k          neighbors to the central node. If  dir=:out  we have the opposite        direction. kws : Further keyword arguments will be passed to the  GNNGraph  constructor. Examples julia> n, k = 10, 3;\n\njulia> x = rand(Float32, 3, n);\n\njulia> g = knn_graph(x, k)\nGNNGraph:\n    num_nodes = 10\n    num_edges = 30\n\njulia> graph_indicator = [1,1,1,1,1,2,2,2,2,2];\n\njulia> g = knn_graph(x, k; graph_indicator)\nGNNGraph:\n    num_nodes = 10\n    num_edges = 30\n    num_graphs = 2 source"},{"id":700,"pagetitle":"GNNGraph","title":"GNNGraphs.radius_graph","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.radius_graph-Tuple{AbstractMatrix, AbstractFloat}","content":" GNNGraphs.radius_graph  —  Method radius_graph(points::AbstractMatrix, \n             r::AbstractFloat; \n             graph_indicator = nothing,\n             self_loops = false, \n             dir = :in, \n             kws...) Create a graph where each node is linked  to its neighbors within a given distance  r .   Arguments points : A num features × num nodes matrix storing the Euclidean positions of the nodes. r : The radius. graph_indicator : Either nothing or a vector containing the graph assignment of each node,                     in which case the returned graph will be a batch of graphs.  self_loops : If  true , consider the node itself among its neighbors, in which               case the graph will contain self-loops.  dir : The direction of the edges. If  dir=:in  edges go from the        neighbors to the central node. If  dir=:out  we have the opposite        direction. kws : Further keyword arguments will be passed to the  GNNGraph  constructor. Examples julia> n, r = 10, 0.75;\n\njulia> x = rand(Float32, 3, n);\n\njulia> g = radius_graph(x, r)\nGNNGraph:\n    num_nodes = 10\n    num_edges = 46\n\njulia> graph_indicator = [1,1,1,1,1,2,2,2,2,2];\n\njulia> g = radius_graph(x, r; graph_indicator)\nGNNGraph:\n    num_nodes = 10\n    num_edges = 20\n    num_graphs = 2 References Section B paragraphs 1 and 2 of the paper  Dynamic Hidden-Variable Network Models source"},{"id":701,"pagetitle":"GNNGraph","title":"GNNGraphs.rand_graph","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.rand_graph-Tuple{Integer, Integer}","content":" GNNGraphs.rand_graph  —  Method rand_graph([rng,] n, m; bidirected=true, edge_weight = nothing, kws...) Generate a random (Erdós-Renyi)  GNNGraph  with  n  nodes and  m  edges. If  bidirected=true  the reverse edge of each edge will be present. If  bidirected=false  instead,  m  unrelated edges are generated. In any case, the output graph will contain no self-loops or multi-edges. A vector can be passed  as  edge_weight . Its length has to be equal to  m  in the directed case, and  m÷2  in the bidirected one. Pass a random number generator as the first argument to make the generation reproducible. Additional keyword arguments will be passed to the  GNNGraph  constructor. Examples julia> g = rand_graph(5, 4, bidirected=false)\nGNNGraph:\n  num_nodes: 5\n  num_edges: 4\n\njulia> edge_index(g)\n([4, 3, 2, 1], [5, 4, 3, 2])\n\n# In the bidirected case, edge data will be duplicated on the reverse edges if needed.\njulia> g = rand_graph(5, 4, edata=rand(Float32, 16, 2))\nGNNGraph:\n  num_nodes: 5\n  num_edges: 4\n  edata:\n        e = 16×4 Matrix{Float32}\n\n# Each edge has a reverse\njulia> edge_index(g)\n([1, 1, 5, 3], [5, 3, 1, 1]) source"},{"id":702,"pagetitle":"GNNGraph","title":"Operators","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/gnngraph/#Operators","content":" Operators"},{"id":703,"pagetitle":"GNNGraph","title":"Base.intersect","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/gnngraph/#Base.intersect","content":" Base.intersect  —  Function intersect(g1::GNNGraph, g2::GNNGraph) Intersect two graphs by keeping only the common edges. source"},{"id":704,"pagetitle":"GNNGraph","title":"Sampling","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/gnngraph/#Sampling","content":" Sampling"},{"id":705,"pagetitle":"GNNGraph","title":"GNNGraphs.NeighborLoader","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.NeighborLoader","content":" GNNGraphs.NeighborLoader  —  Type NeighborLoader(graph; num_neighbors, input_nodes, num_layers, [batch_size]) A data structure for sampling neighbors from a graph for training Graph Neural Networks (GNNs).  It supports multi-layer sampling of neighbors for a batch of input nodes, useful for mini-batch training originally introduced in [\"Inductive Representation Learning on Large Graphs\"}(https://arxiv.org/abs/1706.02216) paper. Fields graph::GNNGraph : The input graph. num_neighbors::Vector{Int} : A vector specifying the number of neighbors to sample per node at each GNN layer. input_nodes::Vector{Int} : A vector containing the starting nodes for neighbor sampling. num_layers::Int : The number of layers for neighborhood expansion (how far to sample neighbors). batch_size::Union{Int, Nothing} : The size of the batch. If not specified, it defaults to the number of  input_nodes . Examples julia> loader = NeighborLoader(graph; num_neighbors=[10, 5], input_nodes=[1, 2, 3], num_layers=2)\n\njulia> batch_counter = 0\n\njulia> for mini_batch_gnn in loader\n            batch_counter += 1\n            println(\"Batch \", batch_counter, \": Nodes in mini-batch graph: \", nv(mini_batch_gnn))\n        end source"},{"id":706,"pagetitle":"GNNGraph","title":"GNNGraphs.sample_neighbors","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/gnngraph/#GNNGraphs.sample_neighbors","content":" GNNGraphs.sample_neighbors  —  Function sample_neighbors(g, nodes, K=-1; dir=:in, replace=false, dropnodes=false) Sample neighboring edges of the given nodes and return the induced subgraph. For each node, a number of inbound (or outbound when  dir = :out ) edges will be randomly chosen.  If dropnodes=false`, the graph returned will then contain all the nodes in the original graph,  but only the sampled edges. The returned graph will contain an edge feature  EID  corresponding to the id of the edge in the original graph. If  dropnodes=true , it will also contain a node feature  NID  with the node ids in the original graph. Arguments g . The graph. nodes . A list of node IDs to sample neighbors from. K . The maximum number of edges to be sampled for each node.      If -1, all the neighboring edges will be selected. dir . Determines whether to sample inbound ( :in ) or outbound (` :out ) edges (Default  :in ). replace . If  true , sample with replacement. dropnodes . If  true , the resulting subgraph will contain only the nodes involved in the sampled edges. Examples julia> g = rand_graph(20, 100)\nGNNGraph:\n    num_nodes = 20\n    num_edges = 100\n\njulia> sample_neighbors(g, 2:3)\nGNNGraph:\n    num_nodes = 20\n    num_edges = 9\n    edata:\n        EID => (9,)\n\njulia> sg = sample_neighbors(g, 2:3, dropnodes=true)\nGNNGraph:\n    num_nodes = 10\n    num_edges = 9\n    ndata:\n        NID => (10,)\n    edata:\n        EID => (9,)\n\njulia> sg.ndata.NID\n10-element Vector{Int64}:\n  2\n  3\n 17\n 14\n 18\n 15\n 16\n 20\n  7\n 10\n\njulia> sample_neighbors(g, 2:3, 5, replace=true)\nGNNGraph:\n    num_nodes = 20\n    num_edges = 10\n    edata:\n        EID => (10,) source"},{"id":707,"pagetitle":"GNNGraph","title":"Graphs.induced_subgraph","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/gnngraph/#Graphs.induced_subgraph-Tuple{GNNGraph, Vector{Int64}}","content":" Graphs.induced_subgraph  —  Method induced_subgraph(graph, nodes) Generates a subgraph from the original graph using the provided  nodes .  The function includes the nodes' neighbors and creates edges between nodes that are connected in the original graph.  If a node has no neighbors, an isolated node will be added to the subgraph.  Returns A new  GNNGraph  containing the subgraph with the specified nodes and their features. Arguments graph . The original GNNGraph containing nodes, edges, and node features. nodes `. A vector of node indices to include in the subgraph. Examples julia> s = [1, 2]\n2-element Vector{Int64}:\n 1\n 2\n\njulia> t = [2, 3]\n2-element Vector{Int64}:\n 2\n 3\n\njulia> graph = GNNGraph((s, t), ndata = (; x=rand(Float32, 32, 3), y=rand(Float32, 3)), edata = rand(Float32, 2))\nGNNGraph:\n  num_nodes: 3\n  num_edges: 2\n  ndata:\n        y = 3-element Vector{Float32}\n        x = 32×3 Matrix{Float32}\n  edata:\n        e = 2-element Vector{Float32}\n\njulia> nodes = [1, 2]\n2-element Vector{Int64}:\n 1\n 2\n\njulia> subgraph = Graphs.induced_subgraph(graph, nodes)\nGNNGraph:\n  num_nodes: 2\n  num_edges: 1\n  ndata:\n        y = 2-element Vector{Float32}\n        x = 32×2 Matrix{Float32}\n  edata:\n        e = 1-element Vector{Float32} source"},{"id":710,"pagetitle":"GNNHeteroGraph","title":"Heterogeneous Graphs","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/heterograph/#Heterogeneous-Graphs","content":" Heterogeneous Graphs"},{"id":711,"pagetitle":"GNNHeteroGraph","title":"GNNHeteroGraph","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/heterograph/#GNNHeteroGraph","content":" GNNHeteroGraph Documentation page for the type  GNNHeteroGraph  representing heterogeneous graphs, where  nodes and edges can have different types."},{"id":712,"pagetitle":"GNNHeteroGraph","title":"GNNGraphs.GNNHeteroGraph","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/heterograph/#GNNGraphs.GNNHeteroGraph","content":" GNNGraphs.GNNHeteroGraph  —  Type GNNHeteroGraph(data; [ndata, edata, gdata, num_nodes])\nGNNHeteroGraph(pairs...; [ndata, edata, gdata, num_nodes]) A type representing a heterogeneous graph structure. It is similar to  GNNGraph  but nodes and edges are of different types. Constructor Arguments data : A dictionary or an iterable object that maps  (source_type, edge_type, target_type)          triples to  (source, target)  index vectors (or to  (source, target, weight)  if also edge weights are present). pairs : Passing multiple relations as pairs is equivalent to passing  data=Dict(pairs...) . ndata : Node features. A dictionary of arrays or named tuple of arrays.          The size of the last dimension of each array must be given by  g.num_nodes . edata : Edge features. A dictionary of arrays or named tuple of arrays. Default  nothing .          The size of the last dimension of each array must be given by  g.num_edges . Default  nothing . gdata : Graph features. An array or named tuple of arrays whose last dimension has size  num_graphs . Default  nothing . num_nodes : The number of nodes for each type. If not specified, inferred from  data . Default  nothing . Fields graph : A dictionary that maps  (source_type, edge_type, target_type)  triples to  (source, target)  index vectors. num_nodes : The number of nodes for each type. num_edges : The number of edges for each type. ndata : Node features. edata : Edge features. gdata : Graph features. ntypes : The node types. etypes : The edge types. Examples julia> using GNNGraphs\n\njulia> nA, nB = 10, 20;\n\njulia> num_nodes = Dict(:A => nA, :B => nB);\n\njulia> edges1 = (rand(1:nA, 20), rand(1:nB, 20))\n([4, 8, 6, 3, 4, 7, 2, 7, 3, 2, 3, 4, 9, 4, 2, 9, 10, 1, 3, 9], [6, 4, 20, 8, 16, 7, 12, 16, 5, 4, 6, 20, 11, 19, 17, 9, 12, 2, 18, 12])\n\njulia> edges2 = (rand(1:nB, 30), rand(1:nA, 30))\n([17, 5, 2, 4, 5, 3, 8, 7, 9, 7  …  19, 8, 20, 7, 16, 2, 9, 15, 8, 13], [1, 1, 3, 1, 1, 3, 2, 7, 4, 4  …  7, 10, 6, 3, 4, 9, 1, 5, 8, 5])\n\njulia> data = ((:A, :rel1, :B) => edges1, (:B, :rel2, :A) => edges2);\n\njulia> hg = GNNHeteroGraph(data; num_nodes)\nGNNHeteroGraph:\n  num_nodes: (:A => 10, :B => 20)\n  num_edges: ((:A, :rel1, :B) => 20, (:B, :rel2, :A) => 30)\n\njulia> hg.num_edges\nDict{Tuple{Symbol, Symbol, Symbol}, Int64} with 2 entries:\n(:A, :rel1, :B) => 20\n(:B, :rel2, :A) => 30\n\n# Let's add some node features\njulia> ndata = Dict(:A => (x = rand(2, nA), y = rand(3, num_nodes[:A])),\n                    :B => rand(10, nB));\n\njulia> hg = GNNHeteroGraph(data; num_nodes, ndata)\nGNNHeteroGraph:\n    num_nodes: (:A => 10, :B => 20)\n    num_edges: ((:A, :rel1, :B) => 20, (:B, :rel2, :A) => 30)\n    ndata:\n    :A  =>  (x = 2×10 Matrix{Float64}, y = 3×10 Matrix{Float64})\n    :B  =>  x = 10×20 Matrix{Float64}\n\n# Access features of nodes of type :A\njulia> hg.ndata[:A].x\n2×10 Matrix{Float64}:\n    0.825882  0.0797502  0.245813  0.142281  0.231253  0.685025  0.821457  0.888838  0.571347   0.53165\n    0.631286  0.316292   0.705325  0.239211  0.533007  0.249233  0.473736  0.595475  0.0623298  0.159307 See also  GNNGraph  for a homogeneous graph type and  rand_heterograph  for a function to generate random heterographs. source"},{"id":713,"pagetitle":"GNNHeteroGraph","title":"GNNGraphs.edge_type_subgraph","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/heterograph/#GNNGraphs.edge_type_subgraph-Tuple{GNNHeteroGraph, Tuple{Symbol, Symbol, Symbol}}","content":" GNNGraphs.edge_type_subgraph  —  Method edge_type_subgraph(g::GNNHeteroGraph, edge_ts) Return a subgraph of  g  that contains only the edges of type  edge_ts . Edge types can be specified as a single edge type (i.e. a tuple containing 3 symbols) or a vector of edge types. source"},{"id":714,"pagetitle":"GNNHeteroGraph","title":"GNNGraphs.num_edge_types","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/heterograph/#GNNGraphs.num_edge_types-Tuple{GNNGraph}","content":" GNNGraphs.num_edge_types  —  Method num_edge_types(g) Return the number of edge types in the graph. For  GNNGraph s, this is always 1. For  GNNHeteroGraph s, this is the number of unique edge types. source"},{"id":715,"pagetitle":"GNNHeteroGraph","title":"GNNGraphs.num_node_types","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/heterograph/#GNNGraphs.num_node_types-Tuple{GNNGraph}","content":" GNNGraphs.num_node_types  —  Method num_node_types(g) Return the number of node types in the graph. For  GNNGraph s, this is always 1. For  GNNHeteroGraph s, this is the number of unique node types. source"},{"id":716,"pagetitle":"GNNHeteroGraph","title":"Query","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/heterograph/#Query","content":" Query"},{"id":717,"pagetitle":"GNNHeteroGraph","title":"GNNGraphs.edge_index","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/heterograph/#GNNGraphs.edge_index-Tuple{GNNHeteroGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}, Tuple{Symbol, Symbol, Symbol}}","content":" GNNGraphs.edge_index  —  Method edge_index(g::GNNHeteroGraph, [edge_t]) Return a tuple containing two vectors, respectively storing the source and target nodes for each edges in  g  of type  edge_t = (src_t, rel_t, trg_t) . If  edge_t  is not provided, it will error if  g  has more than one edge type. source"},{"id":718,"pagetitle":"GNNHeteroGraph","title":"GNNGraphs.graph_indicator","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/heterograph/#GNNGraphs.graph_indicator-Tuple{GNNHeteroGraph}","content":" GNNGraphs.graph_indicator  —  Method graph_indicator(g::GNNHeteroGraph, [node_t]) Return a Dict of vectors containing the graph membership (an integer from  1  to  g.num_graphs ) of each node in the graph for each node type. If  node_t  is provided, return the graph membership of each node of type  node_t  instead. See also  batch . source"},{"id":719,"pagetitle":"GNNHeteroGraph","title":"Graphs.degree","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/heterograph/#Graphs.degree-Union{Tuple{TT}, Tuple{GNNHeteroGraph, Tuple{Symbol, Symbol, Symbol}}, Tuple{GNNHeteroGraph, Tuple{Symbol, Symbol, Symbol}, TT}} where TT<:Union{Nothing, Type{<:Number}}","content":" Graphs.degree  —  Method degree(g::GNNHeteroGraph, edge_type::EType; dir = :in) Return a vector containing the degrees of the nodes in  g  GNNHeteroGraph given  edge_type . Arguments g : A graph. edge_type : A tuple of symbols  (source_t, edge_t, target_t)  representing the edge type. T : Element type of the returned vector. If  nothing , is      chosen based on the graph type. Default  nothing . dir : For  dir = :out  the degree of a node is counted based on the outgoing edges.        For  dir = :in , the ingoing edges are used. If  dir = :both  we have the sum of the two.        Default  dir = :out . source"},{"id":720,"pagetitle":"GNNHeteroGraph","title":"Graphs.has_edge","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/heterograph/#Graphs.has_edge-Tuple{GNNHeteroGraph, Tuple{Symbol, Symbol, Symbol}, Integer, Integer}","content":" Graphs.has_edge  —  Method has_edge(g::GNNHeteroGraph, edge_t, i, j) Return  true  if there is an edge of type  edge_t  from node  i  to node  j  in  g . Examples julia> g = rand_bipartite_heterograph((2, 2), (4, 0), bidirected=false)\nGNNHeteroGraph:\n  num_nodes: Dict(:A => 2, :B => 2)\n  num_edges: Dict((:A, :to, :B) => 4, (:B, :to, :A) => 0)\n\njulia> has_edge(g, (:A,:to,:B), 1, 1)\ntrue\n\njulia> has_edge(g, (:B,:to,:A), 1, 1)\nfalse source"},{"id":721,"pagetitle":"GNNHeteroGraph","title":"Transform","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/heterograph/#Transform","content":" Transform"},{"id":722,"pagetitle":"GNNHeteroGraph","title":"GNNGraphs.add_edges","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/heterograph/#GNNGraphs.add_edges-Tuple{GNNHeteroGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}, Tuple{Symbol, Symbol, Symbol}, AbstractVector, AbstractVector}","content":" GNNGraphs.add_edges  —  Method add_edges(g::GNNHeteroGraph, edge_t, s, t; [edata, num_nodes])\nadd_edges(g::GNNHeteroGraph, edge_t => (s, t); [edata, num_nodes])\nadd_edges(g::GNNHeteroGraph, edge_t => (s, t, w); [edata, num_nodes]) Add to heterograph  g  edges of type  edge_t  with source node vector  s  and target node vector  t . Optionally, pass the  edge weights  w  or the features   edata  for the new edges.  edge_t  is a triplet of symbols  (src_t, rel_t, dst_t) .  If the edge type is not already present in the graph, it is added.  If it involves new node types, they are added to the graph as well. In this case, a dictionary or named tuple of  num_nodes  can be passed to specify the number of nodes of the new types, otherwise the number of nodes is inferred from the maximum node id in  s  and  t . source"},{"id":723,"pagetitle":"GNNHeteroGraph","title":"GNNGraphs.add_self_loops","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/heterograph/#GNNGraphs.add_self_loops-Tuple{GNNHeteroGraph{<:Tuple{T, T, V} where {T<:(AbstractVector{<:Integer}), V<:Union{Nothing, AbstractVector}}}, Tuple{Symbol, Symbol, Symbol}}","content":" GNNGraphs.add_self_loops  —  Method add_self_loops(g::GNNHeteroGraph, edge_t::EType)\nadd_self_loops(g::GNNHeteroGraph) If the source node type is the same as the destination node type in  edge_t , return a graph with the same features as  g  but also add self-loops  of the specified type,  edge_t . Otherwise, it returns  g  unchanged. Nodes with already existing self-loops of type  edge_t  will obtain  a second set of self-loops of the same type. If the graph has edge weights for edges of type  edge_t , the new edges will have weight 1. If no edges of type  edge_t  exist, or all existing edges have no weight,  then all new self loops will have no weight. If  edge_t  is not passed as argument, for the entire graph self-loop is added to each node for every edge type in the graph where the source and destination node types are the same.  This iterates over all edge types present in the graph, applying the self-loop addition logic to each applicable edge type. source"},{"id":724,"pagetitle":"GNNHeteroGraph","title":"Generate","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/heterograph/#Generate","content":" Generate"},{"id":725,"pagetitle":"GNNHeteroGraph","title":"GNNGraphs.rand_bipartite_heterograph","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/heterograph/#GNNGraphs.rand_bipartite_heterograph-Tuple{Any, Any}","content":" GNNGraphs.rand_bipartite_heterograph  —  Method rand_bipartite_heterograph([rng,] \n                           (n1, n2), (m12, m21); \n                           bidirected = true, \n                           node_t = (:A, :B), \n                           edge_t = :to, \n                           kws...) Construct an  GNNHeteroGraph  with random edges representing a bipartite graph. The graph will have two types of nodes, and edges will only connect nodes of different types. The first argument is a tuple  (n1, n2)  specifying the number of nodes of each type. The second argument is a tuple  (m12, m21)  specifying the number of edges connecting nodes of type  1  to nodes of type  2   and vice versa. The type of nodes and edges can be specified with the  node_t  and  edge_t  keyword arguments, which default to  (:A, :B)  and  :to  respectively. If  bidirected=true  (default), the reverse edge of each edge will be present. In this case  m12 == m21  is required. A random number generator can be passed as the first argument to make the generation reproducible. Additional keyword arguments will be passed to the  GNNHeteroGraph  constructor. See  rand_heterograph  for a more general version. Examples julia> g = rand_bipartite_heterograph((10, 15), 20)\nGNNHeteroGraph:\n  num_nodes: (:A => 10, :B => 15)\n  num_edges: ((:A, :to, :B) => 20, (:B, :to, :A) => 20)\n\njulia> g = rand_bipartite_heterograph((10, 15), (20, 0), node_t=(:user, :item), edge_t=:-, bidirected=false)\nGNNHeteroGraph:\n  num_nodes: Dict(:item => 15, :user => 10)\n  num_edges: Dict((:item, :-, :user) => 0, (:user, :-, :item) => 20) source"},{"id":726,"pagetitle":"GNNHeteroGraph","title":"GNNGraphs.rand_heterograph","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/heterograph/#GNNGraphs.rand_heterograph","content":" GNNGraphs.rand_heterograph  —  Function rand_heterograph([rng,] n, m; bidirected=false, kws...) Construct an  GNNHeteroGraph  with random edges and with number of nodes and edges  specified by  n  and  m  respectively.  n  and  m  can be any iterable of pairs specifing node/edge types and their numbers. Pass a random number generator as a first argument to make the generation reproducible. Setting  bidirected=true  will generate a bidirected graph, i.e. each edge will have a reverse edge. Therefore, for each edge type  (:A, :rel, :B)  a corresponding reverse edge type  (:B, :rel, :A)  will be generated. Additional keyword arguments will be passed to the  GNNHeteroGraph  constructor. Examples julia> g = rand_heterograph((:user => 10, :movie => 20),\n                            (:user, :rate, :movie) => 30)\nGNNHeteroGraph:\n  num_nodes: Dict(:movie => 20, :user => 10)\n  num_edges: Dict((:user, :rate, :movie) => 30) source"},{"id":729,"pagetitle":"TemporalSnapshotsGNNGraph","title":"Temporal Graphs","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/temporalgraph/#Temporal-Graphs","content":" Temporal Graphs"},{"id":730,"pagetitle":"TemporalSnapshotsGNNGraph","title":"TemporalSnapshotsGNNGraph","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/temporalgraph/#TemporalSnapshotsGNNGraph","content":" TemporalSnapshotsGNNGraph Documentation page for the graph type  TemporalSnapshotsGNNGraph  and related methods, representing time varying graphs with time varying features."},{"id":731,"pagetitle":"TemporalSnapshotsGNNGraph","title":"GNNGraphs.TemporalSnapshotsGNNGraph","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/temporalgraph/#GNNGraphs.TemporalSnapshotsGNNGraph","content":" GNNGraphs.TemporalSnapshotsGNNGraph  —  Type TemporalSnapshotsGNNGraph(snapshots) A type representing a time-varying graph as a sequence of snapshots, each snapshot being a  GNNGraph . The argument  snapshots  is a collection of  GNNGraph s with arbitrary  number of nodes and edges each.  Calling  tg  the temporal graph,  tg[t]  returns the  t -th snapshot. The snapshots can contain node/edge/graph features, while global features for the whole temporal sequence can be stored in  tg.tgdata . See  add_snapshot  and  remove_snapshot  for adding and removing snapshots. Examples julia> snapshots = [rand_graph(i , 2*i) for i in 10:10:50];\n\njulia> tg = TemporalSnapshotsGNNGraph(snapshots)\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 20, 30, 40, 50]\n  num_edges: [20, 40, 60, 80, 100]\n  num_snapshots: 5\n\njulia> tg.num_snapshots\n5\n\njulia> tg.num_nodes\n5-element Vector{Int64}:\n 10\n 20\n 30\n 40\n 50\n\njulia> tg[1]\nGNNGraph:\n  num_nodes: 10\n  num_edges: 20\n\njulia> tg[2:3]\nTemporalSnapshotsGNNGraph:\n  num_nodes: [20, 30]\n  num_edges: [40, 60]\n  num_snapshots: 2\n\njulia> tg[1] = rand_graph(10, 16)\nGNNGraph:\n  num_nodes: 10\n  num_edges: 16 source"},{"id":732,"pagetitle":"TemporalSnapshotsGNNGraph","title":"GNNGraphs.add_snapshot","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/temporalgraph/#GNNGraphs.add_snapshot-Tuple{TemporalSnapshotsGNNGraph, Int64, GNNGraph}","content":" GNNGraphs.add_snapshot  —  Method add_snapshot(tg::TemporalSnapshotsGNNGraph, t::Int, g::GNNGraph) Return a  TemporalSnapshotsGNNGraph  created starting from  tg  by adding the snapshot  g  at time index  t . Examples julia> snapshots = [rand_graph(10, 20) for i in 1:5];\n\njulia> tg = TemporalSnapshotsGNNGraph(snapshots)\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 10, 10, 10, 10]\n  num_edges: [20, 20, 20, 20, 20]\n  num_snapshots: 5\n\njulia> new_tg = add_snapshot(tg, 3, rand_graph(10, 16)) # add a new snapshot at time 3\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 10, 10, 10, 10, 10]\n  num_edges: [20, 20, 16, 20, 20, 20]\n  num_snapshots: 6 source"},{"id":733,"pagetitle":"TemporalSnapshotsGNNGraph","title":"GNNGraphs.remove_snapshot","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/temporalgraph/#GNNGraphs.remove_snapshot-Tuple{TemporalSnapshotsGNNGraph, Int64}","content":" GNNGraphs.remove_snapshot  —  Method remove_snapshot(tg::TemporalSnapshotsGNNGraph, t::Int) Return a  TemporalSnapshotsGNNGraph  created starting from  tg  by removing the snapshot at time index  t . Examples julia> using GNNGraphs\n\njulia> snapshots = [rand_graph(10,20), rand_graph(10,14), rand_graph(10,22)];\n\njulia> tg = TemporalSnapshotsGNNGraph(snapshots)\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 10, 10]\n  num_edges: [20, 14, 22]\n  num_snapshots: 3\n\njulia> new_tg = remove_snapshot(tg, 2) # remove snapshot at time 2\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 10]\n  num_edges: [20, 22]\n  num_snapshots: 2 source"},{"id":734,"pagetitle":"TemporalSnapshotsGNNGraph","title":"Random Generators","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/temporalgraph/#Random-Generators","content":" Random Generators"},{"id":735,"pagetitle":"TemporalSnapshotsGNNGraph","title":"GNNGraphs.rand_temporal_radius_graph","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/temporalgraph/#GNNGraphs.rand_temporal_radius_graph","content":" GNNGraphs.rand_temporal_radius_graph  —  Function rand_temporal_radius_graph(number_nodes::Int, \n                           number_snapshots::Int,\n                           speed::AbstractFloat,\n                           r::AbstractFloat;\n                           self_loops = false,\n                           dir = :in,\n                           kws...) Create a random temporal graph given  number_nodes  nodes and  number_snapshots  snapshots. First, the positions of the nodes are randomly generated in the unit square. Two nodes are connected if their distance is less than a given radius  r . Each following snapshot is obtained by applying the same construction to new positions obtained as follows. For each snapshot, the new positions of the points are determined by applying random independent displacement vectors to the previous positions. The direction of the displacement is chosen uniformly at random and its length is chosen uniformly in  [0, speed] . Then the connections are recomputed. If a point happens to move outside the boundary, its position is updated as if it had bounced off the boundary. Arguments number_nodes : The number of nodes of each snapshot. number_snapshots : The number of snapshots. speed : The speed to update the nodes. r : The radius of connection. self_loops : If  true , consider the node itself among its neighbors, in which               case the graph will contain self-loops.  dir : The direction of the edges. If  dir=:in  edges go from the        neighbors to the central node. If  dir=:out  we have the opposite        direction. kws : Further keyword arguments will be passed to the  GNNGraph  constructor of each snapshot. Example julia> n, snaps, s, r = 10, 5, 0.1, 1.5;\n\njulia> tg = rand_temporal_radius_graph(n,snaps,s,r) # complete graph at each snapshot\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 10, 10, 10, 10]\n  num_edges: [90, 90, 90, 90, 90]\n  num_snapshots: 5 source"},{"id":736,"pagetitle":"TemporalSnapshotsGNNGraph","title":"GNNGraphs.rand_temporal_hyperbolic_graph","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/api/temporalgraph/#GNNGraphs.rand_temporal_hyperbolic_graph","content":" GNNGraphs.rand_temporal_hyperbolic_graph  —  Function rand_temporal_hyperbolic_graph(number_nodes::Int, \n                               number_snapshots::Int;\n                               α::Real,\n                               R::Real,\n                               speed::Real,\n                               ζ::Real=1,\n                               self_loop = false,\n                               kws...) Create a random temporal graph given  number_nodes  nodes and  number_snapshots  snapshots. First, the positions of the nodes are generated with a quasi-uniform distribution (depending on the parameter  α ) in hyperbolic space within a disk of radius  R . Two nodes are connected if their hyperbolic distance is less than  R . Each following snapshot is created in order to keep the same initial distribution. Arguments number_nodes : The number of nodes of each snapshot. number_snapshots : The number of snapshots. α : The parameter that controls the position of the points. If  α=ζ , the points are uniformly distributed on the disk of radius  R . If  α>ζ , the points are more concentrated in the center of the disk. If  α<ζ , the points are more concentrated at the boundary of the disk. R : The radius of the disk and of connection. speed : The speed to update the nodes. ζ : The parameter that controls the curvature of the disk. self_loops : If  true , consider the node itself among its neighbors, in which               case the graph will contain self-loops. kws : Further keyword arguments will be passed to the  GNNGraph  constructor of each snapshot. Example julia> n, snaps, α, R, speed, ζ = 10, 5, 1.0, 4.0, 0.1, 1.0;\n\njulia> thg = rand_temporal_hyperbolic_graph(n, snaps; α, R, speed, ζ)\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 10, 10, 10, 10]\n  num_edges: [44, 46, 48, 42, 38]\n  num_snapshots: 5 References Section D of the paper  Dynamic Hidden-Variable Network Models  and the paper   Hyperbolic Geometry of Complex Networks source"},{"id":739,"pagetitle":"Datasets","title":"Datasets","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/guides/datasets/#Datasets","content":" Datasets GNNGraphs.jl doesn't come with its own datasets, but leverages those available in the Julia (and non-Julia) ecosystem. "},{"id":740,"pagetitle":"Datasets","title":"MLDatasets.jl","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/guides/datasets/#MLDatasets.jl","content":" MLDatasets.jl Some of the  examples in the GraphNeuralNetworks.jl repository  make use of the  MLDatasets.jl  package. There you will find common graph datasets such as Cora, PubMed, Citeseer, TUDataset and  many others . For graphs with static structures and temporal features, datasets such as METRLA, PEMSBAY, ChickenPox, and WindMillEnergy are available. For graphs featuring both temporal structures and temporal features, the TemporalBrains dataset is suitable. GraphNeuralNetworks.jl provides the  mldataset2gnngraph  method for interfacing with MLDatasets.jl."},{"id":741,"pagetitle":"Datasets","title":"PyGDatasets.jl","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/guides/datasets/#PyGDatasets.jl","content":" PyGDatasets.jl The package  PyGDatasets.jl  makes available to Julia users the datasets from the  pytorch geometric  library.  PyGDatasets' datasets are compatible with GNNGraphs, so no additional conversion is needed.  julia> using PyGDatasets\n\njulia> dataset = load_dataset(\"TUDataset\", name=\"MUTAG\")\nTUDataset(MUTAG) - InMemoryGNNDataset\n  num_graphs: 188\n  node_features: [:x]\n  edge_features: [:edge_attr]\n  graph_features: [:y]\n  root: /Users/carlo/.julia/scratchspaces/44f67abd-f36e-4be4-bfe5-65f468a62b3d/datasets/TUDataset\n\njulia> g = dataset[1]\nGNNGraph:\n  num_nodes: 17\n  num_edges: 38\n  ndata:\n    x = 7×17 Matrix{Float32}\n  edata:\n    edge_attr = 4×38 Matrix{Float32}\n  gdata:\n    y = 1-element Vector{Int64}\n\njulia> using MLUtils: DataLoader\n\njulia> data_loader = DataLoader(dataset, batch_size=32); PyGDatasets is based on  PythonCall.jl . It carries over some heavy dependencies such as python, pytorch and pytorch geometric."},{"id":744,"pagetitle":"Static Graphs","title":"Static Graphs","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/guides/gnngraph/#Static-Graphs","content":" Static Graphs The fundamental graph type in GNNGraphs.jl is the  GNNGraph . A GNNGraph  g  is a directed graph with nodes labeled from 1 to  g.num_nodes . The underlying implementation allows for efficient application of graph neural network operators, gpu movement, and storage of node/edge/graph related feature arrays. GNNGraph  inherits from  Graphs.jl 's  AbstractGraph , therefore it supports most functionality from that library. "},{"id":745,"pagetitle":"Static Graphs","title":"Graph Creation","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/guides/gnngraph/#Graph-Creation","content":" Graph Creation A GNNGraph can be created from several different data sources encoding the graph topology: using GNNGraphs, Graphs, SparseArrays\n\n\n# Construct a GNNGraph from from a Graphs.jl's graph\nlg = erdos_renyi(10, 30)\ng = GNNGraph(lg)\n\n# Same as above using convenience method rand_graph\ng = rand_graph(10, 60)\n\n# From an adjacency matrix\nA = sprand(10, 10, 0.3)\ng = GNNGraph(A)\n\n# From an adjacency list\nadjlist = [[2,3], [1,3], [1,2,4], [3]]\ng = GNNGraph(adjlist)\n\n# From COO representation\nsource = [1,1,2,2,3,3,3,4]\ntarget = [2,3,1,3,1,2,4,3]\ng = GNNGraph(source, target) See also the related methods  Graphs.adjacency_matrix ,  edge_index , and  adjacency_list ."},{"id":746,"pagetitle":"Static Graphs","title":"Basic Queries","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/guides/gnngraph/#Basic-Queries","content":" Basic Queries julia> source = [1,1,2,2,3,3,3,4];\n\njulia> target = [2,3,1,3,1,2,4,3];\n\njulia> g = GNNGraph(source, target)\nGNNGraph:\n  num_nodes: 4\n  num_edges: 8\n\n\njulia> @assert g.num_nodes == 4   # number of nodes\n\njulia> @assert g.num_edges == 8   # number of edges\n\njulia> @assert g.num_graphs == 1  # number of subgraphs (a GNNGraph can batch many graphs together)\n\njulia> is_directed(g)      # a GNNGraph is always directed\ntrue\n\njulia> is_bidirected(g)      # for each edge, also the reverse edge is present\ntrue\n\njulia> has_self_loops(g)\nfalse\n\njulia> has_multi_edges(g)      \nfalse"},{"id":747,"pagetitle":"Static Graphs","title":"Data Features","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/guides/gnngraph/#Data-Features","content":" Data Features One or more arrays can be associated to nodes, edges, and (sub)graphs of a  GNNGraph . They will be stored in the fields  g.ndata ,  g.edata , and  g.gdata  respectively. The data fields are  DataStore  objects.  DataStore s conveniently offer an interface similar to both dictionaries and named tuples. Similarly to dictionaries, DataStores support addition of new features after creation time. The array contained in the datastores have last dimension equal to  num_nodes  (in  ndata ),  num_edges  (in  edata ), or  num_graphs  (in  gdata ) respectively. # Create a graph with a single feature array `x` associated to nodes\ng = rand_graph(10,  60, ndata = (; x = rand(Float32, 32, 10)))\n\ng.ndata.x  # access the features\n\n# Equivalent definition passing directly the array\ng = rand_graph(10,  60, ndata = rand(Float32, 32, 10))\n\ng.ndata.x  # `:x` is the default name for node features\n\ng.ndata.z = rand(Float32, 3, 10)  # add new feature array `z`\n\n# For convenience, we can access the features through the shortcut\ng.x \n\n# You can have multiple feature arrays\ng = rand_graph(10,  60, ndata = (; x=rand(Float32, 32, 10), y=rand(Float32, 10)))\n\ng.ndata.y, g.ndata.x   # or g.x, g.y\n\n# Attach an array with edge features.\n# Since `GNNGraph`s are directed, the number of edges\n# will be double that of the original Graphs' undirected graph.\ng = GNNGraph(erdos_renyi(10,  30), edata = rand(Float32, 60))\n@assert g.num_edges == 60\n\ng.edata.e  # or g.e\n\n# If we pass only half of the edge features, they will be copied\n# on the reversed edges.\ng = GNNGraph(erdos_renyi(10,  30), edata = rand(Float32, 30))\n\n\n# Create a new graph from previous one, inheriting edge data\n# but replacing node data\ng′ = GNNGraph(g, ndata =(; z = ones(Float32, 16, 10)))\n\ng′.z\ng′.e"},{"id":748,"pagetitle":"Static Graphs","title":"Edge weights","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/guides/gnngraph/#Edge-weights","content":" Edge weights It is common to denote scalar edge features as edge weights. The  GNNGraph  has specific support for edge weights: they can be stored as part of internal representations of the graph (COO or adjacency matrix). Some graph convolutional layers, most notably the  GCNConv , can use the edge weights to perform weighted sums over the nodes' neighborhoods. julia> source = [1, 1, 2, 2, 3, 3];\n\njulia> target = [2, 3, 1, 3, 1, 2];\n\njulia> weight = [1.0, 0.5, 2.1, 2.3, 4, 4.1];\n\njulia> g = GNNGraph(source, target, weight)\nGNNGraph:\n  num_nodes: 3\n  num_edges: 6\n\njulia> get_edge_weight(g)\n6-element Vector{Float64}:\n 1.0\n 0.5\n 2.1\n 2.3\n 4.0\n 4.1"},{"id":749,"pagetitle":"Static Graphs","title":"Batches and Subgraphs","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/guides/gnngraph/#Batches-and-Subgraphs","content":" Batches and Subgraphs Multiple  GNNGraph s can be batched together into a single graph that contains the total number of the original nodes  and where the original graphs are disjoint subgraphs. using MLUtils\n\ndata = [rand_graph(10, 30, ndata=rand(Float32, 3, 10)) for _ in 1:160]\ngall = MLUtils.batch(data)\n\n# gall is a GNNGraph containing many graphs\n@assert gall.num_graphs == 160 \n@assert gall.num_nodes == 1600   # 10 nodes x 160 graphs\n@assert gall.num_edges == 4800  # 30 undirected edges x 160 graphs\n\n# Let's create a mini-batch from gall\ng23 = getgraph(gall, 2:3)\n@assert g23.num_graphs == 2\n@assert g23.num_nodes == 20   # 10 nodes x 2 graphs\n@assert g23.num_edges == 60  # 30 undirected edges X 2 graphs\n\n# We can pass a GNNGraph to MLUtils' DataLoader\ntrain_loader = DataLoader(gall, batchsize=16, shuffle=true)\n\nfor g in train_loader\n    @assert g.num_graphs == 16\n    @assert g.num_nodes == 160\n    @assert size(g.ndata.x) = (3, 160)    \n    # .....\nend\n\n# Access the nodes' graph memberships \ngraph_indicator(gall)"},{"id":750,"pagetitle":"Static Graphs","title":"DataLoader and mini-batch iteration","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/guides/gnngraph/#DataLoader-and-mini-batch-iteration","content":" DataLoader and mini-batch iteration While constructing a batched graph and passing it to the  DataLoader  is always  an option for mini-batch iteration, the recommended way for better performance is to pass an array of graphs directly and set the  collate  option to  true : using MLUtils: DataLoader\n\ndata = [rand_graph(10, 30, ndata=rand(Float32, 3, 10)) for _ in 1:320]\n\ntrain_loader = DataLoader(data, batchsize=16, shuffle=true, collate=true)\n\nfor g in train_loader\n    @assert g.num_graphs == 16\n    @assert g.num_nodes == 160\n    @assert size(g.ndata.x) = (3, 160)    \n    # .....\nend"},{"id":751,"pagetitle":"Static Graphs","title":"Graph Manipulation","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/guides/gnngraph/#Graph-Manipulation","content":" Graph Manipulation g′ = add_self_loops(g)\ng′ = remove_self_loops(g)\ng′ = add_edges(g, [1, 2], [2, 3]) # add edges 1->2 and 2->3"},{"id":752,"pagetitle":"Static Graphs","title":"GPU movement","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/guides/gnngraph/#GPU-movement","content":" GPU movement Move a  GNNGraph  to a CUDA device using  Flux.gpu  method.  using Flux, CUDA # or using Metal or using AMDGPU \n\ng_gpu = g |> Flux.gpu"},{"id":753,"pagetitle":"Static Graphs","title":"Integration with Graphs.jl","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/guides/gnngraph/#Integration-with-Graphs.jl","content":" Integration with Graphs.jl Since  GNNGraph <: Graphs.AbstractGraph , we can use any functionality from  Graphs.jl  for querying and analyzing the graph structure.  Moreover, a  GNNGraph  can be easily constructed from a  Graphs.Graph  or a  Graphs.DiGraph : julia> import Graphs\n\njulia> using GNNGraphs\n\n# A Graphs.jl undirected graph\njulia> gu = Graphs.erdos_renyi(10, 20)    \n{10, 20} undirected simple Int64 graph\n\n# Since GNNGraphs are undirected, the edges are doubled when converting \n# to GNNGraph\njulia> GNNGraph(gu)\nGNNGraph:\n  num_nodes: 10\n  num_edges: 40\n\n# A Graphs.jl directed graph\njulia> gd = Graphs.erdos_renyi(10, 20, is_directed=true)\n{10, 20} directed simple Int64 graph\n\njulia> GNNGraph(gd)\nGNNGraph:\n  num_nodes: 10\n  num_edges: 20"},{"id":756,"pagetitle":"Heterogeneous Graphs","title":"Heterogeneous Graphs","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/guides/heterograph/#Heterogeneous-Graphs","content":" Heterogeneous Graphs Heterogeneous graphs (also called heterographs), are graphs where each node has a type, that we denote with symbols such as  :user  and  :movie . Relations such as  :rate  or  :like  can connect nodes of different types. We call a triplet  (source_node_type, relation_type, target_node_type)  the type of a edge, e.g.  (:user, :rate, :movie) . Different node/edge types can store different groups of features and this makes heterographs a very flexible modeling tools  and data containers. In GNNGraphs.jl heterographs are implemented in  the type  GNNHeteroGraph ."},{"id":757,"pagetitle":"Heterogeneous Graphs","title":"Creating a Heterograph","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/guides/heterograph/#Creating-a-Heterograph","content":" Creating a Heterograph A heterograph can be created empty or by passing pairs  edge_type => data  to the constructor. julia> using GNNGraphs\n\njulia> g = GNNHeteroGraph()\nGNNHeteroGraph:\n  num_nodes: Dict()\n  num_edges: Dict()\n  \njulia> g = GNNHeteroGraph((:user, :like, :actor) => ([1,2,2,3], [1,3,2,9]),\n                          (:user, :rate, :movie) => ([1,1,2,3], [7,13,5,7]))\nGNNHeteroGraph:\n  num_nodes: Dict(:actor => 9, :movie => 13, :user => 3)\n  num_edges: Dict((:user, :like, :actor) => 4, (:user, :rate, :movie) => 4)\n\njulia> g = GNNHeteroGraph((:user, :rate, :movie) => ([1,1,2,3], [7,13,5,7]))\nGNNHeteroGraph:\n  num_nodes: Dict(:movie => 13, :user => 3)\n  num_edges: Dict((:user, :rate, :movie) => 4) New relations, possibly with new node types, can be added with the function  add_edges . julia> g = add_edges(g, (:user, :like, :actor) => ([1,2,3,3,3], [3,5,1,9,4]))\nGNNHeteroGraph:\n  num_nodes: Dict(:actor => 9, :movie => 13, :user => 3)\n  num_edges: Dict((:user, :like, :actor) => 5, (:user, :rate, :movie) => 4) See  rand_heterograph ,  rand_bipartite_heterograph  for generating random heterographs.  julia> g = rand_bipartite_heterograph((10, 15), 20)\nGNNHeteroGraph:\n  num_nodes: Dict(:A => 10, :B => 15)\n  num_edges: Dict((:A, :to, :B) => 20, (:B, :to, :A) => 20)"},{"id":758,"pagetitle":"Heterogeneous Graphs","title":"Basic Queries","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/guides/heterograph/#Basic-Queries","content":" Basic Queries Basic queries are similar to those for homogeneous graphs: julia> g = GNNHeteroGraph((:user, :rate, :movie) => ([1,1,2,3], [7,13,5,7]))\nGNNHeteroGraph:\n  num_nodes: Dict(:movie => 13, :user => 3)\n  num_edges: Dict((:user, :rate, :movie) => 4)\n\njulia> g.num_nodes\nDict{Symbol, Int64} with 2 entries:\n  :user  => 3\n  :movie => 13\n\njulia> g.num_edges\nDict{Tuple{Symbol, Symbol, Symbol}, Int64} with 1 entry:\n  (:user, :rate, :movie) => 4\n\njulia> edge_index(g, (:user, :rate, :movie)) # source and target node for a given relation\n([1, 1, 2, 3], [7, 13, 5, 7])\n\njulia> g.ntypes  # node types\n2-element Vector{Symbol}:\n :user\n :movie\n\njulia> g.etypes  # edge types\n1-element Vector{Tuple{Symbol, Symbol, Symbol}}:\n (:user, :rate, :movie)"},{"id":759,"pagetitle":"Heterogeneous Graphs","title":"Data Features","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/guides/heterograph/#Data-Features","content":" Data Features Node, edge, and graph features can be added at construction time or later using: # equivalent to g.ndata[:user][:x] = ...\njulia> g[:user].x = rand(Float32, 64, 3);\n\njulia> g[:movie].z = rand(Float32, 64, 13);\n\n# equivalent to g.edata[(:user, :rate, :movie)][:e] = ...\njulia> g[:user, :rate, :movie].e = rand(Float32, 64, 4);\n\njulia> g\nGNNHeteroGraph:\n  num_nodes: Dict(:movie => 13, :user => 3)\n  num_edges: Dict((:user, :rate, :movie) => 4)\n  ndata:\n        :movie  =>  DataStore(z = [64×13 Matrix{Float32}])\n        :user  =>  DataStore(x = [64×3 Matrix{Float32}])\n  edata:\n        (:user, :rate, :movie)  =>  DataStore(e = [64×4 Matrix{Float32}])"},{"id":760,"pagetitle":"Heterogeneous Graphs","title":"Batching","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/guides/heterograph/#Batching","content":" Batching Similarly to graphs, also heterographs can be batched together. julia> gs = [rand_bipartite_heterograph((5, 10), 20) for _ in 1:32];\n\njulia> MLUtils.batch(gs)\nGNNHeteroGraph:\n  num_nodes: Dict(:A => 160, :B => 320)\n  num_edges: Dict((:A, :to, :B) => 640, (:B, :to, :A) => 640)\n  num_graphs: 32 Batching is automatically performed by the  DataLoader  iterator when the  collate  option is set to  true . using MLUtils: DataLoader\n\ndata = [rand_bipartite_heterograph((5, 10), 20, \n            ndata=Dict(:A=>rand(Float32, 3, 5))) \n        for _ in 1:320];\n\ntrain_loader = DataLoader(data, batchsize=16, shuffle=true, collate=true)\n\nfor g in train_loader\n    @assert g.num_graphs == 16\n    @assert g.num_nodes[:A] == 80\n    @assert size(g.ndata[:A].x) == (3, 80)    \n    # ...\nend"},{"id":761,"pagetitle":"Heterogeneous Graphs","title":"Graph convolutions on heterographs","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/guides/heterograph/#Graph-convolutions-on-heterographs","content":" Graph convolutions on heterographs See  HeteroGraphConv  for how to perform convolutions on heterogeneous graphs."},{"id":764,"pagetitle":"Temporal Graphs","title":"Temporal Graphs","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/guides/temporalgraph/#Temporal-Graphs","content":" Temporal Graphs Temporal graphs are graphs with time-varying topologies and features. In GNNGraphs.jl, they are represented by the  TemporalSnapshotsGNNGraph  type."},{"id":765,"pagetitle":"Temporal Graphs","title":"Creating a TemporalSnapshotsGNNGraph","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/guides/temporalgraph/#Creating-a-TemporalSnapshotsGNNGraph","content":" Creating a TemporalSnapshotsGNNGraph A temporal graph can be created by passing a list of snapshots to the constructor. Each snapshot is a  GNNGraph .  julia> using GNNGraphs\n\njulia> snapshots = [rand_graph(10, 20) for i in 1:5];\n\njulia> tg = TemporalSnapshotsGNNGraph(snapshots)\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 10, 10, 10, 10]\n  num_edges: [20, 20, 20, 20, 20]\n  num_snapshots: 5 A new temporal graph can be created by adding or removing snapshots to an existing temporal graph.  julia> new_tg = add_snapshot(tg, 3, rand_graph(10, 16)) # add a new snapshot at time 3\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 10, 10, 10, 10, 10]\n  num_edges: [20, 20, 16, 20, 20, 20]\n  num_snapshots: 6 julia> snapshots = [rand_graph(10,20), rand_graph(10,14), rand_graph(10,22)];\n\njulia> tg = TemporalSnapshotsGNNGraph(snapshots)\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 10, 10]\n  num_edges: [20, 14, 22]\n  num_snapshots: 3\n\njulia> new_tg = remove_snapshot(tg, 2) # remove snapshot at time 2\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 10]\n  num_edges: [20, 22]\n  num_snapshots: 2 See  rand_temporal_radius_graph  and  rand_temporal_hyperbolic_graph  for generating random temporal graphs.  julia> tg = rand_temporal_radius_graph(10, 3, 0.1, 0.5)\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 10, 10]\n  num_edges: [32, 30, 34]\n  num_snapshots: 3"},{"id":766,"pagetitle":"Temporal Graphs","title":"Indexing","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/guides/temporalgraph/#Indexing","content":" Indexing Snapshots in a temporal graph can be accessed using indexing: julia> snapshots = [rand_graph(10, 20), rand_graph(10, 14), rand_graph(10, 22)];\n\njulia> tg = TemporalSnapshotsGNNGraph(snapshots)\n\njulia> tg[1] # first snapshot\nGNNGraph:\n  num_nodes: 10\n  num_edges: 20\n\njulia> tg[2:3] # snapshots 2 and 3\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 10]\n  num_edges: [14, 22]\n  num_snapshots: 2 A snapshot can be modified by assigning a new snapshot to the temporal graph: julia> tg[1] = rand_graph(10, 16) # replace first snapshot\nGNNGraph:\n  num_nodes: 10\n  num_edges: 16"},{"id":767,"pagetitle":"Temporal Graphs","title":"Iteration and Broadcasting","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/guides/temporalgraph/#Iteration-and-Broadcasting","content":" Iteration and Broadcasting Iteration and broadcasting over a temporal graph is similar to that of a vector of snapshots: julia> snapshots = [rand_graph(10, 20), rand_graph(10, 14), rand_graph(10, 22)];\n\njulia> tg = TemporalSnapshotsGNNGraph(snapshots);\n\njulia> [g for g in tg] # iterate over snapshots\n3-element Vector{GNNGraph{Tuple{Vector{Int64}, Vector{Int64}, Nothing}}}:\n GNNGraph(10, 20) with no data\n GNNGraph(10, 14) with no data\n GNNGraph(10, 22) with no data\n\njulia> f(g) = g isa GNNGraph;\n\njulia> f.(tg) # broadcast over snapshots\n3-element BitVector:\n 1\n 1\n 1"},{"id":768,"pagetitle":"Temporal Graphs","title":"Basic Queries","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/guides/temporalgraph/#Basic-Queries","content":" Basic Queries Basic queries are similar to those for  GNNGraph s: julia> snapshots = [rand_graph(10,20), rand_graph(12,14), rand_graph(14,22)];\n\njulia> tg = TemporalSnapshotsGNNGraph(snapshots)\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 12, 14]\n  num_edges: [20, 14, 22]\n  num_snapshots: 3\n\njulia> tg.num_nodes         # number of nodes in each snapshot\n3-element Vector{Int64}:\n 10\n 12\n 14\n\njulia> tg.num_edges         # number of edges in each snapshot\n3-element Vector{Int64}:\n 20\n 14\n 22\n\njulia> tg.num_snapshots     # number of snapshots\n3\n\njulia> tg.snapshots         # list of snapshots\n3-element Vector{GNNGraph{Tuple{Vector{Int64}, Vector{Int64}, Nothing}}}:\n GNNGraph(10, 20) with no data\n GNNGraph(12, 14) with no data\n GNNGraph(14, 22) with no data\n\njulia> tg.snapshots[1]      # first snapshot, same as tg[1]\nGNNGraph:\n  num_nodes: 10\n  num_edges: 20"},{"id":769,"pagetitle":"Temporal Graphs","title":"Data Features","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/GNNGraphs/guides/temporalgraph/#Data-Features","content":" Data Features A temporal graph can store global feature for the entire time series in the  tgdata  field. Also, each snapshot can store node, edge, and graph features in the  ndata ,  edata , and  gdata  fields, respectively.  julia> snapshots = [rand_graph(10, 20; ndata = rand(Float32, 3, 10)), \n                    rand_graph(10, 14; ndata = rand(Float32, 4, 10)), \n                    rand_graph(10, 22; ndata = rand(Float32, 5, 10))]; # node features at construction time\n\njulia> tg = TemporalSnapshotsGNNGraph(snapshots);\n\njulia> tg.tgdata.y = rand(Float32, 3, 1); # add global features after construction\n\njulia> tg\nTemporalSnapshotsGNNGraph:\n  num_nodes: [10, 10, 10]\n  num_edges: [20, 14, 22]\n  num_snapshots: 3\n  tgdata:\n        y = 3×1 Matrix{Float32}\n\njulia> tg.ndata # vector of DataStore containing node features for each snapshot\n3-element Vector{DataStore}:\n DataStore(10) with 1 element:\n  x = 3×10 Matrix{Float32}\n DataStore(10) with 1 element:\n  x = 4×10 Matrix{Float32}\n DataStore(10) with 1 element:\n  x = 5×10 Matrix{Float32}\n\njulia> [ds.x for ds in tg.ndata]; # vector containing the x feature of each snapshot\n\njulia> [g.x for g in tg.snapshots]; # same vector as above, now accessing \n                                   # the x feature directly from the snapshots"},{"id":772,"pagetitle":"Message Passing","title":"Message Passing","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/api/messagepassing/#Message-Passing","content":" Message Passing"},{"id":773,"pagetitle":"Message Passing","title":"Interface","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/api/messagepassing/#Interface","content":" Interface"},{"id":774,"pagetitle":"Message Passing","title":"GNNlib.apply_edges","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/api/messagepassing/#GNNlib.apply_edges","content":" GNNlib.apply_edges  —  Function apply_edges(fmsg, g; [xi, xj, e])\napply_edges(fmsg, g, xi, xj, e=nothing) Returns the message from node  j  to node  i  applying the message function  fmsg  on the edges in graph  g . In the message-passing scheme, the incoming messages  from the neighborhood of  i  will later be aggregated in order to update the features of node  i  (see  aggregate_neighbors ). The function  fmsg  operates on batches of edges, therefore  xi ,  xj , and  e  are tensors whose last dimension is the batch size, or can be named tuples of  such tensors. Arguments g : An  AbstractGNNGraph . xi : An array or a named tuple containing arrays whose last dimension's size        is  g.num_nodes . It will be appropriately materialized on the       target node of each edge (see also  edge_index ). xj : As  xi , but now to be materialized on each edge's source node.  e : An array or a named tuple containing arrays whose last dimension's size is  g.num_edges . fmsg : A function that takes as inputs the edge-materialized  xi ,  xj , and  e .      These are arrays (or named tuples of arrays) whose last dimension' size is the size of      a batch of edges. The output of  f  has to be an array (or a named tuple of arrays)      with the same batch size. If also  layer  is passed to propagate,     the signature of  fmsg  has to be  fmsg(layer, xi, xj, e)       instead of  fmsg(xi, xj, e) . See also  propagate  and  aggregate_neighbors . source"},{"id":775,"pagetitle":"Message Passing","title":"GNNlib.aggregate_neighbors","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/api/messagepassing/#GNNlib.aggregate_neighbors","content":" GNNlib.aggregate_neighbors  —  Function aggregate_neighbors(g, aggr, m) Given a graph  g , edge features  m , and an aggregation operator  aggr  (e.g  +, min, max, mean ), returns the new node features  \\[\\mathbf{x}_i = \\square_{j \\in \\mathcal{N}(i)} \\mathbf{m}_{j\\to i}\\] Neighborhood aggregation is the second step of  propagate ,  where it comes after  apply_edges . source"},{"id":776,"pagetitle":"Message Passing","title":"GNNlib.propagate","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/api/messagepassing/#GNNlib.propagate","content":" GNNlib.propagate  —  Function propagate(fmsg, g, aggr; [xi, xj, e])\npropagate(fmsg, g, aggr xi, xj, e=nothing) Performs message passing on graph  g . Takes care of materializing the node features on each edge,  applying the message function  fmsg , and returning an aggregated message  $\\bar{\\mathbf{m}}$   (depending on the return value of  fmsg , an array or a named tuple of  arrays with last dimension's size  g.num_nodes ). It can be decomposed in two steps: m = apply_edges(fmsg, g, xi, xj, e)\nm̄ = aggregate_neighbors(g, aggr, m) GNN layers typically call  propagate  in their forward pass, providing as input  f  a closure.   Arguments g : A  GNNGraph . xi : An array or a named tuple containing arrays whose last dimension's size        is  g.num_nodes . It will be appropriately materialized on the       target node of each edge (see also  edge_index ). xj : As  xj , but to be materialized on edges' sources.  e : An array or a named tuple containing arrays whose last dimension's size is  g.num_edges . fmsg : A generic function that will be passed over to  apply_edges .      Has to take as inputs the edge-materialized  xi ,  xj , and  e       (arrays or named tuples of arrays whose last dimension' size is the size of      a batch of edges). Its output has to be an array or a named tuple of arrays     with the same batch size. If also  layer  is passed to propagate,     the signature of  fmsg  has to be  fmsg(layer, xi, xj, e)       instead of  fmsg(xi, xj, e) . aggr : Neighborhood aggregation operator. Use  + ,  mean ,  max , or  min .  Examples using GraphNeuralNetworks, Flux\n\nstruct GNNConv <: GNNLayer\n    W\n    b\n    σ\nend\n\nFlux.@layer GNNConv\n\nfunction GNNConv(ch::Pair{Int,Int}, σ=identity)\n    in, out = ch\n    W = Flux.glorot_uniform(out, in)\n    b = zeros(Float32, out)\n    GNNConv(W, b, σ)\nend\n\nfunction (l::GNNConv)(g::GNNGraph, x::AbstractMatrix)\n    message(xi, xj, e) = l.W * xj\n    m̄ = propagate(message, g, +, xj=x)\n    return l.σ.(m̄ .+ l.bias)\nend\n\nl = GNNConv(10 => 20)\nl(g, x) See also  apply_edges  and  aggregate_neighbors . source"},{"id":777,"pagetitle":"Message Passing","title":"Built-in message functions","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/api/messagepassing/#Built-in-message-functions","content":" Built-in message functions"},{"id":778,"pagetitle":"Message Passing","title":"GNNlib.copy_xi","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/api/messagepassing/#GNNlib.copy_xi","content":" GNNlib.copy_xi  —  Function copy_xi(xi, xj, e) = xi source"},{"id":779,"pagetitle":"Message Passing","title":"GNNlib.copy_xj","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/api/messagepassing/#GNNlib.copy_xj","content":" GNNlib.copy_xj  —  Function copy_xj(xi, xj, e) = xj source"},{"id":780,"pagetitle":"Message Passing","title":"GNNlib.xi_dot_xj","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/api/messagepassing/#GNNlib.xi_dot_xj","content":" GNNlib.xi_dot_xj  —  Function xi_dot_xj(xi, xj, e) = sum(xi .* xj, dims=1) source"},{"id":781,"pagetitle":"Message Passing","title":"GNNlib.xi_sub_xj","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/api/messagepassing/#GNNlib.xi_sub_xj","content":" GNNlib.xi_sub_xj  —  Function xi_sub_xj(xi, xj, e) = xi .- xj source"},{"id":782,"pagetitle":"Message Passing","title":"GNNlib.xj_sub_xi","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/api/messagepassing/#GNNlib.xj_sub_xi","content":" GNNlib.xj_sub_xi  —  Function xj_sub_xi(xi, xj, e) = xj .- xi source"},{"id":783,"pagetitle":"Message Passing","title":"GNNlib.e_mul_xj","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/api/messagepassing/#GNNlib.e_mul_xj","content":" GNNlib.e_mul_xj  —  Function e_mul_xj(xi, xj, e) = reshape(e, (...)) .* xj Reshape  e  into a broadcast compatible shape with  xj  (by prepending singleton dimensions) then perform broadcasted multiplication. source"},{"id":784,"pagetitle":"Message Passing","title":"GNNlib.w_mul_xj","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/api/messagepassing/#GNNlib.w_mul_xj","content":" GNNlib.w_mul_xj  —  Function w_mul_xj(xi, xj, w) = reshape(w, (...)) .* xj Similar to  e_mul_xj  but specialized on scalar edge features (weights). source"},{"id":787,"pagetitle":"Utils","title":"Utility Functions","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/api/utils/#Utility-Functions","content":" Utility Functions"},{"id":788,"pagetitle":"Utils","title":"Graph-wise operations","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/api/utils/#Graph-wise-operations","content":" Graph-wise operations"},{"id":789,"pagetitle":"Utils","title":"GNNlib.reduce_nodes","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/api/utils/#GNNlib.reduce_nodes","content":" GNNlib.reduce_nodes  —  Function reduce_nodes(aggr, g, x) For a batched graph  g , return the graph-wise aggregation of the node features  x . The aggregation operator  aggr  can be  + ,  mean ,  max , or  min . The returned array will have last dimension  g.num_graphs . See also:  reduce_edges . source reduce_nodes(aggr, indicator::AbstractVector, x) Return the graph-wise aggregation of the node features  x  given the graph indicator  indicator . The aggregation operator  aggr  can be  + ,  mean ,  max , or  min . See also  graph_indicator . source"},{"id":790,"pagetitle":"Utils","title":"GNNlib.reduce_edges","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/api/utils/#GNNlib.reduce_edges","content":" GNNlib.reduce_edges  —  Function reduce_edges(aggr, g, e) For a batched graph  g , return the graph-wise aggregation of the edge features  e . The aggregation operator  aggr  can be  + ,  mean ,  max , or  min . The returned array will have last dimension  g.num_graphs . source"},{"id":791,"pagetitle":"Utils","title":"GNNlib.softmax_nodes","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/api/utils/#GNNlib.softmax_nodes","content":" GNNlib.softmax_nodes  —  Function softmax_nodes(g, x) Graph-wise softmax of the node features  x . source"},{"id":792,"pagetitle":"Utils","title":"GNNlib.softmax_edges","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/api/utils/#GNNlib.softmax_edges","content":" GNNlib.softmax_edges  —  Function softmax_edges(g, e) Graph-wise softmax of the edge features  e . source"},{"id":793,"pagetitle":"Utils","title":"GNNlib.broadcast_nodes","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/api/utils/#GNNlib.broadcast_nodes","content":" GNNlib.broadcast_nodes  —  Function broadcast_nodes(g, x) Graph-wise broadcast array  x  of size  (*, g.num_graphs)   to size  (*, g.num_nodes) . source"},{"id":794,"pagetitle":"Utils","title":"GNNlib.broadcast_edges","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/api/utils/#GNNlib.broadcast_edges","content":" GNNlib.broadcast_edges  —  Function broadcast_edges(g, x) Graph-wise broadcast array  x  of size  (*, g.num_graphs)   to size  (*, g.num_edges) . source"},{"id":795,"pagetitle":"Utils","title":"Neighborhood operations","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/api/utils/#Neighborhood-operations","content":" Neighborhood operations"},{"id":796,"pagetitle":"Utils","title":"GNNlib.softmax_edge_neighbors","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/api/utils/#GNNlib.softmax_edge_neighbors","content":" GNNlib.softmax_edge_neighbors  —  Function softmax_edge_neighbors(g, e) Softmax over each node's neighborhood of the edge features  e . \\[\\mathbf{e}'_{j\\to i} = \\frac{e^{\\mathbf{e}_{j\\to i}}}\n                    {\\sum_{j'\\in N(i)} e^{\\mathbf{e}_{j'\\to i}}}.\\] source"},{"id":797,"pagetitle":"Utils","title":"NNlib's gather and scatter functions","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/api/utils/#NNlib's-gather-and-scatter-functions","content":" NNlib's gather and scatter functions Primitive functions for message passing implemented in  NNlib.jl : gather! gather scatter! scatter"},{"id":800,"pagetitle":"Message Passing","title":"Message Passing","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/guides/messagepassing/#Message-Passing","content":" Message Passing A generic message passing on graph takes the form \\[\\begin{aligned}\n\\mathbf{m}_{j\\to i} &= \\phi(\\mathbf{x}_i, \\mathbf{x}_j, \\mathbf{e}_{j\\to i}) \\\\\n\\bar{\\mathbf{m}}_{i} &= \\square_{j\\in N(i)}  \\mathbf{m}_{j\\to i} \\\\\n\\mathbf{x}_{i}' &= \\gamma_x(\\mathbf{x}_{i}, \\bar{\\mathbf{m}}_{i})\\\\\n\\mathbf{e}_{j\\to i}^\\prime &=  \\gamma_e(\\mathbf{e}_{j \\to i},\\mathbf{m}_{j \\to i})\n\\end{aligned}\\] where we refer to  $\\phi$  as to the message function,  and to  $\\gamma_x$  and  $\\gamma_e$  as to the node update and edge update function respectively. The aggregation  $\\square$  is over the neighborhood  $N(i)$  of node  $i$ ,  and it is usually equal either to  $\\sum$ , to  max  or to a  mean  operation.  In GNNlib.jl, the message passing mechanism is exposed by the  propagate  function.  propagate  takes care of materializing the node features on each edge, applying the message function, performing the aggregation, and returning  $\\bar{\\mathbf{m}}$ .  It is then left to the user to perform further node and edge updates, manipulating arrays of size  $D_{node} \\times num\\_nodes$  and     $D_{edge} \\times num\\_edges$ . propagate  is composed of two steps, also available as two independent methods: apply_edges  materializes node features on edges and applies the message function.  aggregate_neighbors  applies a reduction operator on the messages coming from the neighborhood of each node. The whole propagation mechanism internally relies on the  NNlib.gather   and  NNlib.scatter  methods."},{"id":801,"pagetitle":"Message Passing","title":"Examples","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/guides/messagepassing/#Examples","content":" Examples"},{"id":802,"pagetitle":"Message Passing","title":"Basic use of apply_edges and propagate","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/guides/messagepassing/#Basic-use-of-apply_edges-and-propagate","content":" Basic use of apply_edges and propagate The function  apply_edges  can be used to broadcast node data on each edge and produce new edge data. julia> using GNNlib, Graphs, Statistics\n\njulia> g = rand_graph(10, 20)\nGNNGraph:\n    num_nodes = 10\n    num_edges = 20\n\njulia> x = ones(2,10);\n\njulia> z = 2ones(2,10);\n\n# Return an edge features arrays (D × num_edges)\njulia> apply_edges((xi, xj, e) -> xi .+ xj, g, xi=x, xj=z)\n2×20 Matrix{Float64}:\n 3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0\n 3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0  3.0\n\n# now returning a named tuple\njulia> apply_edges((xi, xj, e) -> (a=xi .+ xj, b=xi .- xj), g, xi=x, xj=z)\n(a = [3.0 3.0 … 3.0 3.0; 3.0 3.0 … 3.0 3.0], b = [-1.0 -1.0 … -1.0 -1.0; -1.0 -1.0 … -1.0 -1.0])\n\n# Here we provide a named tuple input\njulia> apply_edges((xi, xj, e) -> xi.a + xi.b .* xj, g, xi=(a=x,b=z), xj=z)\n2×20 Matrix{Float64}:\n 5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0\n 5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0  5.0 The function  propagate  instead performs the  apply_edges  operation but then also applies a reduction over each node's neighborhood (see  aggregate_neighbors ). julia> propagate((xi, xj, e) -> xi .+ xj, g, +, xi=x, xj=z)\n2×10 Matrix{Float64}:\n 3.0  6.0  9.0  9.0  0.0  6.0  6.0  3.0  15.0  3.0\n 3.0  6.0  9.0  9.0  0.0  6.0  6.0  3.0  15.0  3.0\n\n# Previous output can be understood by looking at the degree\njulia> degree(g)\n10-element Vector{Int64}:\n 1\n 2\n 3\n 3\n 0\n 2\n 2\n 1\n 5\n 1"},{"id":803,"pagetitle":"Message Passing","title":"Implementing a custom Graph Convolutional Layer using Flux.jl","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/guides/messagepassing/#Implementing-a-custom-Graph-Convolutional-Layer-using-Flux.jl","content":" Implementing a custom Graph Convolutional Layer using Flux.jl Let's implement a simple graph convolutional layer using the message passing framework using the machine learning framework Flux.jl. The convolution reads  \\[\\mathbf{x}'_i = W \\cdot \\sum_{j \\in N(i)}  \\mathbf{x}_j\\] We will also add a bias and an activation function. using Flux, Graphs, GraphNeuralNetworks\n\nstruct GCN{A<:AbstractMatrix, B, F} <: GNNLayer\n    weight::A\n    bias::B\n    σ::F\nend\n\nFlux.@layer GCN # allow gpu movement, select trainable params etc...\n\nfunction GCN(ch::Pair{Int,Int}, σ=identity)\n    in, out = ch\n    W = Flux.glorot_uniform(out, in)\n    b = zeros(Float32, out)\n    GCN(W, b, σ)\nend\n\nfunction (l::GCN)(g::GNNGraph, x::AbstractMatrix{T}) where T\n    @assert size(x, 2) == g.num_nodes\n\n    # Computes messages from source/neighbour nodes (j) to target/root nodes (i).\n    # The message function will have to handle matrices of size (*, num_edges).\n    # In this simple case we just let the neighbor features go through.\n    message(xi, xj, e) = xj \n\n    # The + operator gives the sum aggregation.\n    # `mean`, `max`, `min`, and `*` are other possibilities.\n    x = propagate(message, g, +, xj=x) \n\n    return l.σ.(l.weight * x .+ l.bias)\nend See the  GATConv  implementation  here  for a more complex example."},{"id":804,"pagetitle":"Message Passing","title":"Built-in message functions","ref":"/GraphNeuralNetworks.jl/docs/GNNlib.jl/stable/guides/messagepassing/#Built-in-message-functions","content":" Built-in message functions In order to exploit optimized specializations of the  propagate , it is recommended  to use built-in message functions such as  copy_xj  whenever possible. "}]