<!DOCTYPE html><HTML lang="en"><head><script charset="utf-8" src="../../../../../assets/default/multidoc_injector.js" type="text/javascript"></script><script charset="utf-8" type="text/javascript">window.MULTIDOCUMENTER_ROOT_PATH = '/GraphNeuralNetworks.jl/'</script><script charset="utf-8" src="../../../../../assets/default/flexsearch.bundle.js" type="text/javascript"></script><script charset="utf-8" src="../../../../../assets/default/flexsearch_integration.js" type="text/javascript"></script><meta charset="UTF-8"/><meta content="width=device-width, initial-scale=1.0" name="viewport"/><title>Graph Classification · GNNLux.jl</title><meta content="Graph Classification · GNNLux.jl" name="title"/><meta content="Graph Classification · GNNLux.jl" property="og:title"/><meta content="Graph Classification · GNNLux.jl" property="twitter:title"/><meta content="Documentation for GNNLux.jl." name="description"/><meta content="Documentation for GNNLux.jl." property="og:description"/><meta content="Documentation for GNNLux.jl." property="twitter:description"/><script data-outdated-warner="" src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script data-main="../../assets/documenter.js" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" data-theme-name="catppuccin-mocha" href="../../assets/themes/catppuccin-mocha.css" rel="stylesheet" type="text/css"/><link class="docs-theme-link" data-theme-name="catppuccin-macchiato" href="../../assets/themes/catppuccin-macchiato.css" rel="stylesheet" type="text/css"/><link class="docs-theme-link" data-theme-name="catppuccin-frappe" href="../../assets/themes/catppuccin-frappe.css" rel="stylesheet" type="text/css"/><link class="docs-theme-link" data-theme-name="catppuccin-latte" href="../../assets/themes/catppuccin-latte.css" rel="stylesheet" type="text/css"/><link class="docs-theme-link" data-theme-name="documenter-dark" data-theme-primary-dark="" href="../../assets/themes/documenter-dark.css" rel="stylesheet" type="text/css"/><link class="docs-theme-link" data-theme-name="documenter-light" data-theme-primary="" href="../../assets/themes/documenter-light.css" rel="stylesheet" type="text/css"/><script src="../../assets/themeswap.js"></script><link href="../../../../../assets/default/multidoc.css" rel="stylesheet" type="text/css"/><link href="../../../../../assets/default/flexsearch.css" rel="stylesheet" type="text/css"/></head><body><nav id="multi-page-nav"><a class="brand" href="../../../../.."><img alt="home" src="../../../../../logo.svg"/></a><div class="hidden-on-mobile" id="nav-items"><a class="nav-link nav-item" href="../../../../GraphNeuralNetworks.jl/">GraphNeuralNetworks.jl</a><a class="nav-link active nav-item" href="../../../">GNNLux.jl</a><a class="nav-link nav-item" href="../../../../GNNGraphs.jl/">GNNGraphs.jl</a><a class="nav-link nav-item" href="../../../../GNNlib.jl/">GNNlib.jl</a><div class="search nav-item"><input id="search-input" placeholder="Search..."/><ul class="suggestions hidden" id="search-result-container"></ul><div class="search-keybinding">/</div></div></div><button id="multidoc-toggler"><svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"></path></svg></button></nav><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img alt="GNNLux.jl logo" src="../../assets/logo.svg"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">GNNLux.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Guides</span><ul><li><a class="tocitem" href="../../GNNGraphs/guides/gnngraph/">Graphs</a></li><li><a class="tocitem" href="../../GNNlib/guides/messagepassing/">Message Passing</a></li><li><a class="tocitem" href="../../guides/models/">Models</a></li><li><a class="tocitem" href="../../GNNGraphs/guides/datasets/">Datasets</a></li><li><a class="tocitem" href="../../GNNGraphs/guides/heterograph/">Heterogeneous Graphs</a></li><li><a class="tocitem" href="../../GNNGraphs/guides/temporalgraph/">Temporal Graphs</a></li></ul></li><li><span class="tocitem">Tutorials</span><ul><li><input checked="" class="collapse-toggle" id="menuitem-3-1" type="checkbox"/><label class="tocitem" for="menuitem-3-1"><span class="docs-label">Introductory tutorials</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../gnn_intro/">Hands on</a></li><li><a class="tocitem" href="../node_classification/">Node Classification</a></li><li class="is-active"><a class="tocitem" href="">Graph Classification</a><ul class="internal"><li><a class="tocitem" href="#Mini-batching-of-graphs"><span>Mini-batching of graphs</span></a></li><li><a class="tocitem" href="#Training-a-Graph-Neural-Network-(GNN)"><span>Training a Graph Neural Network (GNN)</span></a></li><li><a class="tocitem" href="#(Optional)-Exercise"><span>(Optional) Exercise</span></a></li><li><a class="tocitem" href="#Conclusion"><span>Conclusion</span></a></li></ul></li></ul></li></ul></li><li><span class="tocitem">API Reference</span><ul><li><input class="collapse-toggle" id="menuitem-4-1" type="checkbox"/><label class="tocitem" for="menuitem-4-1"><span class="docs-label">Graphs (GNNGraphs.jl)</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../GNNGraphs/api/gnngraph/">GNNGraph</a></li><li><a class="tocitem" href="../../GNNGraphs/api/heterograph/">GNNHeteroGraph</a></li><li><a class="tocitem" href="../../GNNGraphs/api/temporalgraph/">TemporalSnapshotsGNNGraph</a></li><li><a class="tocitem" href="../../GNNGraphs/api/datasets/">Datasets</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4-2" type="checkbox"/><label class="tocitem" for="menuitem-4-2"><span class="docs-label">Message Passing (GNNlib.jl)</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../GNNlib/api/messagepassing/">Message Passing</a></li><li><a class="tocitem" href="../../GNNlib/api/utils/">Other Operators</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-4-3" type="checkbox"/><label class="tocitem" for="menuitem-4-3"><span class="docs-label">Layers</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../api/basic/">Basic layers</a></li><li><a class="tocitem" href="../../api/conv/">Convolutional layers</a></li><li><a class="tocitem" href="../../api/pool/">Pooling layers</a></li><li><a class="tocitem" href="../../api/temporalconv/">Temporal Convolutional layers</a></li></ul></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" href="#" id="documenter-sidebar-button"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorials</a></li><li><a class="is-disabled">Introductory tutorials</a></li><li class="is-active"><a href="">Graph Classification</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href="">Graph Classification</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaGraphs/GraphNeuralNetworks.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/JuliaGraphs/GraphNeuralNetworks.jl/blob/master/GNNLux/docs/src/tutorials/graph_classification.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" href="#" id="documenter-settings-button" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" href="javascript:;" id="documenter-article-toggle-button" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Graph-Classification-with-Graph-Neural-Networks"><a class="docs-heading-anchor" href="#Graph-Classification-with-Graph-Neural-Networks">Graph Classification with Graph Neural Networks</a><a id="Graph-Classification-with-Graph-Neural-Networks-1"></a><a class="docs-heading-anchor-permalink" href="#Graph-Classification-with-Graph-Neural-Networks" title="Permalink"></a></h1><p><em>This tutorial is a Julia adaptation of the Pytorch Geometric tutorial that can be found <a href="https://pytorch-geometric.readthedocs.io/en/latest/notes/colabs.html">here</a>.</em></p><p>In this tutorial session we will have a closer look at how to apply <strong>Graph Neural Networks (GNNs) to the task of graph classification</strong>. Graph classification refers to the problem of classifying entire graphs (in contrast to nodes), given a <strong>dataset of graphs</strong>, based on some structural graph properties and possibly on some input node features. Here, we want to embed entire graphs, and we want to embed those graphs in such a way so that they are linearly separable given a task at hand.</p><p>A common graph classification task is <strong>molecular property prediction</strong>, in which molecules are represented as graphs, and the task may be to infer whether a molecule inhibits HIV virus replication or not.</p><p>The TU Dortmund University has collected a wide range of different graph classification datasets, known as the <a href="https://chrsmrrs.github.io/datasets/"><strong>TUDatasets</strong></a>, which are also accessible via MLDatasets.jl. Let's import the necessary packages. Then we'll load and inspect one of the smaller ones, the <strong>MUTAG dataset</strong>:</p><pre><code class="language-julia hljs">using Lux, GNNLux
using MLDatasets, MLUtils
using LinearAlgebra, Random, Statistics
using Zygote, Optimisers, OneHotArrays

ENV["DATADEPS_ALWAYS_ACCEPT"] = "true"  # don't ask for dataset download confirmation
rng = Random.seed!(42); # for reproducibility

dataset = TUDataset("MUTAG")</code></pre><pre><code class="nohighlight hljs">dataset TUDataset:
  name        =&gt;    MUTAG
  metadata    =&gt;    Dict{String, Any} with 1 entry
  graphs      =&gt;    188-element Vector{MLDatasets.Graph}
  graph_data  =&gt;    (targets = "188-element Vector{Int64}",)
  num_nodes   =&gt;    3371
  num_edges   =&gt;    7442
  num_graphs  =&gt;    188</code></pre><pre><code class="language-julia hljs">dataset.graph_data.targets |&gt; union</code></pre><pre><code class="nohighlight hljs">2-element Vector{Int64}:
  1
 -1</code></pre><pre><code class="language-julia hljs">g1, y1 = dataset[1] # get the first graph and target</code></pre><pre><code class="nohighlight hljs">(graphs = Graph(17, 38), targets = 1)</code></pre><pre><code class="language-julia hljs">reduce(vcat, g.node_data.targets for (g, _) in dataset) |&gt; union</code></pre><pre><code class="nohighlight hljs">7-element Vector{Int64}:
 0
 1
 2
 3
 4
 5
 6</code></pre><pre><code class="language-julia hljs">reduce(vcat, g.edge_data.targets for (g, _) in dataset) |&gt; union</code></pre><pre><code class="nohighlight hljs">4-element Vector{Int64}:
 0
 1
 2
 3</code></pre><p>This dataset provides <strong>188 different graphs</strong>, and the task is to classify each graph into <strong>one out of two classes</strong>.</p><p>By inspecting the first graph object of the dataset, we can see that it comes with <strong>17 nodes</strong> and <strong>38 edges</strong>. It also comes with exactly <strong>one graph label</strong>, and provides additional node labels (7 classes) and edge labels (4 classes). However, for the sake of simplicity, we will not make use of edge labels.</p><p>We now convert the <code>MLDatasets.jl</code> graph types to our <code>GNNGraph</code>s and we also onehot encode both the node labels (which will be used as input features) and the graph labels (what we want to predict):</p><pre><code class="language-julia hljs">graphs = mldataset2gnngraph(dataset)
graphs = [GNNGraph(g,
                    ndata = Float32.(onehotbatch(g.ndata.targets, 0:6)),
                    edata = nothing)
            for g in graphs]
y = onehotbatch(dataset.graph_data.targets, [-1, 1])</code></pre><pre><code class="nohighlight hljs">2×188 OneHotMatrix(::Vector{UInt32}) with eltype Bool:
 ⋅  1  1  ⋅  1  ⋅  1  ⋅  1  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  1  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  1  ⋅  1  1  1  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  1  1  ⋅  ⋅  ⋅  1  ⋅  ⋅  1  ⋅  ⋅  1  1  1  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  1  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  1  ⋅  1  1  ⋅  1  ⋅  ⋅  1  1  ⋅  ⋅  1  1  ⋅  ⋅  ⋅  ⋅  1  1  1  1  1  ⋅  1  ⋅  ⋅  1  1  ⋅  1  1  1  1  ⋅  ⋅  1  ⋅  ⋅  1  ⋅  ⋅  ⋅  1  1  1  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  ⋅  ⋅  1  ⋅  1  1  ⋅  ⋅  1  1  ⋅  1
 1  ⋅  ⋅  1  ⋅  1  ⋅  1  ⋅  1  1  1  1  ⋅  1  1  ⋅  1  ⋅  1  1  1  1  1  1  1  1  1  1  1  1  1  1  ⋅  1  ⋅  1  ⋅  ⋅  ⋅  1  ⋅  1  1  1  1  1  1  1  1  1  1  1  1  ⋅  1  1  1  1  1  1  ⋅  1  1  ⋅  ⋅  1  1  1  ⋅  1  1  ⋅  1  1  ⋅  ⋅  ⋅  1  1  1  1  1  ⋅  1  1  1  ⋅  ⋅  1  1  1  1  1  1  1  1  ⋅  1  ⋅  1  1  1  1  1  1  1  1  1  ⋅  ⋅  1  ⋅  ⋅  1  ⋅  1  1  ⋅  ⋅  1  1  ⋅  ⋅  1  1  1  1  ⋅  ⋅  ⋅  ⋅  ⋅  1  ⋅  1  1  ⋅  ⋅  1  ⋅  ⋅  ⋅  ⋅  1  1  ⋅  1  1  ⋅  1  1  1  ⋅  ⋅  ⋅  1  1  1  ⋅  1  1  1  1  1  1  1  ⋅  1  1  1  1  1  1  ⋅  1  1  1  ⋅  1  ⋅  ⋅  1  1  ⋅  ⋅  1  ⋅</code></pre><p>We have some useful utilities for working with graph datasets, <em>e.g.</em>, we can shuffle the dataset and use the first 150 graphs as training graphs, while using the remaining ones for testing:</p><pre><code class="language-julia hljs">train_data, test_data = splitobs((graphs, y), at = 150, shuffle = true) |&gt; getobs


train_loader = DataLoader(train_data, batchsize = 32, shuffle = true)
test_loader = DataLoader(test_data, batchsize = 32, shuffle = false)</code></pre><pre><code class="nohighlight hljs">2-element DataLoader(::Tuple{Vector{GNNGraphs.GNNGraph{Tuple{Vector{Int64}, Vector{Int64}, Nothing}}}, OneHotArrays.OneHotMatrix{UInt32, Vector{UInt32}}}, batchsize=32)
  with first element:
  (32-element Vector{GNNGraphs.GNNGraph{Tuple{Vector{Int64}, Vector{Int64}, Nothing}}}, 2×32 OneHotMatrix(::Vector{UInt32}) with eltype Bool,)</code></pre><p>Here, we opt for a <code>batch_size</code> of 32, leading to 5 (randomly shuffled) mini-batches, containing all <span>$4 \cdot 32+22 = 150$</span> graphs.</p><h2 id="Mini-batching-of-graphs"><a class="docs-heading-anchor" href="#Mini-batching-of-graphs">Mini-batching of graphs</a><a id="Mini-batching-of-graphs-1"></a><a class="docs-heading-anchor-permalink" href="#Mini-batching-of-graphs" title="Permalink"></a></h2><p>Since graphs in graph classification datasets are usually small, a good idea is to <strong>batch the graphs</strong> before inputting them into a Graph Neural Network to guarantee full GPU utilization. In the image or language domain, this procedure is typically achieved by <strong>rescaling</strong> or <strong>padding</strong> each example into a set of equally-sized shapes, and examples are then grouped in an additional dimension. The length of this dimension is then equal to the number of examples grouped in a mini-batch and is typically referred to as the <code>batchsize</code>.</p><p>However, for GNNs the two approaches described above are either not feasible or may result in a lot of unnecessary memory consumption. Therefore, GNNLux.jl opts for another approach to achieve parallelization across a number of examples. Here, adjacency matrices are stacked in a diagonal fashion (creating a giant graph that holds multiple isolated subgraphs), and node and target features are simply concatenated in the node dimension (the last dimension).</p><p>This procedure has some crucial advantages over other batching procedures:</p><ol><li><p>GNN operators that rely on a message passing scheme do not need to be modified since messages are not exchanged between two nodes that belong to different graphs.</p></li><li><p>There is no computational or memory overhead since adjacency matrices are saved in a sparse fashion holding only non-zero entries, <em>i.e.</em>, the edges.</p></li></ol><p>GNNLux.jl can <strong>batch multiple graphs into a single giant graph</strong>:</p><pre><code class="language-julia hljs">vec_gs, _ = first(train_loader)</code></pre><pre><code class="nohighlight hljs">(GNNGraphs.GNNGraph{Tuple{Vector{Int64}, Vector{Int64}, Nothing}}[GNNGraph(11, 22) with x: 7×11 data, GNNGraph(22, 50) with x: 7×22 data, GNNGraph(19, 44) with x: 7×19 data, GNNGraph(22, 50) with x: 7×22 data, GNNGraph(16, 36) with x: 7×16 data, GNNGraph(20, 44) with x: 7×20 data, GNNGraph(19, 42) with x: 7×19 data, GNNGraph(20, 44) with x: 7×20 data, GNNGraph(13, 26) with x: 7×13 data, GNNGraph(19, 40) with x: 7×19 data, GNNGraph(25, 56) with x: 7×25 data, GNNGraph(16, 34) with x: 7×16 data, GNNGraph(28, 66) with x: 7×28 data, GNNGraph(19, 40) with x: 7×19 data, GNNGraph(19, 44) with x: 7×19 data, GNNGraph(17, 36) with x: 7×17 data, GNNGraph(12, 24) with x: 7×12 data, GNNGraph(16, 34) with x: 7×16 data, GNNGraph(27, 66) with x: 7×27 data, GNNGraph(17, 38) with x: 7×17 data, GNNGraph(19, 42) with x: 7×19 data, GNNGraph(17, 36) with x: 7×17 data, GNNGraph(12, 26) with x: 7×12 data, GNNGraph(24, 50) with x: 7×24 data, GNNGraph(20, 46) with x: 7×20 data, GNNGraph(19, 42) with x: 7×19 data, GNNGraph(11, 22) with x: 7×11 data, GNNGraph(16, 34) with x: 7×16 data, GNNGraph(13, 28) with x: 7×13 data, GNNGraph(13, 28) with x: 7×13 data, GNNGraph(13, 28) with x: 7×13 data, GNNGraph(16, 36) with x: 7×16 data], Bool[1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 1 1 1 0; 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 0 1])</code></pre><pre><code class="language-julia hljs">MLUtils.batch(vec_gs)</code></pre><pre><code class="nohighlight hljs">GNNGraph:
  num_nodes: 570
  num_edges: 1254
  num_graphs: 32
  ndata:
    x = 7×570 Matrix{Float32}</code></pre><p>Each batched graph object is equipped with a <strong><code>graph_indicator</code> vector</strong>, which maps each node to its respective graph in the batch:</p><p class="math-container">\[\textrm{graph\_indicator} = [1, \ldots, 1, 2, \ldots, 2, 3, \ldots ]\]</p><h2 id="Training-a-Graph-Neural-Network-(GNN)"><a class="docs-heading-anchor" href="#Training-a-Graph-Neural-Network-(GNN)">Training a Graph Neural Network (GNN)</a><a id="Training-a-Graph-Neural-Network-(GNN)-1"></a><a class="docs-heading-anchor-permalink" href="#Training-a-Graph-Neural-Network-(GNN)" title="Permalink"></a></h2><p>Training a GNN for graph classification usually follows a simple recipe:</p><ol><li>Embed each node by performing multiple rounds of message passing</li><li>Aggregate node embeddings into a unified graph embedding (<strong>readout layer</strong>)</li><li>Train a final classifier on the graph embedding</li></ol><p>There exists multiple <strong>readout layers</strong> in literature, but the most common one is to simply take the average of node embeddings:</p><p class="math-container">\[\mathbf{x}_{\mathcal{G}} = \frac{1}{|\mathcal{V}|} \sum_{v \in \mathcal{V}} \mathcal{x}^{(L)}_v\]</p><p>GNNLux.jl provides this functionality via <code>GlobalPool(mean)</code>, which takes in the node embeddings of all nodes in the mini-batch and the assignment vector <code>graph_indicator</code> to compute a graph embedding of size <code>[hidden_channels, batchsize]</code>.</p><p>The final architecture for applying GNNs to the task of graph classification then looks as follows and allows for complete end-to-end training:</p><pre><code class="language-julia hljs">function create_model(nin, nh, nout)
    GNNChain(GCNConv(nin =&gt; nh, relu),
             GCNConv(nh =&gt; nh, relu),
             GCNConv(nh =&gt; nh),
             GlobalPool(mean),
             Dropout(0.5),
             Dense(nh, nout))
end;

nin = 7
nh = 64
nout = 2
model = create_model(nin, nh, nout)

ps, st = LuxCore.initialparameters(rng, model), LuxCore.initialstates(rng, model);</code></pre><pre><code class="nohighlight hljs">┌ Warning: `replicate` doesn't work for `TaskLocalRNG`. Returning the same `TaskLocalRNG`.
└ @ LuxCore ~/.julia/packages/LuxCore/GlbG3/src/LuxCore.jl:18
</code></pre><p>Here, we again make use of the <code>GCNConv</code> with <span>$\mathrm{ReLU}(x) = \max(x, 0)$</span> activation for obtaining localized node embeddings, before we apply our final classifier on top of a graph readout layer.</p><p>Let's train our network for a few epochs to see how well it performs on the training as well as test set:</p><pre><code class="language-julia hljs">function custom_loss(model, ps, st, tuple)
    g, x, y = tuple
    logitcrossentropy = CrossEntropyLoss(; logits=Val(true))
    st = Lux.trainmode(st)
    ŷ, st = model(g, x, ps, st)
    return  logitcrossentropy(ŷ, y), (; layers = st), 0
end

function eval_loss_accuracy(model, ps, st, data_loader)
    loss = 0.0
    acc = 0.0
    ntot = 0
    logitcrossentropy = CrossEntropyLoss(; logits=Val(true))
    for (g, y) in data_loader
        g = MLUtils.batch(g)
        n = length(y)
        ŷ, _ = model(g, g.ndata.x, ps, st)
        loss += logitcrossentropy(ŷ, y) * n
        acc += mean((ŷ .&gt; 0) .== y) * n
        ntot += n
    end
    return (loss = round(loss / ntot, digits = 4),
            acc = round(acc * 100 / ntot, digits = 2))
end

function train_model!(model, ps, st; epochs = 500, infotime = 100)
    train_state = Lux.Training.TrainState(model, ps, st, Adam(1e-2))

    function report(epoch)
        train = eval_loss_accuracy(model, ps, st, train_loader)
        st = Lux.testmode(st)
        test = eval_loss_accuracy(model, ps, st, test_loader)
        st = Lux.trainmode(st)
        @info (; epoch, train, test)
    end
    report(0)
    for iter in 1:epochs
        for (g, y) in train_loader
            g = MLUtils.batch(g)
            _, loss, _, train_state = Lux.Training.single_train_step!(AutoZygote(), custom_loss,(g, g.ndata.x, y), train_state)
        end

        iter % infotime == 0 &amp;&amp; report(iter)
    end
    return model, ps, st
end

model, ps, st = train_model!(model, ps, st);</code></pre><pre><code class="nohighlight hljs">┌ Warning: `training` is set to `Val{true}()` but is not being used within an autodiff call (gradient, jacobian, etc...). This will be slow. If you are using a `Lux.jl` model, set it to inference (test) mode using `LuxCore.testmode`. Reliance on this behavior is discouraged, and is not guaranteed by Semantic Versioning, and might be removed without a deprecation cycle. It is recommended to fix this issue in your code.
└ @ LuxLib.Utils ~/.julia/packages/LuxLib/ru5RQ/src/utils.jl:314
┌ Warning: `replicate` doesn't work for `TaskLocalRNG`. Returning the same `TaskLocalRNG`.
└ @ LuxCore ~/.julia/packages/LuxCore/GlbG3/src/LuxCore.jl:18
[ Info: (epoch = 0, train = (loss = 0.6934, acc = 51.67), test = (loss = 0.6902, acc = 50.0))
[ Info: (epoch = 100, train = (loss = 0.3979, acc = 81.33), test = (loss = 0.5769, acc = 69.74))
[ Info: (epoch = 200, train = (loss = 0.3904, acc = 84.0), test = (loss = 0.6402, acc = 65.79))
[ Info: (epoch = 300, train = (loss = 0.3813, acc = 85.33), test = (loss = 0.6331, acc = 69.74))
[ Info: (epoch = 400, train = (loss = 0.3682, acc = 85.0), test = (loss = 0.7273, acc = 69.74))
[ Info: (epoch = 500, train = (loss = 0.3561, acc = 86.67), test = (loss = 0.6825, acc = 73.68))
</code></pre><p>As one can see, our model reaches around <strong>74% test accuracy</strong>. Reasons for the fluctuations in accuracy can be explained by the rather small dataset (only 38 test graphs), and usually disappear once one applies GNNs to larger datasets.</p><h2 id="(Optional)-Exercise"><a class="docs-heading-anchor" href="#(Optional)-Exercise">(Optional) Exercise</a><a id="(Optional)-Exercise-1"></a><a class="docs-heading-anchor-permalink" href="#(Optional)-Exercise" title="Permalink"></a></h2><p>Can we do better than this? As multiple papers pointed out (<a href="https://arxiv.org/abs/1810.00826">Xu et al. (2018)</a>, <a href="https://arxiv.org/abs/1810.02244">Morris et al. (2018)</a>), applying <strong>neighborhood normalization decreases the expressivity of GNNs in distinguishing certain graph structures</strong>. An alternative formulation (<a href="https://arxiv.org/abs/1810.02244">Morris et al. (2018)</a>) omits neighborhood normalization completely and adds a simple skip-connection to the GNN layer in order to preserve central node information:</p><p class="math-container">\[\mathbf{x}_i^{(\ell+1)} = \mathbf{W}^{(\ell + 1)}_1 \mathbf{x}_i^{(\ell)} + \mathbf{W}^{(\ell + 1)}_2 \sum_{j \in \mathcal{N}(i)} \mathbf{x}_j^{(\ell)}\]</p><p>This layer is implemented under the name <code>GraphConv</code> in GraphNeuralNetworks.jl.</p><p>As an exercise, you are invited to complete the following code to the extent that it makes use of <code>GraphConv</code> rather than <code>GCNConv</code>. This should bring you close to <strong>82% test accuracy</strong>.</p><h2 id="Conclusion"><a class="docs-heading-anchor" href="#Conclusion">Conclusion</a><a id="Conclusion-1"></a><a class="docs-heading-anchor-permalink" href="#Conclusion" title="Permalink"></a></h2><p>In this chapter, you have learned how to apply GNNs to the task of graph classification. You have learned how graphs can be batched together for better GPU utilization, and how to apply readout layers for obtaining graph embeddings rather than node embeddings.</p><hr/><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../node_classification/">« Node Classification</a><a class="docs-footer-nextpage" href="../../GNNGraphs/api/gnngraph/">GNNGraph »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label></p><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div><p></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.8.0 on <span class="colophon-date" title="Friday 7 February 2025 00:03">Friday 7 February 2025</span>. Using Julia version 1.11.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></HTML>